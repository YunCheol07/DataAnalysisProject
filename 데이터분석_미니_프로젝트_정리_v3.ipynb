{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCTgOfaaSRNP",
        "outputId": "cf3a9b41-9cea-4fd2-d71b-d3ecd627a31b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ9I9phbOZli",
        "outputId": "d8884ce0-f7db-40fe-e9cd-3c8feddf6784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ë‹¨ê³„ 1: ë°ì´í„° ì ì¬ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ê°œì„  ë²„ì „)\n",
            "======================================================================\n",
            "\n",
            "[ë°ì´í„° ê¸°ë³¸ ì •ë³´]\n",
            "ë°ì´í„° í¬ê¸°: (8349, 3078)\n",
            "ì»¬ëŸ¼ ìˆ˜: 3078\n",
            "ìƒ˜í”Œ ìˆ˜: 8,349ê°œ\n",
            "\n",
            "[ë°ì´í„° íƒ€ì… ë¶„í¬]\n",
            "int64      3073\n",
            "float64       4\n",
            "object        1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[ê²°ì¸¡ì¹˜ ë¶„ì„]\n",
            "ì „ì²´ ê²°ì¸¡ì¹˜: 0ê°œ\n",
            "\n",
            "[ë¼ë²¨ ë¶„í¬ ë¶„ì„]\n",
            "í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:\n",
            "  - í´ë˜ìŠ¤ 1: 4,542ê°œ (54.40%)\n",
            "  - í´ë˜ìŠ¤ 0: 3,807ê°œ (45.60%)\n",
            "\n",
            "í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: 1.19:1\n",
            "âœ“ í´ë˜ìŠ¤ ê· í˜•ì´ ì–‘í˜¸í•©ë‹ˆë‹¤\n",
            "\n",
            "[ì»¬ëŸ¼ ê·¸ë£¹ ì •ì˜]\n",
            "Fingerprint ì»¬ëŸ¼: 3072ê°œ\n",
            "  - ECFP: 1024ê°œ\n",
            "  - FCFP: 1024ê°œ\n",
            "  - PTFP: 1024ê°œ\n",
            "ë¬¼ì„± Descriptor: ['MolWt', 'clogp', 'sa_score', 'qed']\n",
            "\n",
            "[ë¬¼ì„± Descriptor í†µê³„]\n",
            "                mean        std        min          max\n",
            "MolWt     443.248753  88.876374  94.117000  1242.488000\n",
            "clogp       3.794829   1.379045  -4.048930     9.429480\n",
            "sa_score    3.187613   0.727768   1.282432     7.309297\n",
            "qed         0.559151   0.185664   0.024365     0.947494\n",
            "\n",
            "[ì´ìƒì¹˜ ë¶„ì„ (IQR ê¸°ë°˜)]\n",
            "  - MolWt: 130ê°œ (1.56%)\n",
            "  - clogp: 137ê°œ (1.64%)\n",
            "  - sa_score: 158ê°œ (1.89%)\n",
            "\n",
            "[Fingerprint í¬ì†Œì„± ë¶„ì„]\n",
            "í¬ì†Œì„±: 82.59% (0ì˜ ë¹„ìœ¨)\n",
            "\n",
            "[Feature Importance ë¡œë“œ]\n",
            "ì´ í”¼ì²˜: 3076ê°œ\n",
            "\n",
            "ì„ íƒëœ í”¼ì²˜ (50 ì´ìƒ):\n",
            "  - Fingerprint: 281ê°œ\n",
            "  - Descriptor: 4ê°œ\n",
            "  - ì´: 285ê°œ\n",
            "\n",
            "[í•™ìŠµ ë°ì´í„° ì¤€ë¹„]\n",
            "X shape: (8349, 285)\n",
            "y shape: (8349,)\n",
            "\n",
            "[ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ]\n",
            "  - Fingerprint: ê²°ì¸¡ì¹˜ â†’ 0 ëŒ€ì¹˜ (281ê°œ ì»¬ëŸ¼)\n",
            "  - Descriptor: ê²°ì¸¡ì¹˜ â†’ ì¤‘ì•™ê°’ ëŒ€ì¹˜ + RobustScaler (ì´ìƒì¹˜ ê°•ê±´)\n",
            "  - êµì°¨ê²€ì¦: 5-Fold Stratified\n",
            "\n",
            "[ì „ì²˜ë¦¬ ë³€í™˜ í…ŒìŠ¤íŠ¸]\n",
            "\n",
            "Fold 1:\n",
            "  - í•™ìŠµ ë°ì´í„°: (6679, 285)\n",
            "  - ê²€ì¦ ë°ì´í„°: (1670, 285)\n",
            "  - í•™ìŠµ ë¼ë²¨ ë¶„í¬: Class 0=3046, Class 1=3633\n",
            "  - ê²€ì¦ ë¼ë²¨ ë¶„í¬: Class 0=761, Class 1=909\n",
            "  - í•™ìŠµ ë°ì´í„° í†µê³„: mean=0.2465, std=0.4396\n",
            "  - ê²€ì¦ ë°ì´í„° í†µê³„: mean=0.2447, std=0.4389\n",
            "\n",
            "[ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥]\n",
            "âœ“ ì „ì²´ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ\n",
            "\n",
            "[ì„¤ì • ìš”ì•½]\n",
            "  - n_splits: 5\n",
            "  - random_state: 42\n",
            "  - fp_cols: 281ê°œ\n",
            "  - desc_cols: 4ê°œ\n",
            "  - use_feature_selection: True\n",
            "  - n_features: 285\n",
            "  - class_imbalance_ratio: 1.1930654058313632\n",
            "\n",
            "======================================================================\n",
            "âœ“ ë‹¨ê³„ 1 ì™„ë£Œ - ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ê²€ì¦ ì™„ë£Œ\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ë‹¨ê³„ 1: ë°ì´í„° ì ì¬ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (ê°œì„  ë²„ì „)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ë‹¨ê³„ 1: ë°ì´í„° ì ì¬ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ê°œì„  ë²„ì „)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================================\n",
        "# 1.1 ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´\n",
        "# ============================================================\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "print(f\"\\n[ë°ì´í„° ê¸°ë³¸ ì •ë³´]\")\n",
        "print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
        "print(f\"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\")\n",
        "print(f\"ìƒ˜í”Œ ìˆ˜: {len(df):,}ê°œ\")\n",
        "\n",
        "# ë°ì´í„° íƒ€ì… í™•ì¸\n",
        "print(f\"\\n[ë°ì´í„° íƒ€ì… ë¶„í¬]\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ í™•ì¸\n",
        "print(f\"\\n[ê²°ì¸¡ì¹˜ ë¶„ì„]\")\n",
        "missing_count = df.isna().sum().sum()\n",
        "print(f\"ì „ì²´ ê²°ì¸¡ì¹˜: {missing_count:,}ê°œ\")\n",
        "\n",
        "if missing_count > 0:\n",
        "    missing_cols = df.columns[df.isna().any()].tolist()\n",
        "    print(f\"ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ\")\n",
        "    for col in missing_cols[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥\n",
        "        print(f\"  - {col}: {df[col].isna().sum()}ê°œ ({df[col].isna().mean()*100:.2f}%)\")\n",
        "\n",
        "# ============================================================\n",
        "# 1.2 ë¼ë²¨ ë¶„í¬ ë° í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„\n",
        "# ============================================================\n",
        "print(f\"\\n[ë¼ë²¨ ë¶„í¬ ë¶„ì„]\")\n",
        "label_counts = df['label'].value_counts()\n",
        "label_ratio = df['label'].value_counts(normalize=True)\n",
        "\n",
        "print(\"í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:\")\n",
        "for label, count in label_counts.items():\n",
        "    ratio = label_ratio[label] * 100\n",
        "    print(f\"  - í´ë˜ìŠ¤ {label}: {count:,}ê°œ ({ratio:.2f}%)\")\n",
        "\n",
        "# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°\n",
        "imbalance_ratio = label_counts.max() / label_counts.min()\n",
        "print(f\"\\ní´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "if imbalance_ratio > 1.5:\n",
        "    print(\"âš ï¸  í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°ì§€ â†’ class_weight='balanced' ì‚¬ìš© ê¶Œì¥\")\n",
        "else:\n",
        "    print(\"âœ“ í´ë˜ìŠ¤ ê· í˜•ì´ ì–‘í˜¸í•©ë‹ˆë‹¤\")\n",
        "\n",
        "# ============================================================\n",
        "# 1.3 ì»¬ëŸ¼ ê·¸ë£¹ ì •ì˜\n",
        "# ============================================================\n",
        "id_col = 'SMILES'\n",
        "label_col = 'label'\n",
        "\n",
        "# Fingerprint ì»¬ëŸ¼ (ecfp, fcfp, ptfp)\n",
        "fp_cols = [col for col in df.columns if col.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "\n",
        "# ë¬¼ì„± descriptor ì»¬ëŸ¼\n",
        "desc_cols = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "\n",
        "print(f\"\\n[ì»¬ëŸ¼ ê·¸ë£¹ ì •ì˜]\")\n",
        "print(f\"Fingerprint ì»¬ëŸ¼: {len(fp_cols)}ê°œ\")\n",
        "print(f\"  - ECFP: {len([c for c in fp_cols if c.startswith('ecfp_')])}ê°œ\")\n",
        "print(f\"  - FCFP: {len([c for c in fp_cols if c.startswith('fcfp_')])}ê°œ\")\n",
        "print(f\"  - PTFP: {len([c for c in fp_cols if c.startswith('ptfp_')])}ê°œ\")\n",
        "print(f\"ë¬¼ì„± Descriptor: {desc_cols}\")\n",
        "\n",
        "# ============================================================\n",
        "# 1.4 Feature í†µê³„ ë¶„ì„\n",
        "# ============================================================\n",
        "print(f\"\\n[ë¬¼ì„± Descriptor í†µê³„]\")\n",
        "desc_stats = df[desc_cols].describe().T\n",
        "print(desc_stats[['mean', 'std', 'min', 'max']])\n",
        "\n",
        "# ì´ìƒì¹˜ ê°ì§€\n",
        "print(f\"\\n[ì´ìƒì¹˜ ë¶„ì„ (IQR ê¸°ë°˜)]\")\n",
        "for col in desc_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)][col]\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"  - {col}: {len(outliers)}ê°œ ({len(outliers)/len(df)*100:.2f}%)\")\n",
        "\n",
        "# Fingerprint í¬ì†Œì„± ë¶„ì„\n",
        "print(f\"\\n[Fingerprint í¬ì†Œì„± ë¶„ì„]\")\n",
        "fp_data = df[fp_cols]\n",
        "sparsity = (fp_data == 0).sum().sum() / (fp_data.shape[0] * fp_data.shape[1])\n",
        "print(f\"í¬ì†Œì„±: {sparsity*100:.2f}% (0ì˜ ë¹„ìœ¨)\")\n",
        "\n",
        "# ============================================================\n",
        "# 1.5 Feature Importance ê¸°ë°˜ ì„ íƒ (ì„ íƒì )\n",
        "# ============================================================\n",
        "# Feature importance íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œ\n",
        "try:\n",
        "    feature_importance = pd.read_csv('feature_importance_cv.csv')\n",
        "    print(f\"\\n[Feature Importance ë¡œë“œ]\")\n",
        "    print(f\"ì´ í”¼ì²˜: {len(feature_importance)}ê°œ\")\n",
        "\n",
        "    # ìƒìœ„ í”¼ì²˜ ì„ íƒ (ì„ê³„ê°’: importance_mean > 50)\n",
        "    importance_threshold = 50\n",
        "    selected_by_importance = feature_importance[\n",
        "        feature_importance['importance_mean'] > importance_threshold\n",
        "    ]['feature'].tolist()\n",
        "\n",
        "    # DescriptorëŠ” í•­ìƒ í¬í•¨\n",
        "    selected_features = list(set(selected_by_importance + desc_cols))\n",
        "\n",
        "    # Fingerprint ì¤‘ ì„ íƒëœ ê²ƒë§Œ í•„í„°ë§\n",
        "    fp_cols_selected = [col for col in selected_features if col in fp_cols]\n",
        "\n",
        "    print(f\"\\nì„ íƒëœ í”¼ì²˜ ({importance_threshold} ì´ìƒ):\")\n",
        "    print(f\"  - Fingerprint: {len(fp_cols_selected)}ê°œ\")\n",
        "    print(f\"  - Descriptor: {len([c for c in selected_features if c in desc_cols])}ê°œ\")\n",
        "    print(f\"  - ì´: {len(selected_features)}ê°œ\")\n",
        "\n",
        "    # í”¼ì²˜ ì„ íƒ ì ìš©\n",
        "    use_feature_selection = True\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n[Feature Importance íŒŒì¼ ì—†ìŒ - ì „ì²´ í”¼ì²˜ ì‚¬ìš©]\")\n",
        "    selected_features = fp_cols + desc_cols\n",
        "    fp_cols_selected = fp_cols\n",
        "    use_feature_selection = False\n",
        "\n",
        "# ============================================================\n",
        "# 1.6 X, y ë¶„ë¦¬\n",
        "# ============================================================\n",
        "if use_feature_selection:\n",
        "    X = df[selected_features]\n",
        "else:\n",
        "    X = df.drop(columns=[label_col])\n",
        "\n",
        "y = df[label_col].astype(int)\n",
        "\n",
        "print(f\"\\n[í•™ìŠµ ë°ì´í„° ì¤€ë¹„]\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# ============================================================\n",
        "# 1.7 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„± (ê°œì„ )\n",
        "# ============================================================\n",
        "# Fingerprint ì „ì²˜ë¦¬: 0ìœ¼ë¡œ ëŒ€ì¹˜\n",
        "fp_transformer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "\n",
        "# Descriptor ì „ì²˜ë¦¬: RobustScaler ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•ê±´)\n",
        "desc_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', RobustScaler())  # StandardScaler ëŒ€ì‹  RobustScaler ì‚¬ìš©\n",
        "    # RobustScalerëŠ” ì¤‘ì•™ê°’ê³¼ IQRì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ì— ë” ê°•ê±´í•¨\n",
        "])\n",
        "\n",
        "# ì‚¬ìš©í•  fingerprint ì»¬ëŸ¼ ê²°ì •\n",
        "fp_cols_to_use = fp_cols_selected if use_feature_selection else fp_cols\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', fp_transformer, fp_cols_to_use),\n",
        "        ('desc', desc_transformer, desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "print(f\"\\n[ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ]\")\n",
        "print(f\"  - Fingerprint: ê²°ì¸¡ì¹˜ â†’ 0 ëŒ€ì¹˜ ({len(fp_cols_to_use)}ê°œ ì»¬ëŸ¼)\")\n",
        "print(f\"  - Descriptor: ê²°ì¸¡ì¹˜ â†’ ì¤‘ì•™ê°’ ëŒ€ì¹˜ + RobustScaler (ì´ìƒì¹˜ ê°•ê±´)\")\n",
        "\n",
        "# ============================================================\n",
        "# 1.8 êµì°¨ê²€ì¦ ì„¤ì •\n",
        "# ============================================================\n",
        "N_SPLITS = 5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "skf = StratifiedKFold(\n",
        "    n_splits=N_SPLITS,\n",
        "    shuffle=True,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"  - êµì°¨ê²€ì¦: {N_SPLITS}-Fold Stratified\")\n",
        "\n",
        "# ============================================================\n",
        "# 1.9 ìƒ˜í”Œ ë³€í™˜ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦\n",
        "# ============================================================\n",
        "print(f\"\\n[ì „ì²˜ë¦¬ ë³€í™˜ í…ŒìŠ¤íŠ¸]\")\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
        "\n",
        "    # ì „ì²˜ë¦¬ ì ìš©\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    # Fold ì •ë³´\n",
        "    print(f\"\\nFold {fold}:\")\n",
        "    print(f\"  - í•™ìŠµ ë°ì´í„°: {Xt_tr.shape}\")\n",
        "    print(f\"  - ê²€ì¦ ë°ì´í„°: {Xt_va.shape}\")\n",
        "    print(f\"  - í•™ìŠµ ë¼ë²¨ ë¶„í¬: Class 0={sum(y_tr==0)}, Class 1={sum(y_tr==1)}\")\n",
        "    print(f\"  - ê²€ì¦ ë¼ë²¨ ë¶„í¬: Class 0={sum(y_va==0)}, Class 1={sum(y_va==1)}\")\n",
        "\n",
        "    # ë³€í™˜ í›„ í†µê³„\n",
        "    print(f\"  - í•™ìŠµ ë°ì´í„° í†µê³„: mean={Xt_tr.mean():.4f}, std={Xt_tr.std():.4f}\")\n",
        "    print(f\"  - ê²€ì¦ ë°ì´í„° í†µê³„: mean={Xt_va.mean():.4f}, std={Xt_va.std():.4f}\")\n",
        "\n",
        "    break  # ì²« foldë§Œ í™•ì¸\n",
        "\n",
        "# ============================================================\n",
        "# 1.10 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë° ì„¤ì • ì €ì¥\n",
        "# ============================================================\n",
        "# ì „ì²´ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ (ë‚˜ì¤‘ì— í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì‚¬ìš©)\n",
        "preprocessor_full = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', fp_transformer, fp_cols_to_use),\n",
        "        ('desc', desc_transformer, desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "preprocessor_full.fit(X)\n",
        "\n",
        "print(f\"\\n[ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥]\")\n",
        "print(f\"âœ“ ì „ì²´ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ê¸° í•™ìŠµ ì™„ë£Œ\")\n",
        "\n",
        "# ì„¤ì • ì €ì¥\n",
        "config = {\n",
        "    'n_splits': N_SPLITS,\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'fp_cols': fp_cols_to_use,\n",
        "    'desc_cols': desc_cols,\n",
        "    'use_feature_selection': use_feature_selection,\n",
        "    'n_features': len(fp_cols_to_use) + len(desc_cols),\n",
        "    'class_imbalance_ratio': imbalance_ratio\n",
        "}\n",
        "\n",
        "print(f\"\\n[ì„¤ì • ìš”ì•½]\")\n",
        "for key, value in config.items():\n",
        "    if isinstance(value, list):\n",
        "        print(f\"  - {key}: {len(value)}ê°œ\")\n",
        "    else:\n",
        "        print(f\"  - {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"âœ“ ë‹¨ê³„ 1 ì™„ë£Œ - ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ê²€ì¦ ì™„ë£Œ\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ ì €ì¥\n",
        "globals().update({\n",
        "    'preprocessor': preprocessor_full,\n",
        "    'X_data': X,\n",
        "    'y_data': y,\n",
        "    'skf': skf,\n",
        "    'config': config\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ë‹¨ê³„ 2: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ (ìµœì¢… ê°œì„  ë²„ì „)\n",
        "# ============================================================\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import (\n",
        "    f1_score, classification_report, confusion_matrix,\n",
        "    roc_auc_score, precision_recall_curve, roc_curve,\n",
        "    precision_score, recall_score\n",
        ")\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ë‹¨ê³„ 2: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ (ìµœì¢… ê°œì„  ë²„ì „)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[ì ìš©ëœ ê°œì„ ì‚¬í•­]\")\n",
        "print(\"  1. âœ“ Threshold ì¬ìµœì í™” (OOF ê¸°ë°˜ ìë™ íƒìƒ‰)\")\n",
        "print(\"  2. âœ“ XGBoost ì¤‘ì‹¬ Ensemble (50% ê°€ì¤‘ì¹˜)\")\n",
        "print(\"  3. âœ“ Class Weighting (FP í˜ë„í‹° ì¦ê°€)\")\n",
        "print(\"  4. âœ“ Sample Weighting (Low Confidence ìƒ˜í”Œ ì§‘ì¤‘)\")\n",
        "\n",
        "# ============================================================\n",
        "# 2.1 Feature Selection (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Feature Selection\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "try:\n",
        "    feature_importance = pd.read_csv('feature_importance_cv.csv')\n",
        "    importance_threshold = 30\n",
        "\n",
        "    selected_features_from_importance = feature_importance[\n",
        "        feature_importance['importance_mean'] > importance_threshold\n",
        "    ]['feature'].tolist()\n",
        "\n",
        "    desc_cols = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "    selected_features = list(set(selected_features_from_importance + desc_cols))\n",
        "    available_features = [f for f in selected_features if f in X_data.columns]\n",
        "\n",
        "    X_selected = X_data[available_features]\n",
        "\n",
        "    print(f\"\\n[Feature Selection ê²°ê³¼]\")\n",
        "    print(f\"  ì›ë³¸ í”¼ì²˜: {X_data.shape[1]}ê°œ\")\n",
        "    print(f\"  ì‹¤ì œ ì‚¬ìš©: {len(available_features)}ê°œ\")\n",
        "\n",
        "    feature_selection_applied = True\n",
        "    selected_features = available_features\n",
        "\n",
        "except:\n",
        "    X_selected = X_data.copy()\n",
        "    selected_features = X_data.columns.tolist()\n",
        "    feature_selection_applied = False\n",
        "    print(f\"\\nâš ï¸  ì „ì²´ í”¼ì²˜ ì‚¬ìš©: {X_selected.shape[1]}ê°œ\")\n",
        "\n",
        "# ============================================================\n",
        "# 2.2 ì´ˆê¸° ì„¤ì •\n",
        "# ============================================================\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# 1ì°¨ í•™ìŠµìš© (Threshold ìµœì í™”ë¥¼ ìœ„í•œ)\n",
        "results_stage1 = {\n",
        "    'lgbm': {'oof_probabilities': np.zeros(len(X_selected))},\n",
        "    'xgb': {'oof_probabilities': np.zeros(len(X_selected))},\n",
        "    'catboost': {'oof_probabilities': np.zeros(len(X_selected))},\n",
        "    'ensemble': {'oof_probabilities': np.zeros(len(X_selected))}\n",
        "}\n",
        "\n",
        "# ìµœì¢… ê²°ê³¼ ì €ì¥ìš©\n",
        "results_final = {\n",
        "    'lgbm': {'f1_scores': [], 'auc_scores': [], 'models': [],\n",
        "             'oof_predictions': np.zeros(len(X_selected)),\n",
        "             'oof_probabilities': np.zeros(len(X_selected))},\n",
        "    'xgb': {'f1_scores': [], 'auc_scores': [], 'models': [],\n",
        "            'oof_predictions': np.zeros(len(X_selected)),\n",
        "            'oof_probabilities': np.zeros(len(X_selected))},\n",
        "    'catboost': {'f1_scores': [], 'auc_scores': [], 'models': [],\n",
        "                 'oof_predictions': np.zeros(len(X_selected)),\n",
        "                 'oof_probabilities': np.zeros(len(X_selected))},\n",
        "    'ensemble': {'oof_probabilities': np.zeros(len(X_selected)),\n",
        "                 'oof_predictions': np.zeros(len(X_selected))},\n",
        "    'fold_details': []\n",
        "}\n",
        "\n",
        "print(f\"\\n[ëª¨ë¸ ì„¤ì •]\")\n",
        "print(f\"  Random State: {RANDOM_STATE}\")\n",
        "print(f\"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(X_selected):,}ê°œ\")\n",
        "print(f\"  ì‚¬ìš© í”¼ì²˜ ìˆ˜: {X_selected.shape[1]}ê°œ\")\n",
        "\n",
        "# ============================================================\n",
        "# 2.3 1ì°¨ í•™ìŠµ: Threshold ìµœì í™”ë¥¼ ìœ„í•œ í™•ë¥  ì˜ˆì¸¡\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"1ë‹¨ê³„: Threshold ìµœì í™”ë¥¼ ìœ„í•œ OOF í™•ë¥  ìˆ˜ì§‘\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_selected, y_data), 1):\n",
        "    print(f\"\\rFold {fold}/5 ì²˜ë¦¬ ì¤‘...\", end='')\n",
        "\n",
        "    X_tr, X_va = X_selected.iloc[tr_idx], X_selected.iloc[va_idx]\n",
        "    y_tr, y_va = y_data.iloc[tr_idx], y_data.iloc[va_idx]\n",
        "\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    # LightGBM\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "        num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "        colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},  # Class 0(ë…ì„±)ì— ê°€ì¤‘ì¹˜\n",
        "        random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)],\n",
        "                   callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "    results_stage1['lgbm']['oof_probabilities'][va_idx] = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "        gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,  # Class 1ì´ ë§ìœ¼ë¯€ë¡œ Class 0 ê°•í™”\n",
        "        random_state=RANDOM_STATE, n_jobs=-1,\n",
        "        early_stopping_rounds=100, eval_metric='logloss', verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)], verbose=False)\n",
        "    results_stage1['xgb']['oof_probabilities'][va_idx] = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "    # CatBoost\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000, learning_rate=0.03, depth=7,\n",
        "        l2_leaf_reg=3, class_weights=[1.5, 1.0],  # Class 0 ê°•í™”\n",
        "        random_seed=RANDOM_STATE, verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "    cat_model.fit(Xt_tr, y_tr, eval_set=(Xt_va, y_va), verbose=False)\n",
        "    results_stage1['catboost']['oof_probabilities'][va_idx] = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "print(f\"\\râœ“ 1ë‹¨ê³„ ì™„ë£Œ: 5-Fold OOF í™•ë¥  ìˆ˜ì§‘ ì™„ë£Œ\")\n",
        "\n",
        "# XGBoost ì¤‘ì‹¬ Ensemble (50% ê°€ì¤‘ì¹˜)\n",
        "results_stage1['ensemble']['oof_probabilities'] = (\n",
        "    0.25 * results_stage1['lgbm']['oof_probabilities'] +\n",
        "    0.50 * results_stage1['xgb']['oof_probabilities'] +  # XGBoost ì¦ê°€\n",
        "    0.25 * results_stage1['catboost']['oof_probabilities']\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 2.4 Threshold ìµœì í™” (FPR ê³ ë ¤)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"2ë‹¨ê³„: Threshold ìµœì í™” (FPR í˜ë„í‹° ì ìš©)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "def optimize_threshold_with_fpr(y_true, y_pred_proba, max_fpr=0.25):\n",
        "    \"\"\"FPR ì œì•½ í•˜ì—ì„œ F1 Score ìµœì í™”\"\"\"\n",
        "    thresholds = np.arange(0.1, 0.9, 0.005)\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "    best_fpr = 1.0\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "\n",
        "        # F1 Score\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "        # FPR ê³„ì‚°\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "        results_list.append({\n",
        "            'threshold': thresh,\n",
        "            'f1': f1,\n",
        "            'fpr': fpr,\n",
        "            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
        "            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        })\n",
        "\n",
        "        # FPR ì œì•½ ì¡°ê±´ ë§Œì¡±í•˜ë©´ì„œ F1 ìµœëŒ€í™”\n",
        "        if fpr <= max_fpr and f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = thresh\n",
        "            best_fpr = fpr\n",
        "\n",
        "    return best_threshold, best_f1, best_fpr, results_list\n",
        "\n",
        "# Threshold íƒìƒ‰\n",
        "print(f\"\\n[Threshold íƒìƒ‰]\")\n",
        "print(f\"  ì œì•½ ì¡°ê±´: FPR â‰¤ 25%\")\n",
        "\n",
        "optimal_threshold, optimal_f1, optimal_fpr, threshold_results = optimize_threshold_with_fpr(\n",
        "    y_data,\n",
        "    results_stage1['ensemble']['oof_probabilities'],\n",
        "    max_fpr=0.25\n",
        ")\n",
        "\n",
        "print(f\"\\n  ìµœì  Threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"  ì˜ˆìƒ F1 Score: {optimal_f1:.4f}\")\n",
        "print(f\"  ì˜ˆìƒ FPR: {optimal_fpr:.4f} ({optimal_fpr*100:.2f}%)\")\n",
        "\n",
        "# ê¸°ì¡´ thresholdì™€ ë¹„êµ\n",
        "baseline_threshold = 0.37\n",
        "baseline_pred = (results_stage1['ensemble']['oof_probabilities'] >= baseline_threshold).astype(int)\n",
        "baseline_f1 = f1_score(y_data, baseline_pred)\n",
        "baseline_cm = confusion_matrix(y_data, baseline_pred)\n",
        "baseline_fpr = baseline_cm[0,1] / (baseline_cm[0,1] + baseline_cm[0,0])\n",
        "\n",
        "print(f\"\\n[ì´ì „ vs ìµœì ]\")\n",
        "print(f\"  ì´ì „ (0.37): F1={baseline_f1:.4f}, FPR={baseline_fpr:.4f} ({baseline_fpr*100:.2f}%)\")\n",
        "print(f\"  ìµœì  ({optimal_threshold:.3f}): F1={optimal_f1:.4f}, FPR={optimal_fpr:.4f} ({optimal_fpr*100:.2f}%)\")\n",
        "print(f\"  ê°œì„ : F1 {(optimal_f1-baseline_f1)*100:+.2f}%p, FPR {(optimal_fpr-baseline_fpr)*100:+.2f}%p\")\n",
        "\n",
        "# ============================================================\n",
        "# 2.5 Sample Weighting: Low Confidence ìƒ˜í”Œ ê°€ì¤‘ì¹˜\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"3ë‹¨ê³„: Sample Weighting ì ìš©\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Low Confidence ìƒ˜í”Œ ì‹ë³„\n",
        "confidence = np.abs(results_stage1['ensemble']['oof_probabilities'] - 0.5)\n",
        "low_conf_mask = confidence < 0.15  # ì„ê³„ê°’ ì¡°ì •\n",
        "\n",
        "sample_weights = np.ones(len(X_selected))\n",
        "sample_weights[low_conf_mask] = 2.0  # Low Confidenceì— 2ë°° ê°€ì¤‘ì¹˜\n",
        "\n",
        "n_low_conf = low_conf_mask.sum()\n",
        "print(f\"\\n[Sample Weighting]\")\n",
        "print(f\"  Low Confidence ìƒ˜í”Œ: {n_low_conf}ê°œ ({n_low_conf/len(X_selected)*100:.2f}%)\")\n",
        "print(f\"  ì ìš© ê°€ì¤‘ì¹˜: 2.0ë°°\")\n",
        "\n",
        "# ============================================================\n",
        "# 2.6 ìµœì¢… í•™ìŠµ: ê°œì„ ì‚¬í•­ ëª¨ë‘ ì ìš©\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"4ë‹¨ê³„: ìµœì¢… 3-Model Ensemble í•™ìŠµ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_selected, y_data), 1):\n",
        "    print(f\"\\n{'â”€'*70}\")\n",
        "    print(f\"ğŸ“Š Fold {fold}/5\")\n",
        "    print(f\"{'â”€'*70}\")\n",
        "\n",
        "    X_tr, X_va = X_selected.iloc[tr_idx], X_selected.iloc[va_idx]\n",
        "    y_tr, y_va = y_data.iloc[tr_idx], y_data.iloc[va_idx]\n",
        "    sample_weight_tr = sample_weights[tr_idx]\n",
        "\n",
        "    print(f\"\\n[ë°ì´í„° ë¶„í• ]\")\n",
        "    print(f\"  í•™ìŠµ: {len(X_tr):,}ê°œ (Low Conf: {low_conf_mask[tr_idx].sum()}ê°œ)\")\n",
        "    print(f\"  ê²€ì¦: {len(X_va):,}ê°œ\")\n",
        "\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    # ========================================\n",
        "    # LightGBM (Class Weight + Sample Weight)\n",
        "    # ========================================\n",
        "    print(f\"\\n[1/3] LightGBM...\")\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "        num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "        colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        sample_weight=sample_weight_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "    lgbm_proba = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "    lgbm_pred = (lgbm_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "    results_final['lgbm']['models'].append(lgbm_model)\n",
        "    results_final['lgbm']['oof_probabilities'][va_idx] = lgbm_proba\n",
        "    results_final['lgbm']['oof_predictions'][va_idx] = lgbm_pred\n",
        "\n",
        "    lgbm_f1 = f1_score(y_va, lgbm_pred)\n",
        "    lgbm_auc = roc_auc_score(y_va, lgbm_proba)\n",
        "    results_final['lgbm']['f1_scores'].append(lgbm_f1)\n",
        "    results_final['lgbm']['auc_scores'].append(lgbm_auc)\n",
        "\n",
        "    print(f\"  âœ“ F1: {lgbm_f1:.4f}, AUC: {lgbm_auc:.4f}, Iter: {lgbm_model.best_iteration_}\")\n",
        "\n",
        "    # ========================================\n",
        "    # XGBoost (Scale Pos Weight + Sample Weight)\n",
        "    # ========================================\n",
        "    print(f\"\\n[2/3] XGBoost...\")\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "        gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1,\n",
        "        early_stopping_rounds=100, eval_metric='logloss', verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        sample_weight=sample_weight_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        verbose=False\n",
        "    )\n",
        "    xgb_proba = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "    xgb_pred = (xgb_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "    results_final['xgb']['models'].append(xgb_model)\n",
        "    results_final['xgb']['oof_probabilities'][va_idx] = xgb_proba\n",
        "    results_final['xgb']['oof_predictions'][va_idx] = xgb_pred\n",
        "\n",
        "    xgb_f1 = f1_score(y_va, xgb_pred)\n",
        "    xgb_auc = roc_auc_score(y_va, xgb_proba)\n",
        "    results_final['xgb']['f1_scores'].append(xgb_f1)\n",
        "    results_final['xgb']['auc_scores'].append(xgb_auc)\n",
        "\n",
        "    print(f\"  âœ“ F1: {xgb_f1:.4f}, AUC: {xgb_auc:.4f}, Iter: {xgb_model.best_iteration}\")\n",
        "\n",
        "    # ========================================\n",
        "    # CatBoost (Class Weights + Sample Weight)\n",
        "    # ========================================\n",
        "    print(f\"\\n[3/3] CatBoost...\")\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000, learning_rate=0.03, depth=7,\n",
        "        l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE, verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "    cat_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        sample_weight=sample_weight_tr,\n",
        "        eval_set=(Xt_va, y_va),\n",
        "        verbose=False\n",
        "    )\n",
        "    cat_proba = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "    cat_pred = (cat_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "    results_final['catboost']['models'].append(cat_model)\n",
        "    results_final['catboost']['oof_probabilities'][va_idx] = cat_proba\n",
        "    results_final['catboost']['oof_predictions'][va_idx] = cat_pred\n",
        "\n",
        "    cat_f1 = f1_score(y_va, cat_pred)\n",
        "    cat_auc = roc_auc_score(y_va, cat_proba)\n",
        "    results_final['catboost']['f1_scores'].append(cat_f1)\n",
        "    results_final['catboost']['auc_scores'].append(cat_auc)\n",
        "\n",
        "    print(f\"  âœ“ F1: {cat_f1:.4f}, AUC: {cat_auc:.4f}, Iter: {cat_model.best_iteration_}\")\n",
        "\n",
        "    # ========================================\n",
        "    # Ensemble: XGBoost ì¤‘ì‹¬ (50%)\n",
        "    # ========================================\n",
        "    ensemble_proba = (\n",
        "        0.25 * lgbm_proba +\n",
        "        0.50 * xgb_proba +  # XGBoost ì¦ê°€\n",
        "        0.25 * cat_proba\n",
        "    )\n",
        "    ensemble_pred = (ensemble_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "    results_final['ensemble']['oof_probabilities'][va_idx] = ensemble_proba\n",
        "    results_final['ensemble']['oof_predictions'][va_idx] = ensemble_pred\n",
        "\n",
        "    ensemble_f1 = f1_score(y_va, ensemble_pred)\n",
        "    ensemble_auc = roc_auc_score(y_va, ensemble_proba)\n",
        "\n",
        "    print(f\"\\n[Ensemble ì„±ëŠ¥]\")\n",
        "    print(f\"  F1: {ensemble_f1:.4f}, AUC: {ensemble_auc:.4f}\")\n",
        "\n",
        "    # í˜¼ë™ í–‰ë ¬\n",
        "    cm = confusion_matrix(y_va, ensemble_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "    fnr = fn / (fn + tp)\n",
        "\n",
        "    print(f\"\\n[í˜¼ë™ í–‰ë ¬]\")\n",
        "    print(f\"  TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
        "    print(f\"  FPR: {fpr:.4f} ({fpr*100:.2f}%)\")\n",
        "    print(f\"  FNR: {fnr:.4f} ({fnr*100:.2f}%)\")\n",
        "\n",
        "    # Fold ì •ë³´ ì €ì¥\n",
        "    results_final['fold_details'].append({\n",
        "        'fold': fold,\n",
        "        'lgbm_f1': lgbm_f1, 'xgb_f1': xgb_f1, 'cat_f1': cat_f1,\n",
        "        'ensemble_f1': ensemble_f1, 'ensemble_auc': ensemble_auc,\n",
        "        'fpr': fpr, 'fnr': fnr\n",
        "    })\n",
        "\n",
        "# ============================================================\n",
        "# 2.7 ìµœì¢… ê²°ê³¼ ë¶„ì„\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ìµœì¢… ê²°ê³¼\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "ensemble_oof_f1 = f1_score(y_data, results_final['ensemble']['oof_predictions'])\n",
        "ensemble_oof_auc = roc_auc_score(y_data, results_final['ensemble']['oof_probabilities'])\n",
        "\n",
        "oof_cm = confusion_matrix(y_data, results_final['ensemble']['oof_predictions'])\n",
        "tn, fp, fn, tp = oof_cm.ravel()\n",
        "final_fpr = fp / (fp + tn)\n",
        "final_fnr = fn / (fn + tp)\n",
        "final_precision = tp / (tp + fp)\n",
        "final_recall = tp / (tp + fn)\n",
        "\n",
        "print(f\"\\n[Ensemble OOF ì„±ëŠ¥]\")\n",
        "print(f\"  F1 Score:  {ensemble_oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {ensemble_oof_auc:.4f}\")\n",
        "print(f\"  Precision: {final_precision:.4f}\")\n",
        "print(f\"  Recall:    {final_recall:.4f}\")\n",
        "print(f\"  FPR:       {final_fpr:.4f} ({final_fpr*100:.2f}%)\")\n",
        "print(f\"  FNR:       {final_fnr:.4f} ({final_fnr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[OOF í˜¼ë™ í–‰ë ¬]\")\n",
        "print(f\"              ì˜ˆì¸¡: 0    ì˜ˆì¸¡: 1\")\n",
        "print(f\"  ì‹¤ì œ: 0  |   {oof_cm[0,0]:4d}      {oof_cm[0,1]:4d}\")\n",
        "print(f\"  ì‹¤ì œ: 1  |   {oof_cm[1,0]:4d}      {oof_cm[1,1]:4d}\")\n",
        "\n",
        "# ê°œì„  íš¨ê³¼\n",
        "baseline_f1 = 0.8290\n",
        "baseline_fpr_old = 0.3220\n",
        "\n",
        "print(f\"\\n[ê°œì„  íš¨ê³¼]\")\n",
        "print(f\"{'ì§€í‘œ':<15} {'ì´ì „':<10} {'í˜„ì¬':<10} {'ê°œì„ ':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'F1 Score':<15} {baseline_f1:<10.4f} {ensemble_oof_f1:<10.4f} \"\n",
        "      f\"{(ensemble_oof_f1-baseline_f1)*100:+.2f}%p\")\n",
        "print(f\"{'FPR':<15} {baseline_fpr_old:<10.4f} {final_fpr:<10.4f} \"\n",
        "      f\"{(final_fpr-baseline_fpr_old)*100:+.2f}%p\")\n",
        "\n",
        "# Low Confidence ë¶„ì„\n",
        "confidence_final = np.abs(results_final['ensemble']['oof_probabilities'] - 0.5)\n",
        "low_conf_mask_final = confidence_final < 0.1\n",
        "n_low_conf_final = low_conf_mask_final.sum()\n",
        "\n",
        "if n_low_conf_final > 0:\n",
        "    low_conf_acc_final = (\n",
        "        results_final['ensemble']['oof_predictions'][low_conf_mask_final] == y_data[low_conf_mask_final]\n",
        "    ).mean()\n",
        "\n",
        "    print(f\"\\n[Low Confidence ìƒ˜í”Œ]\")\n",
        "    print(f\"  ì´ì „: 1,029ê°œ (ì •í™•ë„ 48.79%)\")\n",
        "    print(f\"  í˜„ì¬: {n_low_conf_final}ê°œ (ì •í™•ë„ {low_conf_acc_final:.4f})\")\n",
        "    print(f\"  ê°œì„ : {1029 - n_low_conf_final}ê°œ ê°ì†Œ, ì •í™•ë„ {(low_conf_acc_final - 0.4879)*100:+.2f}%p\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ“ ìµœì¢… ê°œì„  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ì „ì—­ ë³€ìˆ˜ ì €ì¥\n",
        "globals().update({\n",
        "    'final_results': results_final,\n",
        "    'optimal_threshold': optimal_threshold,\n",
        "    'threshold_results': threshold_results,\n",
        "    'X_selected': X_selected,\n",
        "    'selected_features': selected_features\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3kJPJ83O8-d",
        "outputId": "a7d55602-18cc-4e8d-f2f1-ed7e3da108f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ë‹¨ê³„ 2: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ (ìµœì¢… ê°œì„  ë²„ì „)\n",
            "======================================================================\n",
            "\n",
            "[ì ìš©ëœ ê°œì„ ì‚¬í•­]\n",
            "  1. âœ“ Threshold ì¬ìµœì í™” (OOF ê¸°ë°˜ ìë™ íƒìƒ‰)\n",
            "  2. âœ“ XGBoost ì¤‘ì‹¬ Ensemble (50% ê°€ì¤‘ì¹˜)\n",
            "  3. âœ“ Class Weighting (FP í˜ë„í‹° ì¦ê°€)\n",
            "  4. âœ“ Sample Weighting (Low Confidence ìƒ˜í”Œ ì§‘ì¤‘)\n",
            "\n",
            "======================================================================\n",
            "Feature Selection\n",
            "======================================================================\n",
            "\n",
            "[Feature Selection ê²°ê³¼]\n",
            "  ì›ë³¸ í”¼ì²˜: 285ê°œ\n",
            "  ì‹¤ì œ ì‚¬ìš©: 285ê°œ\n",
            "\n",
            "[ëª¨ë¸ ì„¤ì •]\n",
            "  Random State: 42\n",
            "  ì „ì²´ ìƒ˜í”Œ ìˆ˜: 8,349ê°œ\n",
            "  ì‚¬ìš© í”¼ì²˜ ìˆ˜: 285ê°œ\n",
            "\n",
            "======================================================================\n",
            "1ë‹¨ê³„: Threshold ìµœì í™”ë¥¼ ìœ„í•œ OOF í™•ë¥  ìˆ˜ì§‘\n",
            "======================================================================\n",
            "âœ“ 1ë‹¨ê³„ ì™„ë£Œ: 5-Fold OOF í™•ë¥  ìˆ˜ì§‘ ì™„ë£Œ\n",
            "\n",
            "======================================================================\n",
            "2ë‹¨ê³„: Threshold ìµœì í™” (FPR í˜ë„í‹° ì ìš©)\n",
            "======================================================================\n",
            "\n",
            "[Threshold íƒìƒ‰]\n",
            "  ì œì•½ ì¡°ê±´: FPR â‰¤ 25%\n",
            "\n",
            "  ìµœì  Threshold: 0.385\n",
            "  ì˜ˆìƒ F1 Score: 0.8314\n",
            "  ì˜ˆìƒ FPR: 0.2495 (24.95%)\n",
            "\n",
            "[ì´ì „ vs ìµœì ]\n",
            "  ì´ì „ (0.37): F1=0.8301, FPR=0.2648 (26.48%)\n",
            "  ìµœì  (0.385): F1=0.8314, FPR=0.2495 (24.95%)\n",
            "  ê°œì„ : F1 +0.13%p, FPR -1.52%p\n",
            "\n",
            "======================================================================\n",
            "3ë‹¨ê³„: Sample Weighting ì ìš©\n",
            "======================================================================\n",
            "\n",
            "[Sample Weighting]\n",
            "  Low Confidence ìƒ˜í”Œ: 1521ê°œ (18.22%)\n",
            "  ì ìš© ê°€ì¤‘ì¹˜: 2.0ë°°\n",
            "\n",
            "======================================================================\n",
            "4ë‹¨ê³„: ìµœì¢… 3-Model Ensemble í•™ìŠµ\n",
            "======================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 1/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "[ë°ì´í„° ë¶„í• ]\n",
            "  í•™ìŠµ: 6,679ê°œ (Low Conf: 1227ê°œ)\n",
            "  ê²€ì¦: 1,670ê°œ\n",
            "\n",
            "[1/3] LightGBM...\n",
            "  âœ“ F1: 0.8381, AUC: 0.8991, Iter: 834\n",
            "\n",
            "[2/3] XGBoost...\n",
            "  âœ“ F1: 0.8452, AUC: 0.9041, Iter: 860\n",
            "\n",
            "[3/3] CatBoost...\n",
            "  âœ“ F1: 0.8447, AUC: 0.8968, Iter: 999\n",
            "\n",
            "[Ensemble ì„±ëŠ¥]\n",
            "  F1: 0.8446, AUC: 0.9032\n",
            "\n",
            "[í˜¼ë™ í–‰ë ¬]\n",
            "  TN: 581, FP: 180, FN: 113, TP: 796\n",
            "  FPR: 0.2365 (23.65%)\n",
            "  FNR: 0.1243 (12.43%)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 2/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "[ë°ì´í„° ë¶„í• ]\n",
            "  í•™ìŠµ: 6,679ê°œ (Low Conf: 1212ê°œ)\n",
            "  ê²€ì¦: 1,670ê°œ\n",
            "\n",
            "[1/3] LightGBM...\n",
            "  âœ“ F1: 0.8271, AUC: 0.8816, Iter: 694\n",
            "\n",
            "[2/3] XGBoost...\n",
            "  âœ“ F1: 0.8282, AUC: 0.8847, Iter: 753\n",
            "\n",
            "[3/3] CatBoost...\n",
            "  âœ“ F1: 0.8277, AUC: 0.8770, Iter: 997\n",
            "\n",
            "[Ensemble ì„±ëŠ¥]\n",
            "  F1: 0.8282, AUC: 0.8843\n",
            "\n",
            "[í˜¼ë™ í–‰ë ¬]\n",
            "  TN: 558, FP: 203, FN: 123, TP: 786\n",
            "  FPR: 0.2668 (26.68%)\n",
            "  FNR: 0.1353 (13.53%)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 3/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "[ë°ì´í„° ë¶„í• ]\n",
            "  í•™ìŠµ: 6,679ê°œ (Low Conf: 1209ê°œ)\n",
            "  ê²€ì¦: 1,670ê°œ\n",
            "\n",
            "[1/3] LightGBM...\n",
            "  âœ“ F1: 0.8119, AUC: 0.8708, Iter: 521\n",
            "\n",
            "[2/3] XGBoost...\n",
            "  âœ“ F1: 0.8080, AUC: 0.8687, Iter: 600\n",
            "\n",
            "[3/3] CatBoost...\n",
            "  âœ“ F1: 0.8101, AUC: 0.8668, Iter: 990\n",
            "\n",
            "[Ensemble ì„±ëŠ¥]\n",
            "  F1: 0.8080, AUC: 0.8707\n",
            "\n",
            "[í˜¼ë™ í–‰ë ¬]\n",
            "  TN: 565, FP: 197, FN: 159, TP: 749\n",
            "  FPR: 0.2585 (25.85%)\n",
            "  FNR: 0.1751 (17.51%)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 4/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "[ë°ì´í„° ë¶„í• ]\n",
            "  í•™ìŠµ: 6,679ê°œ (Low Conf: 1194ê°œ)\n",
            "  ê²€ì¦: 1,670ê°œ\n",
            "\n",
            "[1/3] LightGBM...\n",
            "  âœ“ F1: 0.8315, AUC: 0.8889, Iter: 668\n",
            "\n",
            "[2/3] XGBoost...\n",
            "  âœ“ F1: 0.8323, AUC: 0.8909, Iter: 703\n",
            "\n",
            "[3/3] CatBoost...\n",
            "  âœ“ F1: 0.8308, AUC: 0.8867, Iter: 998\n",
            "\n",
            "[Ensemble ì„±ëŠ¥]\n",
            "  F1: 0.8340, AUC: 0.8915\n",
            "\n",
            "[í˜¼ë™ í–‰ë ¬]\n",
            "  TN: 567, FP: 195, FN: 119, TP: 789\n",
            "  FPR: 0.2559 (25.59%)\n",
            "  FNR: 0.1311 (13.11%)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 5/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "[ë°ì´í„° ë¶„í• ]\n",
            "  í•™ìŠµ: 6,680ê°œ (Low Conf: 1242ê°œ)\n",
            "  ê²€ì¦: 1,669ê°œ\n",
            "\n",
            "[1/3] LightGBM...\n",
            "  âœ“ F1: 0.8429, AUC: 0.8971, Iter: 776\n",
            "\n",
            "[2/3] XGBoost...\n",
            "  âœ“ F1: 0.8401, AUC: 0.8979, Iter: 860\n",
            "\n",
            "[3/3] CatBoost...\n",
            "  âœ“ F1: 0.8381, AUC: 0.8941, Iter: 999\n",
            "\n",
            "[Ensemble ì„±ëŠ¥]\n",
            "  F1: 0.8435, AUC: 0.8992\n",
            "\n",
            "[í˜¼ë™ í–‰ë ¬]\n",
            "  TN: 579, FP: 182, FN: 113, TP: 795\n",
            "  FPR: 0.2392 (23.92%)\n",
            "  FNR: 0.1244 (12.44%)\n",
            "\n",
            "======================================================================\n",
            "ìµœì¢… ê²°ê³¼\n",
            "======================================================================\n",
            "\n",
            "[Ensemble OOF ì„±ëŠ¥]\n",
            "  F1 Score:  0.8317\n",
            "  AUC Score: 0.8898\n",
            "  Precision: 0.8036\n",
            "  Recall:    0.8620\n",
            "  FPR:       0.2514 (25.14%)\n",
            "  FNR:       0.1380 (13.80%)\n",
            "\n",
            "[OOF í˜¼ë™ í–‰ë ¬]\n",
            "              ì˜ˆì¸¡: 0    ì˜ˆì¸¡: 1\n",
            "  ì‹¤ì œ: 0  |   2850       957\n",
            "  ì‹¤ì œ: 1  |    627      3915\n",
            "\n",
            "[ê°œì„  íš¨ê³¼]\n",
            "ì§€í‘œ              ì´ì „         í˜„ì¬         ê°œì„              \n",
            "-------------------------------------------------------\n",
            "F1 Score        0.8290     0.8317     +0.27%p\n",
            "FPR             0.3220     0.2514     -7.06%p\n",
            "\n",
            "[Low Confidence ìƒ˜í”Œ]\n",
            "  ì´ì „: 1,029ê°œ (ì •í™•ë„ 48.79%)\n",
            "  í˜„ì¬: 941ê°œ (ì •í™•ë„ 0.5739)\n",
            "  ê°œì„ : 88ê°œ ê°ì†Œ, ì •í™•ë„ +8.60%p\n",
            "\n",
            "======================================================================\n",
            "âœ“ ìµœì¢… ê°œì„  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ë‹¨ê³„ 3: íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ (ìµœì¢… ê°œì„  ë²„ì „)\n",
        "# ============================================================\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ë‹¨ê³„ 3: íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ (3-Model Ensemble ê¸°ë°˜)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[ë¶„ì„ ë°©ë²•]\")\n",
        "print(\"  1. âœ“ 3ê°œ ëª¨ë¸(LGBM, XGB, CAT)ì˜ Feature Importance í†µí•©\")\n",
        "print(\"  2. âœ“ 5-Fold Cross-Validation ê¸°ë°˜\")\n",
        "print(\"  3. âœ“ SHAP ê°’ ë¶„ì„ (ìƒìœ„ í”¼ì²˜)\")\n",
        "print(\"  4. âœ“ ì•ˆì •ì„± ë¶„ì„ (í‘œì¤€í¸ì°¨)\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.1 ë°ì´í„° ì¤€ë¹„\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ë°ì´í„° ì¤€ë¹„\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ì›ë³¸ ë°ì´í„° ë¡œë“œ (dfê°€ ì—†ìœ¼ë©´)\n",
        "try:\n",
        "    df = pd.read_csv('train.csv')\n",
        "except:\n",
        "    print(\"âš ï¸  train.csv íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ì»¬ëŸ¼ ê·¸ë£¹ ì •ì˜\n",
        "fp_cols = [col for col in df.columns if col.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "label_col = 'label'\n",
        "\n",
        "# Feature names (ì „ì²˜ë¦¬ í›„ ìˆœì„œ)\n",
        "feature_names = fp_cols + desc_cols\n",
        "\n",
        "print(f\"\\n[í”¼ì²˜ êµ¬ì„±]\")\n",
        "print(f\"  ì „ì²´ í”¼ì²˜: {len(feature_names)}ê°œ\")\n",
        "print(f\"  - Fingerprint: {len(fp_cols)}ê°œ\")\n",
        "print(f\"    Â· ECFP: {len([c for c in fp_cols if c.startswith('ecfp_')])}ê°œ\")\n",
        "print(f\"    Â· FCFP: {len([c for c in fp_cols if c.startswith('fcfp_')])}ê°œ\")\n",
        "print(f\"    Â· PTFP: {len([c for c in fp_cols if c.startswith('ptfp_')])}ê°œ\")\n",
        "print(f\"  - Descriptor: {len(desc_cols)}ê°œ\")\n",
        "\n",
        "# X, y ë¶„ë¦¬\n",
        "X = df.drop(columns=[label_col])\n",
        "y = df[label_col].astype(int)\n",
        "\n",
        "# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# êµì°¨ê²€ì¦ ì„¤ì •\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ============================================================\n",
        "# 3.2 3-Model Ensemble Feature Importance ê³„ì‚°\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"3-Model Ensemble Feature Importance ê³„ì‚°\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ìš©\n",
        "importance_results = {\n",
        "    'lgbm': {'fold_importances': [], 'models': []},\n",
        "    'xgb': {'fold_importances': [], 'models': []},\n",
        "    'catboost': {'fold_importances': [], 'models': []},\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "    print(f\"\\rFold {fold}/5 ì²˜ë¦¬ ì¤‘...\", end='')\n",
        "\n",
        "    X_tr = X.iloc[tr_idx]\n",
        "    y_tr = y.iloc[tr_idx]\n",
        "\n",
        "    # ì „ì²˜ë¦¬\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "\n",
        "    # ========================================\n",
        "    # LightGBM\n",
        "    # ========================================\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "        num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "        colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=42, n_jobs=-1, verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(Xt_tr, y_tr)\n",
        "\n",
        "    # Gain ê¸°ë°˜ ì¤‘ìš”ë„\n",
        "    lgbm_importances = lgbm_model.booster_.feature_importance(importance_type='gain')\n",
        "    importance_results['lgbm']['fold_importances'].append(lgbm_importances)\n",
        "    importance_results['lgbm']['models'].append(lgbm_model)\n",
        "\n",
        "    # ========================================\n",
        "    # XGBoost\n",
        "    # ========================================\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "        gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=42, n_jobs=-1, verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(Xt_tr, y_tr)\n",
        "\n",
        "    # Gain ê¸°ë°˜ ì¤‘ìš”ë„\n",
        "    xgb_importances = xgb_model.feature_importances_\n",
        "    importance_results['xgb']['fold_importances'].append(xgb_importances)\n",
        "    importance_results['xgb']['models'].append(xgb_model)\n",
        "\n",
        "    # ========================================\n",
        "    # CatBoost\n",
        "    # ========================================\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000, learning_rate=0.03, depth=7,\n",
        "        l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "        random_seed=42, verbose=0\n",
        "    )\n",
        "    cat_model.fit(Xt_tr, y_tr)\n",
        "\n",
        "    # Feature importance\n",
        "    cat_importances = cat_model.get_feature_importance()\n",
        "    importance_results['catboost']['fold_importances'].append(cat_importances)\n",
        "    importance_results['catboost']['models'].append(cat_model)\n",
        "\n",
        "print(f\"\\râœ“ 5-Fold Feature Importance ê³„ì‚° ì™„ë£Œ\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.3 ëª¨ë¸ë³„ ì¤‘ìš”ë„ í†µê³„ ë° Ensemble\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ëª¨ë¸ë³„ Feature Importance í†µí•©\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ê° ëª¨ë¸ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨\n",
        "lgbm_mean = np.mean(importance_results['lgbm']['fold_importances'], axis=0)\n",
        "lgbm_std = np.std(importance_results['lgbm']['fold_importances'], axis=0)\n",
        "\n",
        "xgb_mean = np.mean(importance_results['xgb']['fold_importances'], axis=0)\n",
        "xgb_std = np.std(importance_results['xgb']['fold_importances'], axis=0)\n",
        "\n",
        "cat_mean = np.mean(importance_results['catboost']['fold_importances'], axis=0)\n",
        "cat_std = np.std(importance_results['catboost']['fold_importances'], axis=0)\n",
        "\n",
        "# Ensemble Importance (XGBoost ì¤‘ì‹¬: 50%)\n",
        "ensemble_mean = 0.25 * lgbm_mean + 0.50 * xgb_mean + 0.25 * cat_mean\n",
        "ensemble_std = np.sqrt(\n",
        "    (0.25 * lgbm_std)**2 +\n",
        "    (0.50 * xgb_std)**2 +\n",
        "    (0.25 * cat_std)**2\n",
        ")\n",
        "\n",
        "# DataFrame ìƒì„±\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'ensemble_mean': ensemble_mean,\n",
        "    'ensemble_std': ensemble_std,\n",
        "    'lgbm_mean': lgbm_mean,\n",
        "    'lgbm_std': lgbm_std,\n",
        "    'xgb_mean': xgb_mean,\n",
        "    'xgb_std': xgb_std,\n",
        "    'cat_mean': cat_mean,\n",
        "    'cat_std': cat_std,\n",
        "    'cv_coefficient': ensemble_std / (ensemble_mean + 1e-10)  # ë³€ë™ê³„ìˆ˜\n",
        "}).sort_values('ensemble_mean', ascending=False)\n",
        "\n",
        "print(f\"\\n[ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜ (Ensemble ê¸°ì¤€)]\")\n",
        "print(importance_df.head(20)[['feature', 'ensemble_mean', 'ensemble_std', 'cv_coefficient']].to_string(index=False))\n",
        "\n",
        "# ============================================================\n",
        "# 3.4 ë¬¼ì„± Descriptor ë¶„ì„\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ë¬¼ì„± Descriptor ì¤‘ìš”ë„ ë¶„ì„\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "desc_importance = importance_df[importance_df['feature'].isin(desc_cols)].copy()\n",
        "desc_importance = desc_importance.sort_values('ensemble_mean', ascending=False)\n",
        "\n",
        "print(f\"\\n[Descriptor ì¤‘ìš”ë„ ìˆœìœ„]\")\n",
        "for idx, row in desc_importance.iterrows():\n",
        "    rank = importance_df.index.get_loc(idx) + 1\n",
        "    print(f\"  {rank:3d}ìœ„. {row['feature']:<12} : {row['ensemble_mean']:>10.2f} Â± {row['ensemble_std']:>6.2f}\")\n",
        "\n",
        "# Descriptor ê°„ ìƒëŒ€ ë¹„ìœ¨\n",
        "total_desc_importance = desc_importance['ensemble_mean'].sum()\n",
        "print(f\"\\n[Descriptor ìƒëŒ€ ê¸°ì—¬ë„]\")\n",
        "for idx, row in desc_importance.iterrows():\n",
        "    ratio = row['ensemble_mean'] / total_desc_importance * 100\n",
        "    print(f\"  {row['feature']:<12} : {ratio:5.1f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.5 Fingerprint íƒ€ì…ë³„ ë¶„ì„\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Fingerprint íƒ€ì…ë³„ ì¤‘ìš”ë„ ë¶„ì„\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fp_importance = importance_df[importance_df['feature'].isin(fp_cols)].copy()\n",
        "\n",
        "# íƒ€ì…ë³„ í†µê³„\n",
        "fp_types = {\n",
        "    'ECFP': [f for f in fp_cols if f.startswith('ecfp_')],\n",
        "    'FCFP': [f for f in fp_cols if f.startswith('fcfp_')],\n",
        "    'PTFP': [f for f in fp_cols if f.startswith('ptfp_')]\n",
        "}\n",
        "\n",
        "print(f\"\\n[Fingerprint íƒ€ì…ë³„ í†µê³„]\")\n",
        "print(f\"{'íƒ€ì…':<8} {'í‰ê·  ì¤‘ìš”ë„':<15} {'Top 10 ê°œìˆ˜':<12} {'Top 50 ê°œìˆ˜':<12}\")\n",
        "print(f\"{'-'*50}\")\n",
        "\n",
        "top10_features = importance_df.head(10)['feature'].tolist()\n",
        "top50_features = importance_df.head(50)['feature'].tolist()\n",
        "\n",
        "for fp_type, fp_list in fp_types.items():\n",
        "    type_importance = importance_df[importance_df['feature'].isin(fp_list)]\n",
        "    avg_importance = type_importance['ensemble_mean'].mean()\n",
        "    count_top10 = sum(1 for f in fp_list if f in top10_features)\n",
        "    count_top50 = sum(1 for f in fp_list if f in top50_features)\n",
        "\n",
        "    print(f\"{fp_type:<8} {avg_importance:<15.2f} {count_top10:<12} {count_top50:<12}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.6 Feature Selection ì „ëµ\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Feature Selection ì „ëµ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ì „ëµ 1: ìƒìœ„ Nê°œ ì„ íƒ\n",
        "top_n_options = [50, 100, 150, 200]\n",
        "print(f\"\\n[ì „ëµ 1: ìƒìœ„ Nê°œ ì„ íƒ]\")\n",
        "print(f\"{'N':<8} {'ëˆ„ì  ì¤‘ìš”ë„':<15} {'ì••ì¶•ë¥ ':<10}\")\n",
        "print(f\"{'-'*35}\")\n",
        "\n",
        "total_importance = importance_df['ensemble_mean'].sum()\n",
        "for n in top_n_options:\n",
        "    cumsum = importance_df.head(n)['ensemble_mean'].sum()\n",
        "    ratio = cumsum / total_importance * 100\n",
        "    compression = n / len(feature_names) * 100\n",
        "    print(f\"{n:<8} {ratio:<15.2f}% {compression:<10.1f}%\")\n",
        "\n",
        "# ì „ëµ 2: ì„ê³„ê°’ ê¸°ë°˜ ì„ íƒ\n",
        "threshold_options = [10, 20, 30, 50, 100]\n",
        "print(f\"\\n[ì „ëµ 2: ì„ê³„ê°’ ê¸°ë°˜ ì„ íƒ]\")\n",
        "print(f\"{'ì„ê³„ê°’':<10} {'ì„ íƒ í”¼ì²˜ ìˆ˜':<15} {'ì••ì¶•ë¥ ':<10}\")\n",
        "print(f\"{'-'*35}\")\n",
        "\n",
        "for thresh in threshold_options:\n",
        "    selected = importance_df[importance_df['ensemble_mean'] >= thresh]\n",
        "    count = len(selected)\n",
        "    compression = count / len(feature_names) * 100\n",
        "    print(f\"{thresh:<10} {count:<15} {compression:<10.1f}%\")\n",
        "\n",
        "# ì „ëµ 3: ëˆ„ì  ê¸°ì—¬ë„ ê¸°ë°˜\n",
        "cumsum_options = [80, 90, 95, 99]\n",
        "print(f\"\\n[ì „ëµ 3: ëˆ„ì  ê¸°ì—¬ë„ ê¸°ë°˜]\")\n",
        "print(f\"{'ëˆ„ì  %':<10} {'í•„ìš” í”¼ì²˜ ìˆ˜':<15} {'ì••ì¶•ë¥ ':<10}\")\n",
        "print(f\"{'-'*35}\")\n",
        "\n",
        "importance_df['cumsum'] = importance_df['ensemble_mean'].cumsum()\n",
        "importance_df['cumsum_pct'] = importance_df['cumsum'] / total_importance * 100\n",
        "\n",
        "for target_pct in cumsum_options:\n",
        "    needed = len(importance_df[importance_df['cumsum_pct'] <= target_pct]) + 1\n",
        "    compression = needed / len(feature_names) * 100\n",
        "    print(f\"{target_pct:<10} {needed:<15} {compression:<10.1f}%\")\n",
        "\n",
        "# ê¶Œì¥ ì„¤ì •\n",
        "recommended_n = 150  # ìƒìœ„ 150ê°œ\n",
        "recommended_features = importance_df.head(recommended_n)['feature'].tolist()\n",
        "\n",
        "print(f\"\\n[ê¶Œì¥ ì„¤ì •]\")\n",
        "print(f\"  ì„ íƒ í”¼ì²˜: ìƒìœ„ {recommended_n}ê°œ\")\n",
        "print(f\"  ëˆ„ì  ì¤‘ìš”ë„: {importance_df.head(recommended_n)['ensemble_mean'].sum() / total_importance * 100:.1f}%\")\n",
        "print(f\"  ì••ì¶•ë¥ : {recommended_n / len(feature_names) * 100:.1f}%\")\n",
        "print(f\"  - Fingerprint: {len([f for f in recommended_features if f in fp_cols])}ê°œ\")\n",
        "print(f\"  - Descriptor: {len([f for f in recommended_features if f in desc_cols])}ê°œ\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.7 ì•ˆì •ì„± ë¶„ì„\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Feature Importance ì•ˆì •ì„± ë¶„ì„\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ë³€ë™ê³„ìˆ˜(CV) ë¶„ì„\n",
        "high_cv = importance_df[importance_df['cv_coefficient'] > 0.5].sort_values('cv_coefficient', ascending=False)\n",
        "\n",
        "print(f\"\\n[ë†’ì€ ë³€ë™ì„± í”¼ì²˜ (CV > 0.5)]\")\n",
        "print(f\"  ë¶ˆì•ˆì •í•œ í”¼ì²˜: {len(high_cv)}ê°œ\")\n",
        "if len(high_cv) > 0:\n",
        "    print(f\"\\n  ìƒìœ„ 10ê°œ:\")\n",
        "    print(high_cv.head(10)[['feature', 'ensemble_mean', 'ensemble_std', 'cv_coefficient']].to_string(index=False))\n",
        "\n",
        "# ì•ˆì •ì ì¸ ìƒìœ„ í”¼ì²˜\n",
        "stable_important = importance_df[\n",
        "    (importance_df['ensemble_mean'] >= 50) &\n",
        "    (importance_df['cv_coefficient'] <= 0.3)\n",
        "]\n",
        "\n",
        "print(f\"\\n[ì•ˆì •ì ì´ë©´ì„œ ì¤‘ìš”í•œ í”¼ì²˜ (Importance â‰¥ 50, CV â‰¤ 0.3)]\")\n",
        "print(f\"  ê°œìˆ˜: {len(stable_important)}ê°œ\")\n",
        "if len(stable_important) > 0:\n",
        "    print(f\"\\n  Top 10:\")\n",
        "    print(stable_important.head(10)[['feature', 'ensemble_mean', 'cv_coefficient']].to_string(index=False))\n",
        "\n",
        "# ============================================================\n",
        "# 3.8 SHAP ë¶„ì„ (ìƒìœ„ í”¼ì²˜)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"SHAP ê°’ ë¶„ì„ (ìƒìœ„ 20ê°œ í”¼ì²˜)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ì²« ë²ˆì§¸ Foldì˜ ì²« ë²ˆì§¸ ëª¨ë¸(LightGBM) ì‚¬ìš©\n",
        "sample_model = importance_results['lgbm']['models'][0]\n",
        "X_sample = X.iloc[:1000]  # ìƒ˜í”Œ 1000ê°œë§Œ (ì†ë„)\n",
        "Xt_sample = preprocessor.fit_transform(X_sample)\n",
        "\n",
        "# ìƒìœ„ 20ê°œ í”¼ì²˜ë§Œ ì„ íƒ\n",
        "top20_indices = importance_df.head(20).index.tolist()\n",
        "top20_features = importance_df.head(20)['feature'].tolist()\n",
        "\n",
        "print(f\"\\n  ìƒ˜í”Œ ìˆ˜: {len(X_sample)}ê°œ\")\n",
        "print(f\"  ë¶„ì„ í”¼ì²˜: ìƒìœ„ 20ê°œ\")\n",
        "\n",
        "try:\n",
        "    # SHAP Explainer\n",
        "    explainer = shap.TreeExplainer(sample_model)\n",
        "    shap_values = explainer.shap_values(Xt_sample)\n",
        "\n",
        "    # Class 1 (ë¬´ë…ì„±)ì— ëŒ€í•œ SHAP ê°’\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values_class1 = shap_values[1]\n",
        "    else:\n",
        "        shap_values_class1 = shap_values\n",
        "\n",
        "    # ìƒìœ„ 20ê°œë§Œ ì¶”ì¶œ\n",
        "    shap_values_top20 = shap_values_class1[:, top20_indices]\n",
        "\n",
        "    # SHAP í‰ê·  ì ˆëŒ€ê°’\n",
        "    shap_mean_abs = np.abs(shap_values_top20).mean(axis=0)\n",
        "\n",
        "    print(f\"\\n[SHAP í‰ê·  ì ˆëŒ€ê°’ (ìƒìœ„ 10ê°œ)]\")\n",
        "    shap_df = pd.DataFrame({\n",
        "        'feature': top20_features,\n",
        "        'shap_mean_abs': shap_mean_abs\n",
        "    }).sort_values('shap_mean_abs', ascending=False)\n",
        "\n",
        "    print(shap_df.head(10).to_string(index=False))\n",
        "\n",
        "    shap_available = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸  SHAP ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
        "    print(\"  â†’ Feature Importance ë¶„ì„ì€ ì •ìƒ ì™„ë£Œë¨\")\n",
        "    shap_available = False\n",
        "\n",
        "# ============================================================\n",
        "# 3.9 ì‹œê°í™”\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ì‹œê°í™” ìƒì„±\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "# 1. Top 20 Feature Importance (Ensemble)\n",
        "ax = axes[0, 0]\n",
        "top20 = importance_df.head(20)\n",
        "colors = ['#1f77b4' if f in desc_cols else '#ff7f0e' for f in top20['feature']]\n",
        "ax.barh(range(len(top20)), top20['ensemble_mean'], color=colors, alpha=0.7, edgecolor='black')\n",
        "ax.set_yticks(range(len(top20)))\n",
        "ax.set_yticklabels(top20['feature'], fontsize=9)\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Ensemble Importance (Gain)', fontsize=11)\n",
        "ax.set_title('Top 20 Feature Importance (Ensemble)', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# ë²”ë¡€\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#1f77b4', label='Descriptor'),\n",
        "    Patch(facecolor='#ff7f0e', label='Fingerprint')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "# 2. ëª¨ë¸ë³„ ë¹„êµ (ìƒìœ„ 10ê°œ)\n",
        "ax = axes[0, 1]\n",
        "top10 = importance_df.head(10)\n",
        "x = np.arange(len(top10))\n",
        "width = 0.25\n",
        "\n",
        "ax.barh(x - width, top10['lgbm_mean'], width, label='LightGBM', alpha=0.8)\n",
        "ax.barh(x, top10['xgb_mean'], width, label='XGBoost', alpha=0.8)\n",
        "ax.barh(x + width, top10['cat_mean'], width, label='CatBoost', alpha=0.8)\n",
        "\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(top10['feature'], fontsize=9)\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Importance (Gain)', fontsize=11)\n",
        "ax.set_title('Top 10 Features by Model', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 3. ì•ˆì •ì„± ë¶„ì„ (ìƒìœ„ 30ê°œ)\n",
        "ax = axes[0, 2]\n",
        "top30 = importance_df.head(30)\n",
        "scatter = ax.scatter(\n",
        "    top30['ensemble_mean'],\n",
        "    top30['cv_coefficient'],\n",
        "    s=100, alpha=0.6, c=range(len(top30)), cmap='viridis',\n",
        "    edgecolors='black', linewidth=1\n",
        ")\n",
        "ax.axhline(y=0.3, color='r', linestyle='--', label='Stability Threshold (0.3)')\n",
        "ax.set_xlabel('Ensemble Importance', fontsize=11)\n",
        "ax.set_ylabel('Coefficient of Variation (CV)', fontsize=11)\n",
        "ax.set_title('Importance vs Stability (Top 30)', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# ìƒìœ„ 5ê°œ ë¼ë²¨ë§\n",
        "for idx in range(min(5, len(top30))):\n",
        "    row = top30.iloc[idx]\n",
        "    ax.annotate(\n",
        "        row['feature'],\n",
        "        (row['ensemble_mean'], row['cv_coefficient']),\n",
        "        fontsize=8, alpha=0.7,\n",
        "        xytext=(5, 5), textcoords='offset points'\n",
        "    )\n",
        "\n",
        "# 4. ëˆ„ì  ì¤‘ìš”ë„\n",
        "ax = axes[1, 0]\n",
        "cumsum_pct = importance_df['cumsum_pct'].values\n",
        "ax.plot(range(1, len(cumsum_pct)+1), cumsum_pct, linewidth=2)\n",
        "ax.axhline(y=90, color='r', linestyle='--', label='90%')\n",
        "ax.axhline(y=95, color='orange', linestyle='--', label='95%')\n",
        "ax.axvline(x=recommended_n, color='g', linestyle='--', label=f'Top {recommended_n}')\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('Cumulative Importance (%)', fontsize=11)\n",
        "ax.set_title('Cumulative Feature Importance', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim([0, 500])\n",
        "\n",
        "# 5. Fingerprint íƒ€ì…ë³„ ë¶„í¬\n",
        "ax = axes[1, 1]\n",
        "fp_type_data = []\n",
        "fp_type_labels = []\n",
        "for fp_type, fp_list in fp_types.items():\n",
        "    type_importance = importance_df[importance_df['feature'].isin(fp_list)]['ensemble_mean'].values\n",
        "    fp_type_data.append(type_importance)\n",
        "    fp_type_labels.append(fp_type)\n",
        "\n",
        "bp = ax.boxplot(fp_type_data, labels=fp_type_labels, patch_artist=True)\n",
        "for patch, color in zip(bp['boxes'], ['lightblue', 'lightgreen', 'lightcoral']):\n",
        "    patch.set_facecolor(color)\n",
        "ax.set_ylabel('Ensemble Importance', fontsize=11)\n",
        "ax.set_title('Fingerprint Type Distribution', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 6. Descriptor ì¤‘ìš”ë„\n",
        "ax = axes[1, 2]\n",
        "desc_data = desc_importance[['feature', 'ensemble_mean', 'ensemble_std']].copy()\n",
        "x_pos = np.arange(len(desc_data))\n",
        "ax.bar(x_pos, desc_data['ensemble_mean'], yerr=desc_data['ensemble_std'],\n",
        "       alpha=0.7, capsize=5, edgecolor='black', color='steelblue')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(desc_data['feature'], fontsize=10, rotation=0)\n",
        "ax.set_ylabel('Ensemble Importance (Gain)', fontsize=11)\n",
        "ax.set_title('Descriptor Importance', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_comprehensive.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"\\nâœ“ ì¢…í•© ì‹œê°í™” ì €ì¥: feature_importance_comprehensive.png\")\n",
        "\n",
        "# ìƒìœ„ 20ê°œë§Œ ë³„ë„ ì €ì¥\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "top20 = importance_df.head(20)\n",
        "colors = ['#2E86AB' if f in desc_cols else '#A23B72' for f in top20['feature']]\n",
        "bars = ax.barh(range(len(top20)), top20['ensemble_mean'],\n",
        "               xerr=top20['ensemble_std'], color=colors, alpha=0.8,\n",
        "               edgecolor='black', linewidth=1.5, capsize=3)\n",
        "ax.set_yticks(range(len(top20)))\n",
        "ax.set_yticklabels(top20['feature'], fontsize=11, fontweight='bold')\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Ensemble Importance (Gain)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Top 20 Feature Importance\\n(3-Model Ensemble, 5-Fold CV)',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# ë²”ë¡€\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2E86AB', label='Descriptor (ë¬¼ì„±)'),\n",
        "    Patch(facecolor='#A23B72', label='Fingerprint (êµ¬ì¡°)')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_top20.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ“ Top 20 ì‹œê°í™” ì €ì¥: feature_importance_top20.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.10 ê²°ê³¼ ì €ì¥\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ê²°ê³¼ ì €ì¥\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ì „ì²´ ê²°ê³¼ ì €ì¥\n",
        "importance_df.to_csv('feature_importance_ensemble_cv.csv', index=False)\n",
        "print(f\"âœ“ ì „ì²´ Feature Importance ì €ì¥: feature_importance_ensemble_cv.csv\")\n",
        "\n",
        "# ê¶Œì¥ í”¼ì²˜ ì €ì¥\n",
        "recommended_df = importance_df.head(recommended_n)[['feature', 'ensemble_mean', 'ensemble_std']]\n",
        "recommended_df.to_csv('selected_features_top150.csv', index=False)\n",
        "print(f\"âœ“ ê¶Œì¥ í”¼ì²˜ ì €ì¥ (Top {recommended_n}): selected_features_top150.csv\")\n",
        "\n",
        "# ì•ˆì •ì  í”¼ì²˜ ì €ì¥\n",
        "if len(stable_important) > 0:\n",
        "    stable_important[['feature', 'ensemble_mean', 'cv_coefficient']].to_csv(\n",
        "        'stable_important_features.csv', index=False\n",
        "    )\n",
        "    print(f\"âœ“ ì•ˆì •ì  ì¤‘ìš” í”¼ì²˜ ì €ì¥: stable_important_features.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ“ ë‹¨ê³„ 3 ì™„ë£Œ - Feature Importance ë¶„ì„ ì™„ë£Œ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ì „ì—­ ë³€ìˆ˜ ì €ì¥\n",
        "globals().update({\n",
        "    'feature_importance_df': importance_df,\n",
        "    'recommended_features': recommended_features,\n",
        "    'stable_important_features': stable_important['feature'].tolist() if len(stable_important) > 0 else []\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL5qin1LXkZU",
        "outputId": "3a0aeb63-b3c4-4c86-eb9c-14f6945e5aa0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ë‹¨ê³„ 3: íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ (3-Model Ensemble ê¸°ë°˜)\n",
            "======================================================================\n",
            "\n",
            "[ë¶„ì„ ë°©ë²•]\n",
            "  1. âœ“ 3ê°œ ëª¨ë¸(LGBM, XGB, CAT)ì˜ Feature Importance í†µí•©\n",
            "  2. âœ“ 5-Fold Cross-Validation ê¸°ë°˜\n",
            "  3. âœ“ SHAP ê°’ ë¶„ì„ (ìƒìœ„ í”¼ì²˜)\n",
            "  4. âœ“ ì•ˆì •ì„± ë¶„ì„ (í‘œì¤€í¸ì°¨)\n",
            "\n",
            "======================================================================\n",
            "ë°ì´í„° ì¤€ë¹„\n",
            "======================================================================\n",
            "\n",
            "[í”¼ì²˜ êµ¬ì„±]\n",
            "  ì „ì²´ í”¼ì²˜: 3076ê°œ\n",
            "  - Fingerprint: 3072ê°œ\n",
            "    Â· ECFP: 1024ê°œ\n",
            "    Â· FCFP: 1024ê°œ\n",
            "    Â· PTFP: 1024ê°œ\n",
            "  - Descriptor: 4ê°œ\n",
            "\n",
            "======================================================================\n",
            "3-Model Ensemble Feature Importance ê³„ì‚°\n",
            "======================================================================\n",
            "âœ“ 5-Fold Feature Importance ê³„ì‚° ì™„ë£Œ\n",
            "\n",
            "======================================================================\n",
            "ëª¨ë¸ë³„ Feature Importance í†µí•©\n",
            "======================================================================\n",
            "\n",
            "[ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜ (Ensemble ê¸°ì¤€)]\n",
            "  feature  ensemble_mean  ensemble_std  cv_coefficient\n",
            "    clogp    4553.962173    226.119807        0.049653\n",
            " ecfp_807    1051.635186    120.216099        0.114313\n",
            "      qed     969.271712     51.724258        0.053364\n",
            " sa_score     930.372308     41.783955        0.044911\n",
            "    MolWt     832.295959     56.295232        0.067638\n",
            "  fcfp_18     591.584150    100.075438        0.169165\n",
            " fcfp_926     568.021539     93.016768        0.163756\n",
            " ecfp_893     270.677769     87.739696        0.324148\n",
            " ecfp_219     254.949614     76.201583        0.298889\n",
            " ecfp_887     251.834884     58.347738        0.231690\n",
            " fcfp_546     213.282075     54.517532        0.255612\n",
            " ecfp_767     179.931747     93.189335        0.517915\n",
            " fcfp_671     179.540593     33.953107        0.189111\n",
            "ptfp_1013     171.717176     41.981088        0.244478\n",
            " fcfp_370     169.288684     29.659545        0.175201\n",
            " ptfp_666     142.569514     34.167209        0.239653\n",
            "fcfp_1008     133.119162     18.165724        0.136462\n",
            "  fcfp_24     121.819698     53.861914        0.442145\n",
            " ptfp_281     120.242729     55.822532        0.464249\n",
            " ptfp_415     111.151152     19.113425        0.171959\n",
            "\n",
            "======================================================================\n",
            "ë¬¼ì„± Descriptor ì¤‘ìš”ë„ ë¶„ì„\n",
            "======================================================================\n",
            "\n",
            "[Descriptor ì¤‘ìš”ë„ ìˆœìœ„]\n",
            "    1ìœ„. clogp        :    4553.96 Â± 226.12\n",
            "    3ìœ„. qed          :     969.27 Â±  51.72\n",
            "    4ìœ„. sa_score     :     930.37 Â±  41.78\n",
            "    5ìœ„. MolWt        :     832.30 Â±  56.30\n",
            "\n",
            "[Descriptor ìƒëŒ€ ê¸°ì—¬ë„]\n",
            "  clogp        :  62.5%\n",
            "  qed          :  13.3%\n",
            "  sa_score     :  12.8%\n",
            "  MolWt        :  11.4%\n",
            "\n",
            "======================================================================\n",
            "Fingerprint íƒ€ì…ë³„ ì¤‘ìš”ë„ ë¶„ì„\n",
            "======================================================================\n",
            "\n",
            "[Fingerprint íƒ€ì…ë³„ í†µê³„]\n",
            "íƒ€ì…       í‰ê·  ì¤‘ìš”ë„          Top 10 ê°œìˆ˜    Top 50 ê°œìˆ˜   \n",
            "--------------------------------------------------\n",
            "ECFP     9.91            4            14          \n",
            "FCFP     7.79            2            18          \n",
            "PTFP     10.82           0            14          \n",
            "\n",
            "======================================================================\n",
            "Feature Selection ì „ëµ\n",
            "======================================================================\n",
            "\n",
            "[ì „ëµ 1: ìƒìœ„ Nê°œ ì„ íƒ]\n",
            "N        ëˆ„ì  ì¤‘ìš”ë„          ì••ì¶•ë¥        \n",
            "-----------------------------------\n",
            "50       39.57          % 1.6       %\n",
            "100      47.71          % 3.3       %\n",
            "150      54.00          % 4.9       %\n",
            "200      59.31          % 6.5       %\n",
            "\n",
            "[ì „ëµ 2: ì„ê³„ê°’ ê¸°ë°˜ ì„ íƒ]\n",
            "ì„ê³„ê°’        ì„ íƒ í”¼ì²˜ ìˆ˜         ì••ì¶•ë¥        \n",
            "-----------------------------------\n",
            "10         747             24.3      %\n",
            "20         391             12.7      %\n",
            "30         240             7.8       %\n",
            "50         107             3.5       %\n",
            "100        26              0.8       %\n",
            "\n",
            "[ì „ëµ 3: ëˆ„ì  ê¸°ì—¬ë„ ê¸°ë°˜]\n",
            "ëˆ„ì  %       í•„ìš” í”¼ì²˜ ìˆ˜         ì••ì¶•ë¥        \n",
            "-----------------------------------\n",
            "80         537             17.5      %\n",
            "90         863             28.1      %\n",
            "95         1154            37.5      %\n",
            "99         1697            55.2      %\n",
            "\n",
            "[ê¶Œì¥ ì„¤ì •]\n",
            "  ì„ íƒ í”¼ì²˜: ìƒìœ„ 150ê°œ\n",
            "  ëˆ„ì  ì¤‘ìš”ë„: 54.0%\n",
            "  ì••ì¶•ë¥ : 4.9%\n",
            "  - Fingerprint: 146ê°œ\n",
            "  - Descriptor: 4ê°œ\n",
            "\n",
            "======================================================================\n",
            "Feature Importance ì•ˆì •ì„± ë¶„ì„\n",
            "======================================================================\n",
            "\n",
            "[ë†’ì€ ë³€ë™ì„± í”¼ì²˜ (CV > 0.5)]\n",
            "  ë¶ˆì•ˆì •í•œ í”¼ì²˜: 2142ê°œ\n",
            "\n",
            "  ìƒìœ„ 10ê°œ:\n",
            " feature  ensemble_mean  ensemble_std  cv_coefficient\n",
            "ptfp_608       0.539050      1.078100             2.0\n",
            " fcfp_77       0.326095      0.652190             2.0\n",
            "fcfp_547       0.219839      0.439679             2.0\n",
            "ecfp_412       0.133430      0.266859             2.0\n",
            " fcfp_84       0.062700      0.125400             2.0\n",
            "ecfp_889       0.057199      0.114398             2.0\n",
            "ptfp_278       0.056466      0.112932             2.0\n",
            "fcfp_432       0.054418      0.108837             2.0\n",
            " ecfp_71       0.051415      0.102829             2.0\n",
            "fcfp_881       0.039461      0.078922             2.0\n",
            "\n",
            "[ì•ˆì •ì ì´ë©´ì„œ ì¤‘ìš”í•œ í”¼ì²˜ (Importance â‰¥ 50, CV â‰¤ 0.3)]\n",
            "  ê°œìˆ˜: 68ê°œ\n",
            "\n",
            "  Top 10:\n",
            " feature  ensemble_mean  cv_coefficient\n",
            "   clogp    4553.962173        0.049653\n",
            "ecfp_807    1051.635186        0.114313\n",
            "     qed     969.271712        0.053364\n",
            "sa_score     930.372308        0.044911\n",
            "   MolWt     832.295959        0.067638\n",
            " fcfp_18     591.584150        0.169165\n",
            "fcfp_926     568.021539        0.163756\n",
            "ecfp_219     254.949614        0.298889\n",
            "ecfp_887     251.834884        0.231690\n",
            "fcfp_546     213.282075        0.255612\n",
            "\n",
            "======================================================================\n",
            "SHAP ê°’ ë¶„ì„ (ìƒìœ„ 20ê°œ í”¼ì²˜)\n",
            "======================================================================\n",
            "\n",
            "  ìƒ˜í”Œ ìˆ˜: 1000ê°œ\n",
            "  ë¶„ì„ í”¼ì²˜: ìƒìœ„ 20ê°œ\n",
            "\n",
            "[SHAP í‰ê·  ì ˆëŒ€ê°’ (ìƒìœ„ 10ê°œ)]\n",
            " feature  shap_mean_abs\n",
            "   clogp       0.876432\n",
            "ecfp_807       0.193554\n",
            " fcfp_18       0.161359\n",
            "fcfp_926       0.137317\n",
            "fcfp_546       0.098971\n",
            "fcfp_671       0.081812\n",
            "fcfp_370       0.079659\n",
            "sa_score       0.075516\n",
            "     qed       0.064025\n",
            "ptfp_415       0.056925\n",
            "\n",
            "======================================================================\n",
            "ì‹œê°í™” ìƒì„±\n",
            "======================================================================\n",
            "\n",
            "âœ“ ì¢…í•© ì‹œê°í™” ì €ì¥: feature_importance_comprehensive.png\n",
            "âœ“ Top 20 ì‹œê°í™” ì €ì¥: feature_importance_top20.png\n",
            "\n",
            "======================================================================\n",
            "ê²°ê³¼ ì €ì¥\n",
            "======================================================================\n",
            "âœ“ ì „ì²´ Feature Importance ì €ì¥: feature_importance_ensemble_cv.csv\n",
            "âœ“ ê¶Œì¥ í”¼ì²˜ ì €ì¥ (Top 150): selected_features_top150.csv\n",
            "âœ“ ì•ˆì •ì  ì¤‘ìš” í”¼ì²˜ ì €ì¥: stable_important_features.csv\n",
            "\n",
            "======================================================================\n",
            "âœ“ ë‹¨ê³„ 3 ì™„ë£Œ - Feature Importance ë¶„ì„ ì™„ë£Œ\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ë‹¨ê³„ 4: Top 150 í”¼ì²˜ ê¸°ë°˜ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° Threshold ìµœì í™”\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    f1_score, precision_recall_curve, roc_curve,\n",
        "    roc_auc_score, average_precision_score,\n",
        "    classification_report, confusion_matrix,\n",
        "    precision_score, recall_score\n",
        ")\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ë‹¨ê³„ 4: Top 150 í”¼ì²˜ ê¸°ë°˜ ìµœì¢… ëª¨ë¸ í•™ìŠµ\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[ëª©í‘œ]\")\n",
        "print(\"  1. âœ“ Top 150 í”¼ì²˜ë¡œ 3-Model Ensemble ì¬í•™ìŠµ\")\n",
        "print(\"  2. âœ“ Threshold ì¬ìµœì í™” (FPR â‰¤ 25%)\")\n",
        "print(\"  3. âœ“ ì„±ëŠ¥ í–¥ìƒ ê²€ì¦\")\n",
        "print(\"  4. âœ“ Test ë°ì´í„° ì˜ˆì¸¡ ì¤€ë¹„\")\n",
        "\n",
        "# ============================================================\n",
        "# 4.1 Top 150 í”¼ì²˜ ë¡œë“œ ë° ë°ì´í„° ì¤€ë¹„\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ë°ì´í„° ì¤€ë¹„\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Top 150 í”¼ì²˜ ë¡œë“œ\n",
        "selected_features_df = pd.read_csv('selected_features_top150.csv')\n",
        "selected_features = selected_features_df['feature'].tolist()\n",
        "\n",
        "print(f\"\\n[í”¼ì²˜ ì„ íƒ]\")\n",
        "print(f\"  ì„ íƒëœ í”¼ì²˜: {len(selected_features)}ê°œ\")\n",
        "print(f\"  - Fingerprint: {len([f for f in selected_features if f.startswith(('ecfp_', 'fcfp_', 'ptfp_'))])}ê°œ\")\n",
        "print(f\"  - Descriptor: {len([f for f in selected_features if f in ['MolWt', 'clogp', 'sa_score', 'qed']])}ê°œ\")\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "df = pd.read_csv('train.csv')\n",
        "X = df[selected_features]\n",
        "y = df['label'].astype(int)\n",
        "\n",
        "print(f\"\\n[ë°ì´í„° í¬ê¸°]\")\n",
        "print(f\"  ì´ì „: (8349, 3076)\")\n",
        "print(f\"  í˜„ì¬: {X.shape}\")\n",
        "print(f\"  ì••ì¶•ë¥ : {X.shape[1]/3076*100:.1f}% ({3076-X.shape[1]}ê°œ ì œê±°)\")\n",
        "\n",
        "# ============================================================\n",
        "# 4.2 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "# ============================================================\n",
        "fp_cols = [f for f in selected_features if f.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# êµì°¨ê²€ì¦\n",
        "RANDOM_STATE = 42\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# ============================================================\n",
        "# 4.3 1ë‹¨ê³„: OOF í™•ë¥  ìˆ˜ì§‘ (Threshold ìµœì í™”ìš©)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"1ë‹¨ê³„: OOF í™•ë¥  ìˆ˜ì§‘\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "results_stage1 = {\n",
        "    'lgbm': {'oof_probabilities': np.zeros(len(X))},\n",
        "    'xgb': {'oof_probabilities': np.zeros(len(X))},\n",
        "    'catboost': {'oof_probabilities': np.zeros(len(X))},\n",
        "    'ensemble': {'oof_probabilities': np.zeros(len(X))}\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "    print(f\"\\rFold {fold}/5 ì²˜ë¦¬ ì¤‘...\", end='')\n",
        "\n",
        "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
        "\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    # LightGBM\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "        num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "        colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)],\n",
        "                   callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "    results_stage1['lgbm']['oof_probabilities'][va_idx] = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "        gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1,\n",
        "        early_stopping_rounds=100, eval_metric='logloss', verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)], verbose=False)\n",
        "    results_stage1['xgb']['oof_probabilities'][va_idx] = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "    # CatBoost\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000, learning_rate=0.03, depth=7,\n",
        "        l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE, verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "    cat_model.fit(Xt_tr, y_tr, eval_set=(Xt_va, y_va), verbose=False)\n",
        "    results_stage1['catboost']['oof_probabilities'][va_idx] = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "print(f\"\\râœ“ 1ë‹¨ê³„ ì™„ë£Œ: OOF í™•ë¥  ìˆ˜ì§‘ ì™„ë£Œ\")\n",
        "\n",
        "# Ensemble\n",
        "results_stage1['ensemble']['oof_probabilities'] = (\n",
        "    0.25 * results_stage1['lgbm']['oof_probabilities'] +\n",
        "    0.50 * results_stage1['xgb']['oof_probabilities'] +\n",
        "    0.25 * results_stage1['catboost']['oof_probabilities']\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 4.4 Threshold ì¬ìµœì í™”\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"2ë‹¨ê³„: Threshold ì¬ìµœì í™”\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "def optimize_threshold_with_fpr(y_true, y_pred_proba, max_fpr=0.25):\n",
        "    \"\"\"FPR ì œì•½ í•˜ì—ì„œ F1 Score ìµœì í™”\"\"\"\n",
        "    thresholds = np.arange(0.1, 0.9, 0.005)\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "    best_fpr = 1.0\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "        results_list.append({\n",
        "            'threshold': thresh,\n",
        "            'f1': f1,\n",
        "            'fpr': fpr,\n",
        "            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
        "            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        })\n",
        "\n",
        "        if fpr <= max_fpr and f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = thresh\n",
        "            best_fpr = fpr\n",
        "\n",
        "    return best_threshold, best_f1, best_fpr, results_list\n",
        "\n",
        "# Threshold íƒìƒ‰\n",
        "optimal_threshold, optimal_f1, optimal_fpr, threshold_results = optimize_threshold_with_fpr(\n",
        "    y,\n",
        "    results_stage1['ensemble']['oof_probabilities'],\n",
        "    max_fpr=0.25\n",
        ")\n",
        "\n",
        "print(f\"\\n[ìµœì  Threshold íƒìƒ‰ ê²°ê³¼]\")\n",
        "print(f\"  ì œì•½ ì¡°ê±´: FPR â‰¤ 25%\")\n",
        "print(f\"  ìµœì  Threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"  ì˜ˆìƒ F1 Score: {optimal_f1:.4f}\")\n",
        "print(f\"  ì˜ˆìƒ FPR: {optimal_fpr:.4f} ({optimal_fpr*100:.2f}%)\")\n",
        "\n",
        "# ì´ì „ê³¼ ë¹„êµ\n",
        "print(f\"\\n[ì´ì „ ëŒ€ë¹„ ë¹„êµ]\")\n",
        "print(f\"  ì´ì „ Threshold: 0.385 (2ë‹¨ê³„)\")\n",
        "print(f\"  í˜„ì¬ Threshold: {optimal_threshold:.3f} (Top 150 ê¸°ë°˜)\")\n",
        "print(f\"  ë³€í™”: {optimal_threshold - 0.385:+.3f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 4.5 ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"3ë‹¨ê³„: ìµœì¢… 3-Model Ensemble í•™ìŠµ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "results_final = {\n",
        "    'lgbm': {'f1_scores': [], 'auc_scores': [], 'models': [],\n",
        "             'oof_predictions': np.zeros(len(X)),\n",
        "             'oof_probabilities': np.zeros(len(X))},\n",
        "    'xgb': {'f1_scores': [], 'auc_scores': [], 'models': [],\n",
        "            'oof_predictions': np.zeros(len(X)),\n",
        "            'oof_probabilities': np.zeros(len(X))},\n",
        "    'catboost': {'f1_scores': [], 'auc_scores': [], 'models': [],\n",
        "                 'oof_predictions': np.zeros(len(X)),\n",
        "                 'oof_probabilities': np.zeros(len(X))},\n",
        "    'ensemble': {'oof_probabilities': np.zeros(len(X)),\n",
        "                 'oof_predictions': np.zeros(len(X))},\n",
        "    'fold_details': []\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "    print(f\"\\n{'â”€'*70}\")\n",
        "    print(f\"ğŸ“Š Fold {fold}/5\")\n",
        "    print(f\"{'â”€'*70}\")\n",
        "\n",
        "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
        "\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    print(f\"  í•™ìŠµ: {Xt_tr.shape}, ê²€ì¦: {Xt_va.shape}\")\n",
        "\n",
        "    # LightGBM\n",
        "    print(f\"  [1/3] LightGBM...\", end=' ')\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "        num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "        colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)],\n",
        "                   callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "\n",
        "    lgbm_proba = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "    lgbm_pred = (lgbm_proba >= optimal_threshold).astype(int)\n",
        "    lgbm_f1 = f1_score(y_va, lgbm_pred)\n",
        "\n",
        "    results_final['lgbm']['models'].append(lgbm_model)\n",
        "    results_final['lgbm']['oof_probabilities'][va_idx] = lgbm_proba\n",
        "    results_final['lgbm']['oof_predictions'][va_idx] = lgbm_pred\n",
        "    results_final['lgbm']['f1_scores'].append(lgbm_f1)\n",
        "    results_final['lgbm']['auc_scores'].append(roc_auc_score(y_va, lgbm_proba))\n",
        "\n",
        "    print(f\"F1: {lgbm_f1:.4f}\")\n",
        "\n",
        "    # XGBoost\n",
        "    print(f\"  [2/3] XGBoost...\", end=' ')\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "        gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1,\n",
        "        early_stopping_rounds=100, eval_metric='logloss', verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)], verbose=False)\n",
        "\n",
        "    xgb_proba = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "    xgb_pred = (xgb_proba >= optimal_threshold).astype(int)\n",
        "    xgb_f1 = f1_score(y_va, xgb_pred)\n",
        "\n",
        "    results_final['xgb']['models'].append(xgb_model)\n",
        "    results_final['xgb']['oof_probabilities'][va_idx] = xgb_proba\n",
        "    results_final['xgb']['oof_predictions'][va_idx] = xgb_pred\n",
        "    results_final['xgb']['f1_scores'].append(xgb_f1)\n",
        "    results_final['xgb']['auc_scores'].append(roc_auc_score(y_va, xgb_proba))\n",
        "\n",
        "    print(f\"F1: {xgb_f1:.4f}\")\n",
        "\n",
        "    # CatBoost\n",
        "    print(f\"  [3/3] CatBoost...\", end=' ')\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000, learning_rate=0.03, depth=7,\n",
        "        l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE, verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "    cat_model.fit(Xt_tr, y_tr, eval_set=(Xt_va, y_va), verbose=False)\n",
        "\n",
        "    cat_proba = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "    cat_pred = (cat_proba >= optimal_threshold).astype(int)\n",
        "    cat_f1 = f1_score(y_va, cat_pred)\n",
        "\n",
        "    results_final['catboost']['models'].append(cat_model)\n",
        "    results_final['catboost']['oof_probabilities'][va_idx] = cat_proba\n",
        "    results_final['catboost']['oof_predictions'][va_idx] = cat_pred\n",
        "    results_final['catboost']['f1_scores'].append(cat_f1)\n",
        "    results_final['catboost']['auc_scores'].append(roc_auc_score(y_va, cat_proba))\n",
        "\n",
        "    print(f\"F1: {cat_f1:.4f}\")\n",
        "\n",
        "    # Ensemble\n",
        "    ensemble_proba = 0.25 * lgbm_proba + 0.50 * xgb_proba + 0.25 * cat_proba\n",
        "    ensemble_pred = (ensemble_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "    results_final['ensemble']['oof_probabilities'][va_idx] = ensemble_proba\n",
        "    results_final['ensemble']['oof_predictions'][va_idx] = ensemble_pred\n",
        "\n",
        "    ensemble_f1 = f1_score(y_va, ensemble_pred)\n",
        "    ensemble_auc = roc_auc_score(y_va, ensemble_proba)\n",
        "\n",
        "    cm = confusion_matrix(y_va, ensemble_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "\n",
        "    print(f\"\\n  [Ensemble] F1: {ensemble_f1:.4f}, AUC: {ensemble_auc:.4f}, FPR: {fpr:.4f}\")\n",
        "\n",
        "    results_final['fold_details'].append({\n",
        "        'fold': fold,\n",
        "        'lgbm_f1': lgbm_f1, 'xgb_f1': xgb_f1, 'cat_f1': cat_f1,\n",
        "        'ensemble_f1': ensemble_f1, 'ensemble_auc': ensemble_auc,\n",
        "        'fpr': fpr\n",
        "    })\n",
        "\n",
        "# ============================================================\n",
        "# 4.6 ìµœì¢… ê²°ê³¼ ë° ë¹„êµ\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ìµœì¢… ê²°ê³¼\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# OOF ì„±ëŠ¥\n",
        "ensemble_oof_f1 = f1_score(y, results_final['ensemble']['oof_predictions'])\n",
        "ensemble_oof_auc = roc_auc_score(y, results_final['ensemble']['oof_probabilities'])\n",
        "\n",
        "oof_cm = confusion_matrix(y, results_final['ensemble']['oof_predictions'])\n",
        "tn, fp, fn, tp = oof_cm.ravel()\n",
        "final_fpr = fp / (fp + tn)\n",
        "final_precision = tp / (tp + fp)\n",
        "final_recall = tp / (tp + fn)\n",
        "\n",
        "print(f\"\\n[Ensemble OOF ì„±ëŠ¥]\")\n",
        "print(f\"  F1 Score:  {ensemble_oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {ensemble_oof_auc:.4f}\")\n",
        "print(f\"  Precision: {final_precision:.4f}\")\n",
        "print(f\"  Recall:    {final_recall:.4f}\")\n",
        "print(f\"  FPR:       {final_fpr:.4f} ({final_fpr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[OOF í˜¼ë™ í–‰ë ¬]\")\n",
        "print(f\"              ì˜ˆì¸¡: 0    ì˜ˆì¸¡: 1\")\n",
        "print(f\"  ì‹¤ì œ: 0  |   {oof_cm[0,0]:4d}      {oof_cm[0,1]:4d}\")\n",
        "print(f\"  ì‹¤ì œ: 1  |   {oof_cm[1,0]:4d}      {oof_cm[1,1]:4d}\")\n",
        "\n",
        "# ì´ì „ ê²°ê³¼ì™€ ë¹„êµ\n",
        "print(f\"\\n[ì„±ëŠ¥ ë¹„êµ]\")\n",
        "print(f\"{'ì§€í‘œ':<15} {'2ë‹¨ê³„ (3076ê°œ)':<18} {'4ë‹¨ê³„ (150ê°œ)':<18} {'ë³€í™”':<10}\")\n",
        "print(f\"{'-'*65}\")\n",
        "\n",
        "baseline_f1 = 0.8317\n",
        "baseline_fpr = 0.2514\n",
        "\n",
        "f1_change = ensemble_oof_f1 - baseline_f1\n",
        "fpr_change = final_fpr - baseline_fpr\n",
        "\n",
        "print(f\"{'F1 Score':<15} {baseline_f1:<18.4f} {ensemble_oof_f1:<18.4f} {f1_change:+.4f}\")\n",
        "print(f\"{'FPR':<15} {baseline_fpr:<18.4f} {final_fpr:<18.4f} {fpr_change:+.4f}\")\n",
        "print(f\"{'í”¼ì²˜ ìˆ˜':<15} {'3076':<18} {'150':<18} {'-2926'}\")\n",
        "\n",
        "# Low Confidence\n",
        "confidence = np.abs(results_final['ensemble']['oof_probabilities'] - 0.5)\n",
        "low_conf_mask = confidence < 0.1\n",
        "n_low_conf = low_conf_mask.sum()\n",
        "low_conf_acc = (\n",
        "    results_final['ensemble']['oof_predictions'][low_conf_mask] == y[low_conf_mask]\n",
        ").mean() if n_low_conf > 0 else 0\n",
        "\n",
        "print(f\"\\n[Low Confidence]\")\n",
        "print(f\"  ì´ì „: 941ê°œ (ì •í™•ë„ 57.39%)\")\n",
        "print(f\"  í˜„ì¬: {n_low_conf}ê°œ (ì •í™•ë„ {low_conf_acc:.4f})\")\n",
        "\n",
        "# ============================================================\n",
        "# 4.7 ì‹œê°í™”\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ì‹œê°í™” ìƒì„±\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# 1. F1 Score vs Threshold\n",
        "ax = axes[0, 0]\n",
        "thresh_vals = [r['threshold'] for r in threshold_results]\n",
        "f1_vals = [r['f1'] for r in threshold_results]\n",
        "ax.plot(thresh_vals, f1_vals, linewidth=2)\n",
        "ax.axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal: {optimal_threshold:.3f}')\n",
        "ax.axvline(0.5, color='gray', linestyle=':', label='Default: 0.5')\n",
        "ax.set_xlabel('Threshold')\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('F1 Score vs Threshold (Top 150 Features)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. ROC Curve\n",
        "ax = axes[0, 1]\n",
        "fpr_curve, tpr_curve, _ = roc_curve(y, results_final['ensemble']['oof_probabilities'])\n",
        "ax.plot(fpr_curve, tpr_curve, linewidth=2, label=f'AUC={ensemble_oof_auc:.4f}')\n",
        "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve (Ensemble OOF)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Precision-Recall\n",
        "ax = axes[0, 2]\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(\n",
        "    y, results_final['ensemble']['oof_probabilities']\n",
        ")\n",
        "ax.plot(recall_curve, precision_curve, linewidth=2)\n",
        "ax.set_xlabel('Recall')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision-Recall Curve')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Confidence Distribution\n",
        "ax = axes[1, 0]\n",
        "ax.hist(confidence, bins=50, edgecolor='black', alpha=0.7)\n",
        "ax.axvline(0.1, color='r', linestyle='--', label=f'Low: {n_low_conf}')\n",
        "ax.set_xlabel('Confidence')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Confidence Distribution')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Confusion Matrix\n",
        "ax = axes[1, 1]\n",
        "sns.heatmap(oof_cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix (Ensemble OOF)')\n",
        "\n",
        "# 6. ì„±ëŠ¥ ë¹„êµ\n",
        "ax = axes[1, 2]\n",
        "models = ['2ë‹¨ê³„\\n(3076ê°œ)', '4ë‹¨ê³„\\n(150ê°œ)']\n",
        "f1_values = [baseline_f1, ensemble_oof_f1]\n",
        "colors = ['lightblue', 'darkgreen']\n",
        "\n",
        "bars = ax.bar(models, f1_values, color=colors, alpha=0.8, edgecolor='black')\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('Performance Comparison')\n",
        "ax.set_ylim([0.82, 0.85])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, f1_values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.4f}',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('top150_final_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"\\nâœ“ ì‹œê°í™” ì €ì¥: top150_final_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 4.8 ê²°ê³¼ ì €ì¥\n",
        "# ============================================================\n",
        "results_df = pd.DataFrame(results_final['fold_details'])\n",
        "results_df.to_csv('top150_cv_results.csv', index=False)\n",
        "print(f\"âœ“ CV ê²°ê³¼ ì €ì¥: top150_cv_results.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ“ ë‹¨ê³„ 4 ì™„ë£Œ - Top 150 í”¼ì²˜ ê¸°ë°˜ ìµœì¢… ëª¨ë¸ ì™„ì„±\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\n[ìµœì¢… ê¶Œì¥ ì„¤ì •]\")\n",
        "print(f\"  ì‚¬ìš© í”¼ì²˜: Top 150ê°œ\")\n",
        "print(f\"  Threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"  ì˜ˆìƒ F1: {ensemble_oof_f1:.4f}\")\n",
        "print(f\"  ì˜ˆìƒ FPR: {final_fpr*100:.2f}%\")\n",
        "\n",
        "# ì „ì—­ ë³€ìˆ˜ ì €ì¥\n",
        "globals().update({\n",
        "    'top150_results': results_final,\n",
        "    'optimal_threshold_final': optimal_threshold,\n",
        "    'selected_features_final': selected_features\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhkul0WcY6Hh",
        "outputId": "ac8f5927-084a-4177-c4a4-9acfb445dc70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ë‹¨ê³„ 4: Top 150 í”¼ì²˜ ê¸°ë°˜ ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
            "======================================================================\n",
            "\n",
            "[ëª©í‘œ]\n",
            "  1. âœ“ Top 150 í”¼ì²˜ë¡œ 3-Model Ensemble ì¬í•™ìŠµ\n",
            "  2. âœ“ Threshold ì¬ìµœì í™” (FPR â‰¤ 25%)\n",
            "  3. âœ“ ì„±ëŠ¥ í–¥ìƒ ê²€ì¦\n",
            "  4. âœ“ Test ë°ì´í„° ì˜ˆì¸¡ ì¤€ë¹„\n",
            "\n",
            "======================================================================\n",
            "ë°ì´í„° ì¤€ë¹„\n",
            "======================================================================\n",
            "\n",
            "[í”¼ì²˜ ì„ íƒ]\n",
            "  ì„ íƒëœ í”¼ì²˜: 150ê°œ\n",
            "  - Fingerprint: 146ê°œ\n",
            "  - Descriptor: 4ê°œ\n",
            "\n",
            "[ë°ì´í„° í¬ê¸°]\n",
            "  ì´ì „: (8349, 3076)\n",
            "  í˜„ì¬: (8349, 150)\n",
            "  ì••ì¶•ë¥ : 4.9% (2926ê°œ ì œê±°)\n",
            "\n",
            "======================================================================\n",
            "1ë‹¨ê³„: OOF í™•ë¥  ìˆ˜ì§‘\n",
            "======================================================================\n",
            "âœ“ 1ë‹¨ê³„ ì™„ë£Œ: OOF í™•ë¥  ìˆ˜ì§‘ ì™„ë£Œ\n",
            "\n",
            "======================================================================\n",
            "2ë‹¨ê³„: Threshold ì¬ìµœì í™”\n",
            "======================================================================\n",
            "\n",
            "[ìµœì  Threshold íƒìƒ‰ ê²°ê³¼]\n",
            "  ì œì•½ ì¡°ê±´: FPR â‰¤ 25%\n",
            "  ìµœì  Threshold: 0.405\n",
            "  ì˜ˆìƒ F1 Score: 0.8232\n",
            "  ì˜ˆìƒ FPR: 0.2467 (24.67%)\n",
            "\n",
            "[ì´ì „ ëŒ€ë¹„ ë¹„êµ]\n",
            "  ì´ì „ Threshold: 0.385 (2ë‹¨ê³„)\n",
            "  í˜„ì¬ Threshold: 0.405 (Top 150 ê¸°ë°˜)\n",
            "  ë³€í™”: +0.020\n",
            "\n",
            "======================================================================\n",
            "3ë‹¨ê³„: ìµœì¢… 3-Model Ensemble í•™ìŠµ\n",
            "======================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 1/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 150), ê²€ì¦: (1670, 150)\n",
            "  [1/3] LightGBM... F1: 0.8311\n",
            "  [2/3] XGBoost... F1: 0.8379\n",
            "  [3/3] CatBoost... F1: 0.8356\n",
            "\n",
            "  [Ensemble] F1: 0.8395, AUC: 0.8997, FPR: 0.2352\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 2/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 150), ê²€ì¦: (1670, 150)\n",
            "  [1/3] LightGBM... F1: 0.8114\n",
            "  [2/3] XGBoost... F1: 0.8193\n",
            "  [3/3] CatBoost... F1: 0.8172\n",
            "\n",
            "  [Ensemble] F1: 0.8186, AUC: 0.8777, FPR: 0.2562\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 3/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 150), ê²€ì¦: (1670, 150)\n",
            "  [1/3] LightGBM... F1: 0.7915\n",
            "  [2/3] XGBoost... F1: 0.7967\n",
            "  [3/3] CatBoost... F1: 0.7998\n",
            "\n",
            "  [Ensemble] F1: 0.7978, AUC: 0.8654, FPR: 0.2441\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 4/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 150), ê²€ì¦: (1670, 150)\n",
            "  [1/3] LightGBM... F1: 0.8121\n",
            "  [2/3] XGBoost... F1: 0.8166\n",
            "  [3/3] CatBoost... F1: 0.8236\n",
            "\n",
            "  [Ensemble] F1: 0.8219, AUC: 0.8866, FPR: 0.2625\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 5/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6680, 150), ê²€ì¦: (1669, 150)\n",
            "  [1/3] LightGBM... F1: 0.8331\n",
            "  [2/3] XGBoost... F1: 0.8375\n",
            "  [3/3] CatBoost... F1: 0.8311\n",
            "\n",
            "  [Ensemble] F1: 0.8374, AUC: 0.8961, FPR: 0.2352\n",
            "\n",
            "======================================================================\n",
            "ìµœì¢… ê²°ê³¼\n",
            "======================================================================\n",
            "\n",
            "[Ensemble OOF ì„±ëŠ¥]\n",
            "  F1 Score:  0.8232\n",
            "  AUC Score: 0.8852\n",
            "  Precision: 0.8033\n",
            "  Recall:    0.8441\n",
            "  FPR:       0.2467 (24.67%)\n",
            "\n",
            "[OOF í˜¼ë™ í–‰ë ¬]\n",
            "              ì˜ˆì¸¡: 0    ì˜ˆì¸¡: 1\n",
            "  ì‹¤ì œ: 0  |   2868       939\n",
            "  ì‹¤ì œ: 1  |    708      3834\n",
            "\n",
            "[ì„±ëŠ¥ ë¹„êµ]\n",
            "ì§€í‘œ              2ë‹¨ê³„ (3076ê°œ)        4ë‹¨ê³„ (150ê°œ)         ë³€í™”        \n",
            "-----------------------------------------------------------------\n",
            "F1 Score        0.8317             0.8232             -0.0085\n",
            "FPR             0.2514             0.2467             -0.0047\n",
            "í”¼ì²˜ ìˆ˜            3076               150                -2926\n",
            "\n",
            "[Low Confidence]\n",
            "  ì´ì „: 941ê°œ (ì •í™•ë„ 57.39%)\n",
            "  í˜„ì¬: 1037ê°œ (ì •í™•ë„ 0.5641)\n",
            "\n",
            "======================================================================\n",
            "ì‹œê°í™” ìƒì„±\n",
            "======================================================================\n",
            "\n",
            "âœ“ ì‹œê°í™” ì €ì¥: top150_final_analysis.png\n",
            "âœ“ CV ê²°ê³¼ ì €ì¥: top150_cv_results.csv\n",
            "\n",
            "======================================================================\n",
            "âœ“ ë‹¨ê³„ 4 ì™„ë£Œ - Top 150 í”¼ì²˜ ê¸°ë°˜ ìµœì¢… ëª¨ë¸ ì™„ì„±\n",
            "======================================================================\n",
            "\n",
            "[ìµœì¢… ê¶Œì¥ ì„¤ì •]\n",
            "  ì‚¬ìš© í”¼ì²˜: Top 150ê°œ\n",
            "  Threshold: 0.405\n",
            "  ì˜ˆìƒ F1: 0.8232\n",
            "  ì˜ˆìƒ FPR: 24.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ë‹¨ê³„ 5: ìµœì  í”¼ì²˜ ìˆ˜ íƒìƒ‰ (Grid Search)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ë‹¨ê³„ 5: ìµœì  í”¼ì²˜ ìˆ˜ íƒìƒ‰ (Grid Search)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[ëª©í‘œ]\")\n",
        "print(\"  1. âœ“ ë‹¤ì–‘í•œ í”¼ì²˜ ìˆ˜ (100~300) ì‹¤í—˜\")\n",
        "print(\"  2. âœ“ ì„±ëŠ¥ vs íš¨ìœ¨ì„± Trade-off ë¶„ì„\")\n",
        "print(\"  3. âœ“ ìµœì  í”¼ì²˜ ìˆ˜ ê²°ì •\")\n",
        "print(\"  4. âœ“ ìµœì¢… ëª¨ë¸ ì„ íƒ\")\n",
        "\n",
        "# ============================================================\n",
        "# 5.1 ë°ì´í„° ë¡œë“œ\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ë°ì´í„° ì¤€ë¹„\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Feature importance ë¡œë“œ\n",
        "importance_df = pd.read_csv('feature_importance_ensemble_cv.csv')\n",
        "df = pd.read_csv('train.csv')\n",
        "y = df['label'].astype(int)\n",
        "\n",
        "# êµì°¨ê²€ì¦ ì„¤ì •\n",
        "RANDOM_STATE = 42\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸í•  í”¼ì²˜ ìˆ˜ ëª©ë¡\n",
        "FEATURE_COUNTS = [100, 150, 200, 250, 300]\n",
        "\n",
        "print(f\"\\n[ì‹¤í—˜ ì„¤ì •]\")\n",
        "print(f\"  í…ŒìŠ¤íŠ¸í•  í”¼ì²˜ ìˆ˜: {FEATURE_COUNTS}\")\n",
        "print(f\"  5-Fold Cross-Validation\")\n",
        "print(f\"  3-Model Ensemble (LGBM 25%, XGB 50%, CAT 25%)\")\n",
        "\n",
        "# ============================================================\n",
        "# 5.2 í”¼ì²˜ ìˆ˜ë³„ ì‹¤í—˜ í•¨ìˆ˜\n",
        "# ============================================================\n",
        "\n",
        "def train_and_evaluate(n_features, X, y, skf, verbose=True):\n",
        "    \"\"\"\n",
        "    ì£¼ì–´ì§„ í”¼ì²˜ ìˆ˜ë¡œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
        "    \"\"\"\n",
        "    # í”¼ì²˜ ì„ íƒ\n",
        "    selected_features = importance_df.head(n_features)['feature'].tolist()\n",
        "    X_selected = df[selected_features]\n",
        "\n",
        "    # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "    fp_cols = [f for f in selected_features if f.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "    desc_cols = [f for f in selected_features if f in ['MolWt', 'clogp', 'sa_score', 'qed']]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "            ('desc', Pipeline([\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), desc_cols)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    results = {\n",
        "        'oof_probabilities': np.zeros(len(X_selected)),\n",
        "        'f1_scores': [],\n",
        "        'auc_scores': [],\n",
        "        'fpr_scores': [],\n",
        "        'train_times': []\n",
        "    }\n",
        "\n",
        "    # 5-Fold CV\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_selected, y), 1):\n",
        "        if verbose:\n",
        "            print(f\"\\r    Fold {fold}/5 ì²˜ë¦¬ ì¤‘...\", end='')\n",
        "\n",
        "        X_tr, X_va = X_selected.iloc[tr_idx], X_selected.iloc[va_idx]\n",
        "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
        "\n",
        "        # ì „ì²˜ë¦¬\n",
        "        Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "        Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "        # í•™ìŠµ ì‹œê°„ ì¸¡ì • ì‹œì‘\n",
        "        fold_start_time = time.time()\n",
        "\n",
        "        # LightGBM\n",
        "        lgbm_model = LGBMClassifier(\n",
        "            n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "            num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "            colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "            class_weight={0: 1.5, 1: 1.0},\n",
        "            random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
        "        )\n",
        "        lgbm_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)],\n",
        "                       callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "        lgbm_proba = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "        # XGBoost\n",
        "        xgb_model = XGBClassifier(\n",
        "            n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "            min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "            gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "            scale_pos_weight=0.67,\n",
        "            random_state=RANDOM_STATE, n_jobs=-1,\n",
        "            early_stopping_rounds=100, eval_metric='logloss', verbosity=0\n",
        "        )\n",
        "        xgb_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)], verbose=False)\n",
        "        xgb_proba = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "        # CatBoost\n",
        "        cat_model = CatBoostClassifier(\n",
        "            iterations=1000, learning_rate=0.03, depth=7,\n",
        "            l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "            random_seed=RANDOM_STATE, verbose=0,\n",
        "            early_stopping_rounds=100\n",
        "        )\n",
        "        cat_model.fit(Xt_tr, y_tr, eval_set=(Xt_va, y_va), verbose=False)\n",
        "        cat_proba = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "        # Ensemble\n",
        "        ensemble_proba = 0.25 * lgbm_proba + 0.50 * xgb_proba + 0.25 * cat_proba\n",
        "        results['oof_probabilities'][va_idx] = ensemble_proba\n",
        "\n",
        "        # í•™ìŠµ ì‹œê°„\n",
        "        fold_time = time.time() - fold_start_time\n",
        "        results['train_times'].append(fold_time)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\r    âœ“ 5-Fold ì™„ë£Œ\")\n",
        "\n",
        "    # Threshold ìµœì í™”\n",
        "    def optimize_threshold_with_fpr(y_true, y_pred_proba, max_fpr=0.25):\n",
        "        thresholds = np.arange(0.1, 0.9, 0.005)\n",
        "        best_f1 = 0\n",
        "        best_threshold = 0.5\n",
        "\n",
        "        for thresh in thresholds:\n",
        "            y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "            f1 = f1_score(y_true, y_pred)\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "            if fpr <= max_fpr and f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = thresh\n",
        "\n",
        "        return best_threshold\n",
        "\n",
        "    optimal_threshold = optimize_threshold_with_fpr(y, results['oof_probabilities'])\n",
        "\n",
        "    # ìµœì¢… ì˜ˆì¸¡\n",
        "    oof_predictions = (results['oof_probabilities'] >= optimal_threshold).astype(int)\n",
        "\n",
        "    # ì„±ëŠ¥ ì§€í‘œ\n",
        "    f1 = f1_score(y, oof_predictions)\n",
        "    auc = roc_auc_score(y, results['oof_probabilities'])\n",
        "\n",
        "    cm = confusion_matrix(y, oof_predictions)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "    # Low Confidence\n",
        "    confidence = np.abs(results['oof_probabilities'] - 0.5)\n",
        "    low_conf_count = (confidence < 0.1).sum()\n",
        "    low_conf_acc = (oof_predictions[confidence < 0.1] == y[confidence < 0.1]).mean() if low_conf_count > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'n_features': n_features,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'fpr': fpr,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'threshold': optimal_threshold,\n",
        "        'low_conf_count': low_conf_count,\n",
        "        'low_conf_acc': low_conf_acc,\n",
        "        'avg_train_time': np.mean(results['train_times']),\n",
        "        'total_train_time': np.sum(results['train_times']),\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 5.3 Grid Search ì‹¤í–‰\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Grid Search ì‹¤í–‰\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for n_features in FEATURE_COUNTS:\n",
        "    print(f\"\\n[{n_features}ê°œ í”¼ì²˜]\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    result = train_and_evaluate(n_features, df, y, skf, verbose=True)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    result['wall_time'] = total_time\n",
        "    all_results.append(result)\n",
        "\n",
        "    print(f\"  F1 Score:  {result['f1']:.4f}\")\n",
        "    print(f\"  AUC:       {result['auc']:.4f}\")\n",
        "    print(f\"  FPR:       {result['fpr']:.4f} ({result['fpr']*100:.2f}%)\")\n",
        "    print(f\"  Threshold: {result['threshold']:.3f}\")\n",
        "    print(f\"  Low Conf:  {result['low_conf_count']}ê°œ (ì •í™•ë„ {result['low_conf_acc']:.4f})\")\n",
        "    print(f\"  í•™ìŠµ ì‹œê°„: {total_time:.1f}ì´ˆ\")\n",
        "\n",
        "# ============================================================\n",
        "# 5.4 ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ê²°ê³¼ ë¹„êµ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# DataFrame ìƒì„±\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "print(f\"\\n[ì¢…í•© ë¹„êµí‘œ]\")\n",
        "print(f\"{'í”¼ì²˜ ìˆ˜':<10} {'F1':<10} {'AUC':<10} {'FPR':<10} {'Precision':<12} {'Recall':<10} {'ì‹œê°„(ì´ˆ)':<10}\")\n",
        "print(f\"{'-'*80}\")\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"{row['n_features']:<10} {row['f1']:<10.4f} {row['auc']:<10.4f} \"\n",
        "          f\"{row['fpr']:<10.4f} {row['precision']:<12.4f} {row['recall']:<10.4f} \"\n",
        "          f\"{row['wall_time']:<10.1f}\")\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ì°¾ê¸°\n",
        "best_f1_idx = results_df['f1'].idxmax()\n",
        "best_f1_row = results_df.iloc[best_f1_idx]\n",
        "\n",
        "print(f\"\\n[ìµœê³  F1 Score]\")\n",
        "print(f\"  í”¼ì²˜ ìˆ˜: {best_f1_row['n_features']:.0f}ê°œ\")\n",
        "print(f\"  F1 Score: {best_f1_row['f1']:.4f}\")\n",
        "print(f\"  FPR: {best_f1_row['fpr']:.4f} ({best_f1_row['fpr']*100:.2f}%)\")\n",
        "\n",
        "# ìµœì  Trade-off ì°¾ê¸° (F1 * ì†ë„)\n",
        "results_df['efficiency_score'] = results_df['f1'] / (results_df['wall_time'] / results_df['wall_time'].min())\n",
        "best_tradeoff_idx = results_df['efficiency_score'].idxmax()\n",
        "best_tradeoff_row = results_df.iloc[best_tradeoff_idx]\n",
        "\n",
        "print(f\"\\n[ìµœì  Trade-off (ì„±ëŠ¥ vs ì†ë„)]\")\n",
        "print(f\"  í”¼ì²˜ ìˆ˜: {best_tradeoff_row['n_features']:.0f}ê°œ\")\n",
        "print(f\"  F1 Score: {best_tradeoff_row['f1']:.4f}\")\n",
        "print(f\"  í•™ìŠµ ì‹œê°„: {best_tradeoff_row['wall_time']:.1f}ì´ˆ\")\n",
        "print(f\"  íš¨ìœ¨ì„± ì ìˆ˜: {best_tradeoff_row['efficiency_score']:.4f}\")\n",
        "\n",
        "# 2ë‹¨ê³„ (3076ê°œ) ê¸°ì¤€ê³¼ ë¹„êµ\n",
        "baseline_f1 = 0.8317\n",
        "baseline_time = 100  # ìƒëŒ€ê°’\n",
        "\n",
        "print(f\"\\n[2ë‹¨ê³„ (3076ê°œ í”¼ì²˜) ëŒ€ë¹„ ë¹„êµ]\")\n",
        "print(f\"{'í”¼ì²˜ ìˆ˜':<10} {'F1 ë³€í™”':<15} {'ì†ë„ ê°œì„ ':<15} {'ì¢…í•© í‰ê°€':<15}\")\n",
        "print(f\"{'-'*60}\")\n",
        "for _, row in results_df.iterrows():\n",
        "    f1_change = row['f1'] - baseline_f1\n",
        "    f1_change_pct = (f1_change / baseline_f1) * 100\n",
        "    speed_improvement = (baseline_time - row['wall_time']) / baseline_time * 100\n",
        "\n",
        "    if f1_change >= 0 and speed_improvement > 80:\n",
        "        evaluation = \"âœ“âœ“ ìµœê³ \"\n",
        "    elif f1_change >= -0.005 and speed_improvement > 80:\n",
        "        evaluation = \"âœ“ ìš°ìˆ˜\"\n",
        "    elif f1_change >= -0.01:\n",
        "        evaluation = \"â–³ ì–‘í˜¸\"\n",
        "    else:\n",
        "        evaluation = \"âš ï¸ ì£¼ì˜\"\n",
        "\n",
        "    print(f\"{row['n_features']:<10} {f1_change:+.4f} ({f1_change_pct:+.2f}%){'':<3} \"\n",
        "          f\"{speed_improvement:+.1f}%{'':<8} {evaluation}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5.5 ì‹œê°í™”\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ì‹œê°í™” ìƒì„±\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# 1. F1 Score vs í”¼ì²˜ ìˆ˜\n",
        "ax = axes[0, 0]\n",
        "ax.plot(results_df['n_features'], results_df['f1'], 'o-', linewidth=2, markersize=8)\n",
        "ax.axhline(baseline_f1, color='r', linestyle='--', label=f'Baseline (3076ê°œ): {baseline_f1:.4f}')\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('F1 Score', fontsize=11)\n",
        "ax.set_title('F1 Score vs Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# ìµœê³  F1 ë§ˆí‚¹\n",
        "ax.scatter([best_f1_row['n_features']], [best_f1_row['f1']],\n",
        "           color='green', s=200, marker='*', zorder=5, label='Best')\n",
        "\n",
        "# 2. AUC vs í”¼ì²˜ ìˆ˜\n",
        "ax = axes[0, 1]\n",
        "ax.plot(results_df['n_features'], results_df['auc'], 's-', linewidth=2, markersize=8, color='orange')\n",
        "ax.axhline(0.8914, color='r', linestyle='--', label='Baseline: 0.8914')\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('AUC Score', fontsize=11)\n",
        "ax.set_title('AUC vs Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. FPR vs í”¼ì²˜ ìˆ˜\n",
        "ax = axes[0, 2]\n",
        "ax.plot(results_df['n_features'], results_df['fpr'] * 100, '^-', linewidth=2, markersize=8, color='red')\n",
        "ax.axhline(25, color='gray', linestyle=':', label='Target: 25%')\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('False Positive Rate (%)', fontsize=11)\n",
        "ax.set_title('FPR vs Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. í•™ìŠµ ì‹œê°„ vs í”¼ì²˜ ìˆ˜\n",
        "ax = axes[1, 0]\n",
        "ax.bar(results_df['n_features'].astype(str), results_df['wall_time'],\n",
        "       alpha=0.7, edgecolor='black', color='steelblue')\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('Training Time (seconds)', fontsize=11)\n",
        "ax.set_title('Training Time vs Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 5. Precision vs Recall\n",
        "ax = axes[1, 1]\n",
        "for idx, row in results_df.iterrows():\n",
        "    ax.scatter(row['recall'], row['precision'], s=150, alpha=0.7,\n",
        "               label=f\"{row['n_features']:.0f} features\")\n",
        "ax.set_xlabel('Recall', fontsize=11)\n",
        "ax.set_ylabel('Precision', fontsize=11)\n",
        "ax.set_title('Precision vs Recall Trade-off', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. ì¢…í•© ì„±ëŠ¥ ë¹„êµ (ë§‰ëŒ€ ê·¸ë˜í”„)\n",
        "ax = axes[1, 2]\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "\n",
        "f1_normalized = (results_df['f1'] - results_df['f1'].min()) / (results_df['f1'].max() - results_df['f1'].min())\n",
        "speed_normalized = 1 - (results_df['wall_time'] - results_df['wall_time'].min()) / (results_df['wall_time'].max() - results_df['wall_time'].min())\n",
        "\n",
        "ax.bar(x - width/2, f1_normalized, width, label='F1 (Normalized)', alpha=0.8)\n",
        "ax.bar(x + width/2, speed_normalized, width, label='Speed (Normalized)', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Feature Count', fontsize=11)\n",
        "ax.set_ylabel('Normalized Score', fontsize=11)\n",
        "ax.set_title('Performance vs Speed (Normalized)', fontsize=12, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f\"{int(n)}\" for n in results_df['n_features']])\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_count_optimization.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"\\nâœ“ ì‹œê°í™” ì €ì¥: feature_count_optimization.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 5.6 ìµœì¢… ê¶Œì¥ì‚¬í•­\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ìµœì¢… ê¶Œì¥ì‚¬í•­\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ìƒí™©ë³„ ê¶Œì¥\n",
        "print(f\"\\n[ìƒí™©ë³„ ê¶Œì¥ í”¼ì²˜ ìˆ˜]\")\n",
        "\n",
        "print(f\"\\n1. ìµœê³  ì •í™•ë„ ìš°ì„ \")\n",
        "print(f\"   í”¼ì²˜ ìˆ˜: {best_f1_row['n_features']:.0f}ê°œ\")\n",
        "print(f\"   F1 Score: {best_f1_row['f1']:.4f}\")\n",
        "print(f\"   FPR: {best_f1_row['fpr']*100:.2f}%\")\n",
        "print(f\"   ì ìš©: ê²½ìŸ, ë…¼ë¬¸, ì •í™•ë„ ì¤‘ì‹œ\")\n",
        "\n",
        "print(f\"\\n2. ê· í˜• (ê¶Œì¥) âœ“\")\n",
        "print(f\"   í”¼ì²˜ ìˆ˜: {best_tradeoff_row['n_features']:.0f}ê°œ\")\n",
        "print(f\"   F1 Score: {best_tradeoff_row['f1']:.4f}\")\n",
        "print(f\"   í•™ìŠµ ì‹œê°„: {best_tradeoff_row['wall_time']:.1f}ì´ˆ\")\n",
        "print(f\"   ì ìš©: ì¼ë°˜ì ì¸ ì‚¬ìš©, í”„ë¡œë•ì…˜\")\n",
        "\n",
        "print(f\"\\n3. ì†ë„ ìš°ì„ \")\n",
        "fastest_idx = results_df['wall_time'].idxmin()\n",
        "fastest_row = results_df.iloc[fastest_idx]\n",
        "print(f\"   í”¼ì²˜ ìˆ˜: {fastest_row['n_features']:.0f}ê°œ\")\n",
        "print(f\"   F1 Score: {fastest_row['f1']:.4f}\")\n",
        "print(f\"   í•™ìŠµ ì‹œê°„: {fastest_row['wall_time']:.1f}ì´ˆ\")\n",
        "print(f\"   ì ìš©: ì‹¤ì‹œê°„ ì¶”ë¡ , ë¦¬ì†ŒìŠ¤ ì œì•½\")\n",
        "\n",
        "# CSV ì €ì¥\n",
        "results_df.to_csv('feature_count_optimization_results.csv', index=False)\n",
        "print(f\"\\nâœ“ ê²°ê³¼ ì €ì¥: feature_count_optimization_results.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ“ ë‹¨ê³„ 5 ì™„ë£Œ - ìµœì  í”¼ì²˜ ìˆ˜ íƒìƒ‰ ì™„ë£Œ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ì „ì—­ ë³€ìˆ˜ ì €ì¥\n",
        "globals().update({\n",
        "    'optimization_results': all_results,\n",
        "    'best_n_features': int(best_f1_row['n_features']),\n",
        "    'best_f1_score': best_f1_row['f1']\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQHqddDwiPqf",
        "outputId": "285acf3d-896c-4c78-fe68-f468d1d6cb23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ë‹¨ê³„ 5: ìµœì  í”¼ì²˜ ìˆ˜ íƒìƒ‰ (Grid Search)\n",
            "======================================================================\n",
            "\n",
            "[ëª©í‘œ]\n",
            "  1. âœ“ ë‹¤ì–‘í•œ í”¼ì²˜ ìˆ˜ (100~300) ì‹¤í—˜\n",
            "  2. âœ“ ì„±ëŠ¥ vs íš¨ìœ¨ì„± Trade-off ë¶„ì„\n",
            "  3. âœ“ ìµœì  í”¼ì²˜ ìˆ˜ ê²°ì •\n",
            "  4. âœ“ ìµœì¢… ëª¨ë¸ ì„ íƒ\n",
            "\n",
            "======================================================================\n",
            "ë°ì´í„° ì¤€ë¹„\n",
            "======================================================================\n",
            "\n",
            "[ì‹¤í—˜ ì„¤ì •]\n",
            "  í…ŒìŠ¤íŠ¸í•  í”¼ì²˜ ìˆ˜: [100, 150, 200, 250, 300]\n",
            "  5-Fold Cross-Validation\n",
            "  3-Model Ensemble (LGBM 25%, XGB 50%, CAT 25%)\n",
            "\n",
            "======================================================================\n",
            "Grid Search ì‹¤í–‰\n",
            "======================================================================\n",
            "\n",
            "[100ê°œ í”¼ì²˜]\n",
            "    âœ“ 5-Fold ì™„ë£Œ\n",
            "  F1 Score:  0.8184\n",
            "  AUC:       0.8805\n",
            "  FPR:       0.2488 (24.88%)\n",
            "  Threshold: 0.415\n",
            "  Low Conf:  1118ê°œ (ì •í™•ë„ 0.5608)\n",
            "  í•™ìŠµ ì‹œê°„: 99.7ì´ˆ\n",
            "\n",
            "[150ê°œ í”¼ì²˜]\n",
            "    âœ“ 5-Fold ì™„ë£Œ\n",
            "  F1 Score:  0.8243\n",
            "  AUC:       0.8857\n",
            "  FPR:       0.2493 (24.93%)\n",
            "  Threshold: 0.405\n",
            "  Low Conf:  1049ê°œ (ì •í™•ë„ 0.5577)\n",
            "  í•™ìŠµ ì‹œê°„: 123.2ì´ˆ\n",
            "\n",
            "[200ê°œ í”¼ì²˜]\n",
            "    âœ“ 5-Fold ì™„ë£Œ\n",
            "  F1 Score:  0.8275\n",
            "  AUC:       0.8890\n",
            "  FPR:       0.2488 (24.88%)\n",
            "  Threshold: 0.390\n",
            "  Low Conf:  983ê°œ (ì •í™•ë„ 0.5738)\n",
            "  í•™ìŠµ ì‹œê°„: 163.2ì´ˆ\n",
            "\n",
            "[250ê°œ í”¼ì²˜]\n",
            "    âœ“ 5-Fold ì™„ë£Œ\n",
            "  F1 Score:  0.8294\n",
            "  AUC:       0.8909\n",
            "  FPR:       0.2427 (24.27%)\n",
            "  Threshold: 0.400\n",
            "  Low Conf:  1017ê°œ (ì •í™•ë„ 0.5693)\n",
            "  í•™ìŠµ ì‹œê°„: 192.1ì´ˆ\n",
            "\n",
            "[300ê°œ í”¼ì²˜]\n",
            "    âœ“ 5-Fold ì™„ë£Œ\n",
            "  F1 Score:  0.8303\n",
            "  AUC:       0.8925\n",
            "  FPR:       0.2474 (24.74%)\n",
            "  Threshold: 0.390\n",
            "  Low Conf:  972ê°œ (ì •í™•ë„ 0.5514)\n",
            "  í•™ìŠµ ì‹œê°„: 222.3ì´ˆ\n",
            "\n",
            "======================================================================\n",
            "ê²°ê³¼ ë¹„êµ\n",
            "======================================================================\n",
            "\n",
            "[ì¢…í•© ë¹„êµí‘œ]\n",
            "í”¼ì²˜ ìˆ˜       F1         AUC        FPR        Precision    Recall     ì‹œê°„(ì´ˆ)     \n",
            "--------------------------------------------------------------------------------\n",
            "100        0.8184     0.8805     0.2488     0.8006       0.8371     99.7      \n",
            "150        0.8243     0.8857     0.2493     0.8023       0.8476     123.2     \n",
            "200        0.8275     0.8890     0.2488     0.8036       0.8529     163.2     \n",
            "250        0.8294     0.8909     0.2427     0.8074       0.8527     192.1     \n",
            "300        0.8303     0.8925     0.2474     0.8052       0.8571     222.3     \n",
            "\n",
            "[ìµœê³  F1 Score]\n",
            "  í”¼ì²˜ ìˆ˜: 300ê°œ\n",
            "  F1 Score: 0.8303\n",
            "  FPR: 0.2474 (24.74%)\n",
            "\n",
            "[ìµœì  Trade-off (ì„±ëŠ¥ vs ì†ë„)]\n",
            "  í”¼ì²˜ ìˆ˜: 100ê°œ\n",
            "  F1 Score: 0.8184\n",
            "  í•™ìŠµ ì‹œê°„: 99.7ì´ˆ\n",
            "  íš¨ìœ¨ì„± ì ìˆ˜: 0.8184\n",
            "\n",
            "[2ë‹¨ê³„ (3076ê°œ í”¼ì²˜) ëŒ€ë¹„ ë¹„êµ]\n",
            "í”¼ì²˜ ìˆ˜       F1 ë³€í™”           ì†ë„ ê°œì„            ì¢…í•© í‰ê°€          \n",
            "------------------------------------------------------------\n",
            "100        -0.0133 (-1.60%)    +0.3%         âš ï¸ ì£¼ì˜\n",
            "150        -0.0074 (-0.89%)    -23.2%         â–³ ì–‘í˜¸\n",
            "200        -0.0042 (-0.50%)    -63.2%         â–³ ì–‘í˜¸\n",
            "250        -0.0023 (-0.27%)    -92.1%         â–³ ì–‘í˜¸\n",
            "300        -0.0014 (-0.16%)    -122.3%         â–³ ì–‘í˜¸\n",
            "\n",
            "======================================================================\n",
            "ì‹œê°í™” ìƒì„±\n",
            "======================================================================\n",
            "\n",
            "âœ“ ì‹œê°í™” ì €ì¥: feature_count_optimization.png\n",
            "\n",
            "======================================================================\n",
            "ìµœì¢… ê¶Œì¥ì‚¬í•­\n",
            "======================================================================\n",
            "\n",
            "[ìƒí™©ë³„ ê¶Œì¥ í”¼ì²˜ ìˆ˜]\n",
            "\n",
            "1. ìµœê³  ì •í™•ë„ ìš°ì„ \n",
            "   í”¼ì²˜ ìˆ˜: 300ê°œ\n",
            "   F1 Score: 0.8303\n",
            "   FPR: 24.74%\n",
            "   ì ìš©: ê²½ìŸ, ë…¼ë¬¸, ì •í™•ë„ ì¤‘ì‹œ\n",
            "\n",
            "2. ê· í˜• (ê¶Œì¥) âœ“\n",
            "   í”¼ì²˜ ìˆ˜: 100ê°œ\n",
            "   F1 Score: 0.8184\n",
            "   í•™ìŠµ ì‹œê°„: 99.7ì´ˆ\n",
            "   ì ìš©: ì¼ë°˜ì ì¸ ì‚¬ìš©, í”„ë¡œë•ì…˜\n",
            "\n",
            "3. ì†ë„ ìš°ì„ \n",
            "   í”¼ì²˜ ìˆ˜: 100ê°œ\n",
            "   F1 Score: 0.8184\n",
            "   í•™ìŠµ ì‹œê°„: 99.7ì´ˆ\n",
            "   ì ìš©: ì‹¤ì‹œê°„ ì¶”ë¡ , ë¦¬ì†ŒìŠ¤ ì œì•½\n",
            "\n",
            "âœ“ ê²°ê³¼ ì €ì¥: feature_count_optimization_results.csv\n",
            "\n",
            "======================================================================\n",
            "âœ“ ë‹¨ê³„ 5 ì™„ë£Œ - ìµœì  í”¼ì²˜ ìˆ˜ íƒìƒ‰ ì™„ë£Œ\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ë‹¨ê³„ 6: ìµœì¢… ëª¨ë¸ êµ¬ì„± ë° Test ì˜ˆì¸¡ (Top 300 í”¼ì²˜)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ë‹¨ê³„ 6: ìµœì¢… ëª¨ë¸ êµ¬ì„± ë° Test ì˜ˆì¸¡ (Top 300)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[ìµœì¢… ì„¤ì •]\")\n",
        "print(\"  í”¼ì²˜ ìˆ˜: 300ê°œ (ìµœê³  ì„±ëŠ¥)\")\n",
        "print(\"  Threshold: 0.390 (5ë‹¨ê³„ ê²°ê³¼)\")\n",
        "print(\"  Ensemble: LGBM(25%) + XGB(50%) + CAT(25%)\")\n",
        "print(\"  ëª©í‘œ F1: 0.8303, AUC: 0.8925\")\n",
        "\n",
        "# ============================================================\n",
        "# 6.1 Top 300 í”¼ì²˜ ì„ íƒ ë° ë°ì´í„° ì¤€ë¹„\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ë°ì´í„° ì¤€ë¹„\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Feature importance ë¡œë“œ ë° Top 300 ì„ íƒ\n",
        "importance_df = pd.read_csv('feature_importance_ensemble_cv.csv')\n",
        "selected_features = importance_df.head(300)['feature'].tolist()\n",
        "\n",
        "print(f\"\\n[Top 300 í”¼ì²˜]\")\n",
        "print(f\"  ì´ í”¼ì²˜: 300ê°œ\")\n",
        "\n",
        "# íƒ€ì…ë³„ ë¶„í¬\n",
        "fp_cols = [f for f in selected_features if f.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = [f for f in selected_features if f in ['MolWt', 'clogp', 'sa_score', 'qed']]\n",
        "\n",
        "ecfp_count = len([f for f in fp_cols if f.startswith('ecfp_')])\n",
        "fcfp_count = len([f for f in fp_cols if f.startswith('fcfp_')])\n",
        "ptfp_count = len([f for f in fp_cols if f.startswith('ptfp_')])\n",
        "\n",
        "print(f\"  - Descriptor: {len(desc_cols)}ê°œ\")\n",
        "print(f\"  - Fingerprint: {len(fp_cols)}ê°œ\")\n",
        "print(f\"    Â· ECFP: {ecfp_count}ê°œ\")\n",
        "print(f\"    Â· FCFP: {fcfp_count}ê°œ\")\n",
        "print(f\"    Â· PTFP: {ptfp_count}ê°œ\")\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "df_train = pd.read_csv('train.csv')\n",
        "X_train = df_train[selected_features]\n",
        "y_train = df_train['label'].astype(int)\n",
        "\n",
        "print(f\"\\n[Train ë°ì´í„°]\")\n",
        "print(f\"  Shape: {X_train.shape}\")\n",
        "print(f\"  Label ë¶„í¬: Class 0 = {sum(y_train==0)}, Class 1 = {sum(y_train==1)}\")\n",
        "\n",
        "# Test ë°ì´í„° ë¡œë“œ\n",
        "try:\n",
        "    df_test = pd.read_csv('predict_input.csv')\n",
        "    X_test = df_test[selected_features]\n",
        "    print(f\"\\n[Test ë°ì´í„°]\")\n",
        "    print(f\"  Shape: {X_test.shape}\")\n",
        "    test_available = True\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nâš ï¸  Test ë°ì´í„°(predict_input.csv) ì—†ìŒ - í•™ìŠµë§Œ ì§„í–‰\")\n",
        "    test_available = False\n",
        "\n",
        "# ============================================================\n",
        "# 6.2 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "# ============================================================\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 6.3 êµì°¨ê²€ì¦ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ (5-Fold CV)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "OPTIMAL_THRESHOLD = 0.390  # 5ë‹¨ê³„ ê²°ê³¼\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥\n",
        "final_results = {\n",
        "    'lgbm': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'xgb': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'catboost': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'ensemble': {'oof_probabilities': np.zeros(len(X_train)),\n",
        "                 'oof_predictions': np.zeros(len(X_train))},\n",
        "    'fold_details': []\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print(f\"\\n{'â”€'*70}\")\n",
        "    print(f\"ğŸ“Š Fold {fold}/5\")\n",
        "    print(f\"{'â”€'*70}\")\n",
        "\n",
        "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
        "    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
        "\n",
        "    # ì „ì²˜ë¦¬\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    print(f\"  í•™ìŠµ: {Xt_tr.shape}, ê²€ì¦: {Xt_va.shape}\")\n",
        "\n",
        "    # ========================================\n",
        "    # LightGBM\n",
        "    # ========================================\n",
        "    print(f\"  [1/3] LightGBM...\", end=' ')\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=8,\n",
        "        num_leaves=63,\n",
        "        min_child_samples=30,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    )\n",
        "\n",
        "    lgbm_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "\n",
        "    lgbm_proba = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "    lgbm_pred = (lgbm_proba >= OPTIMAL_THRESHOLD).astype(int)\n",
        "    lgbm_f1 = f1_score(y_va, lgbm_pred)\n",
        "\n",
        "    final_results['lgbm']['models'].append(lgbm_model)\n",
        "    final_results['lgbm']['oof_probabilities'][va_idx] = lgbm_proba\n",
        "\n",
        "    print(f\"F1: {lgbm_f1:.4f}, Iter: {lgbm_model.best_iteration_}\")\n",
        "\n",
        "    # ========================================\n",
        "    # XGBoost\n",
        "    # ========================================\n",
        "    print(f\"  [2/3] XGBoost...\", end=' ')\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=7,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        gamma=0.1,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        early_stopping_rounds=100,\n",
        "        eval_metric='logloss',\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    xgb_proba = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "    xgb_pred = (xgb_proba >= OPTIMAL_THRESHOLD).astype(int)\n",
        "    xgb_f1 = f1_score(y_va, xgb_pred)\n",
        "\n",
        "    final_results['xgb']['models'].append(xgb_model)\n",
        "    final_results['xgb']['oof_probabilities'][va_idx] = xgb_proba\n",
        "\n",
        "    print(f\"F1: {xgb_f1:.4f}, Iter: {xgb_model.best_iteration}\")\n",
        "\n",
        "    # ========================================\n",
        "    # CatBoost\n",
        "    # ========================================\n",
        "    print(f\"  [3/3] CatBoost...\", end=' ')\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.03,\n",
        "        depth=7,\n",
        "        l2_leaf_reg=3,\n",
        "        class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE,\n",
        "        verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "\n",
        "    cat_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=(Xt_va, y_va),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    cat_proba = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "    cat_pred = (cat_proba >= OPTIMAL_THRESHOLD).astype(int)\n",
        "    cat_f1 = f1_score(y_va, cat_pred)\n",
        "\n",
        "    final_results['catboost']['models'].append(cat_model)\n",
        "    final_results['catboost']['oof_probabilities'][va_idx] = cat_proba\n",
        "\n",
        "    print(f\"F1: {cat_f1:.4f}, Iter: {cat_model.best_iteration_}\")\n",
        "\n",
        "    # ========================================\n",
        "    # Ensemble\n",
        "    # ========================================\n",
        "    ensemble_proba = (\n",
        "        0.25 * lgbm_proba +\n",
        "        0.50 * xgb_proba +\n",
        "        0.25 * cat_proba\n",
        "    )\n",
        "    ensemble_pred = (ensemble_proba >= OPTIMAL_THRESHOLD).astype(int)\n",
        "\n",
        "    final_results['ensemble']['oof_probabilities'][va_idx] = ensemble_proba\n",
        "    final_results['ensemble']['oof_predictions'][va_idx] = ensemble_pred\n",
        "\n",
        "    ensemble_f1 = f1_score(y_va, ensemble_pred)\n",
        "    ensemble_auc = roc_auc_score(y_va, ensemble_proba)\n",
        "\n",
        "    cm = confusion_matrix(y_va, ensemble_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "\n",
        "    print(f\"\\n  [Ensemble] F1: {ensemble_f1:.4f}, AUC: {ensemble_auc:.4f}, FPR: {fpr:.4f}\")\n",
        "\n",
        "    final_results['fold_details'].append({\n",
        "        'fold': fold,\n",
        "        'lgbm_f1': lgbm_f1,\n",
        "        'xgb_f1': xgb_f1,\n",
        "        'cat_f1': cat_f1,\n",
        "        'ensemble_f1': ensemble_f1,\n",
        "        'ensemble_auc': ensemble_auc,\n",
        "        'fpr': fpr\n",
        "    })\n",
        "\n",
        "# ============================================================\n",
        "# 6.4 ìµœì¢… ê²€ì¦ ì„±ëŠ¥\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ìµœì¢… ê²€ì¦ ì„±ëŠ¥ (OOF)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# OOF ì„±ëŠ¥\n",
        "oof_f1 = f1_score(y_train, final_results['ensemble']['oof_predictions'])\n",
        "oof_auc = roc_auc_score(y_train, final_results['ensemble']['oof_probabilities'])\n",
        "\n",
        "oof_cm = confusion_matrix(y_train, final_results['ensemble']['oof_predictions'])\n",
        "tn, fp, fn, tp = oof_cm.ravel()\n",
        "oof_fpr = fp / (fp + tn)\n",
        "oof_precision = tp / (tp + fp)\n",
        "oof_recall = tp / (tp + fn)\n",
        "\n",
        "print(f\"\\n[Ensemble OOF ì„±ëŠ¥]\")\n",
        "print(f\"  F1 Score:  {oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {oof_auc:.4f}\")\n",
        "print(f\"  Precision: {oof_precision:.4f}\")\n",
        "print(f\"  Recall:    {oof_recall:.4f}\")\n",
        "print(f\"  FPR:       {oof_fpr:.4f} ({oof_fpr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[OOF í˜¼ë™ í–‰ë ¬]\")\n",
        "print(f\"              ì˜ˆì¸¡: 0    ì˜ˆì¸¡: 1\")\n",
        "print(f\"  ì‹¤ì œ: 0  |   {oof_cm[0,0]:4d}      {oof_cm[0,1]:4d}\")\n",
        "print(f\"  ì‹¤ì œ: 1  |   {oof_cm[1,0]:4d}      {oof_cm[1,1]:4d}\")\n",
        "\n",
        "# Low Confidence\n",
        "confidence = np.abs(final_results['ensemble']['oof_probabilities'] - 0.5)\n",
        "low_conf_mask = confidence < 0.1\n",
        "n_low_conf = low_conf_mask.sum()\n",
        "low_conf_acc = (\n",
        "    final_results['ensemble']['oof_predictions'][low_conf_mask] == y_train[low_conf_mask]\n",
        ").mean() if n_low_conf > 0 else 0\n",
        "\n",
        "print(f\"\\n[Low Confidence ìƒ˜í”Œ]\")\n",
        "print(f\"  ê°œìˆ˜: {n_low_conf}ê°œ ({n_low_conf/len(y_train)*100:.2f}%)\")\n",
        "print(f\"  ì •í™•ë„: {low_conf_acc:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6.5 Test ë°ì´í„° ì˜ˆì¸¡\n",
        "# ============================================================\n",
        "if test_available:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Test ë°ì´í„° ì˜ˆì¸¡\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # ì „ì²´ Train ë°ì´í„°ë¡œ ìµœì¢… ì „ì²˜ë¦¬ê¸° í•™ìŠµ\n",
        "    Xt_train_full = preprocessor.fit_transform(X_train)\n",
        "    Xt_test = preprocessor.transform(X_test)\n",
        "\n",
        "    print(f\"\\n  Test ë°ì´í„° shape: {Xt_test.shape}\")\n",
        "\n",
        "    # ê° Fold ëª¨ë¸ë¡œ ì˜ˆì¸¡ í›„ ì•™ìƒë¸”\n",
        "    test_predictions = {\n",
        "        'lgbm': np.zeros((len(X_test), 5)),\n",
        "        'xgb': np.zeros((len(X_test), 5)),\n",
        "        'catboost': np.zeros((len(X_test), 5))\n",
        "    }\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"\\r  Fold {fold+1}/5 ëª¨ë¸ë¡œ ì˜ˆì¸¡ ì¤‘...\", end='')\n",
        "\n",
        "        test_predictions['lgbm'][:, fold] = final_results['lgbm']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "        test_predictions['xgb'][:, fold] = final_results['xgb']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "        test_predictions['catboost'][:, fold] = final_results['catboost']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "    print(f\"\\r  âœ“ 5-Fold ì˜ˆì¸¡ ì™„ë£Œ\")\n",
        "\n",
        "    # í‰ê·  í™•ë¥ \n",
        "    lgbm_proba_test = test_predictions['lgbm'].mean(axis=1)\n",
        "    xgb_proba_test = test_predictions['xgb'].mean(axis=1)\n",
        "    cat_proba_test = test_predictions['catboost'].mean(axis=1)\n",
        "\n",
        "    # Ensemble\n",
        "    ensemble_proba_test = (\n",
        "        0.25 * lgbm_proba_test +\n",
        "        0.50 * xgb_proba_test +\n",
        "        0.25 * cat_proba_test\n",
        "    )\n",
        "\n",
        "    # ìµœì¢… ì˜ˆì¸¡ (Threshold ì ìš©)\n",
        "    ensemble_pred_test = (ensemble_proba_test >= OPTIMAL_THRESHOLD).astype(int)\n",
        "\n",
        "    # Confidence ê³„ì‚°\n",
        "    confidence_test = np.abs(ensemble_proba_test - 0.5)\n",
        "\n",
        "    print(f\"\\n[Test ì˜ˆì¸¡ ê²°ê³¼]\")\n",
        "    print(f\"  ì˜ˆì¸¡ Class 0: {sum(ensemble_pred_test == 0)}ê°œ\")\n",
        "    print(f\"  ì˜ˆì¸¡ Class 1: {sum(ensemble_pred_test == 1)}ê°œ\")\n",
        "    print(f\"  í‰ê·  Confidence: {confidence_test.mean():.4f}\")\n",
        "    print(f\"  Low Confidence (<0.1): {sum(confidence_test < 0.1)}ê°œ ({sum(confidence_test < 0.1)/len(confidence_test)*100:.2f}%)\")\n",
        "\n",
        "    # ============================================================\n",
        "    # 6.6 Submission íŒŒì¼ ìƒì„±\n",
        "    # ============================================================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Submission íŒŒì¼ ìƒì„±\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # ê¸°ë³¸ Submission\n",
        "    submission = pd.DataFrame({\n",
        "        'id': range(len(ensemble_pred_test)),\n",
        "        'label': ensemble_pred_test\n",
        "    })\n",
        "\n",
        "    submission.to_csv('submission_final_top300.csv', index=False)\n",
        "    print(f\"\\nâœ“ ê¸°ë³¸ ì œì¶œ íŒŒì¼: submission_final_top300.csv\")\n",
        "\n",
        "    # ìƒì„¸ Submission (í™•ë¥  í¬í•¨)\n",
        "    submission_detailed = pd.DataFrame({\n",
        "        'id': range(len(ensemble_pred_test)),\n",
        "        'label': ensemble_pred_test,\n",
        "        'probability': ensemble_proba_test,\n",
        "        'confidence': confidence_test,\n",
        "        'lgbm_proba': lgbm_proba_test,\n",
        "        'xgb_proba': xgb_proba_test,\n",
        "        'catboost_proba': cat_proba_test\n",
        "    })\n",
        "\n",
        "    submission_detailed.to_csv('submission_detailed_final_top300.csv', index=False)\n",
        "    print(f\"âœ“ ìƒì„¸ ì œì¶œ íŒŒì¼: submission_detailed_final_top300.csv\")\n",
        "\n",
        "    # í†µê³„\n",
        "    print(f\"\\n[ì œì¶œ íŒŒì¼ í†µê³„]\")\n",
        "    print(f\"  ì „ì²´ ìƒ˜í”Œ: {len(submission)}ê°œ\")\n",
        "    print(f\"  Class 0 (ë…ì„±): {sum(submission['label'] == 0)}ê°œ ({sum(submission['label'] == 0)/len(submission)*100:.2f}%)\")\n",
        "    print(f\"  Class 1 (ë¬´ë…ì„±): {sum(submission['label'] == 1)}ê°œ ({sum(submission['label'] == 1)/len(submission)*100:.2f}%)\")\n",
        "\n",
        "# ============================================================\n",
        "# 6.7 ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ìµœì¢… ëª¨ë¸ ì €ì¥\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# ëª¨ë¸ ë° ì„¤ì • ì €ì¥\n",
        "final_model_package = {\n",
        "    'lgbm_models': final_results['lgbm']['models'],\n",
        "    'xgb_models': final_results['xgb']['models'],\n",
        "    'catboost_models': final_results['catboost']['models'],\n",
        "    'selected_features': selected_features,\n",
        "    'preprocessor': preprocessor,\n",
        "    'optimal_threshold': OPTIMAL_THRESHOLD,\n",
        "    'oof_f1': oof_f1,\n",
        "    'oof_auc': oof_auc,\n",
        "    'oof_fpr': oof_fpr\n",
        "}\n",
        "\n",
        "with open('final_model_top300.pkl', 'wb') as f:\n",
        "    pickle.dump(final_model_package, f)\n",
        "\n",
        "print(f\"\\nâœ“ ëª¨ë¸ ì €ì¥: final_model_top300.pkl\")\n",
        "print(f\"  - 5-Fold Ã— 3-Model = 15ê°œ ëª¨ë¸\")\n",
        "print(f\"  - Top 300 í”¼ì²˜ ë¦¬ìŠ¤íŠ¸\")\n",
        "print(f\"  - ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\")\n",
        "print(f\"  - ìµœì  Threshold: {OPTIMAL_THRESHOLD}\")\n",
        "\n",
        "# í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ ë³„ë„ ì €ì¥\n",
        "pd.DataFrame({'feature': selected_features}).to_csv('selected_features_top300.csv', index=False)\n",
        "print(f\"âœ“ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸: selected_features_top300.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 6.8 ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[ëª¨ë¸ ì‚¬ì–‘]\")\n",
        "print(f\"  í”¼ì²˜ ìˆ˜: 300ê°œ (ì••ì¶•ë¥  90.2%)\")\n",
        "print(f\"  Threshold: {OPTIMAL_THRESHOLD}\")\n",
        "print(f\"  Ensemble: LGBM(25%) + XGB(50%) + CAT(25%)\")\n",
        "\n",
        "print(f\"\\n[ê²€ì¦ ì„±ëŠ¥ (5-Fold CV OOF)]\")\n",
        "print(f\"  F1 Score:  {oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {oof_auc:.4f}\")\n",
        "print(f\"  Precision: {oof_precision:.4f}\")\n",
        "print(f\"  Recall:    {oof_recall:.4f}\")\n",
        "print(f\"  FPR:       {oof_fpr*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n[Foldë³„ ì„±ëŠ¥]\")\n",
        "print(f\"{'Fold':<6} {'LGBM F1':<10} {'XGB F1':<10} {'CAT F1':<10} {'Ensemble':<10} {'FPR':<8}\")\n",
        "print(f\"{'-'*60}\")\n",
        "for detail in final_results['fold_details']:\n",
        "    print(f\"{detail['fold']:<6} {detail['lgbm_f1']:<10.4f} {detail['xgb_f1']:<10.4f} \"\n",
        "          f\"{detail['cat_f1']:<10.4f} {detail['ensemble_f1']:<10.4f} {detail['fpr']*100:<8.2f}%\")\n",
        "\n",
        "print(f\"\\n[2ë‹¨ê³„ (3076ê°œ í”¼ì²˜) ëŒ€ë¹„]\")\n",
        "baseline_f1 = 0.8317\n",
        "baseline_fpr = 0.2514\n",
        "print(f\"  F1 Score:  {baseline_f1:.4f} â†’ {oof_f1:.4f} ({(oof_f1-baseline_f1)*100:+.2f}%p)\")\n",
        "print(f\"  FPR:       {baseline_fpr*100:.2f}% â†’ {oof_fpr*100:.2f}% ({(oof_fpr-baseline_fpr)*100:+.2f}%p)\")\n",
        "print(f\"  í”¼ì²˜ ìˆ˜:    3076ê°œ â†’ 300ê°œ (-90.2%)\")\n",
        "\n",
        "if test_available:\n",
        "    print(f\"\\n[Test ì˜ˆì¸¡]\")\n",
        "    print(f\"  ì˜ˆì¸¡ ì™„ë£Œ: {len(ensemble_pred_test)}ê°œ\")\n",
        "    print(f\"  Class 1 ë¹„ìœ¨: {sum(ensemble_pred_test == 1)/len(ensemble_pred_test)*100:.2f}%\")\n",
        "    print(f\"  ì œì¶œ íŒŒì¼: submission_final_top300.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ“ ë‹¨ê³„ 6 ì™„ë£Œ - ìµœì¢… ëª¨ë¸ êµ¬ì„± ì™„ë£Œ\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\n[ë‹¤ìŒ ë‹¨ê³„]\")\n",
        "if test_available:\n",
        "    print(f\"  1. submission_final_top300.csv ì œì¶œ\")\n",
        "    print(f\"  2. ì„±ëŠ¥ í™•ì¸ í›„ í”¼ë“œë°±\")\n",
        "    print(f\"  3. í•„ìš” ì‹œ Threshold ì¬ì¡°ì •\")\n",
        "else:\n",
        "    print(f\"  1. predict_input.csv ì¤€ë¹„\")\n",
        "    print(f\"  2. ì½”ë“œ ì¬ì‹¤í–‰\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPR4jnZEthMo",
        "outputId": "f33db1ec-4f44-4bbe-beca-e41fd7428ab6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ë‹¨ê³„ 6: ìµœì¢… ëª¨ë¸ êµ¬ì„± ë° Test ì˜ˆì¸¡ (Top 300)\n",
            "======================================================================\n",
            "\n",
            "[ìµœì¢… ì„¤ì •]\n",
            "  í”¼ì²˜ ìˆ˜: 300ê°œ (ìµœê³  ì„±ëŠ¥)\n",
            "  Threshold: 0.390 (5ë‹¨ê³„ ê²°ê³¼)\n",
            "  Ensemble: LGBM(25%) + XGB(50%) + CAT(25%)\n",
            "  ëª©í‘œ F1: 0.8303, AUC: 0.8925\n",
            "\n",
            "======================================================================\n",
            "ë°ì´í„° ì¤€ë¹„\n",
            "======================================================================\n",
            "\n",
            "[Top 300 í”¼ì²˜]\n",
            "  ì´ í”¼ì²˜: 300ê°œ\n",
            "  - Descriptor: 4ê°œ\n",
            "  - Fingerprint: 296ê°œ\n",
            "    Â· ECFP: 92ê°œ\n",
            "    Â· FCFP: 79ê°œ\n",
            "    Â· PTFP: 125ê°œ\n",
            "\n",
            "[Train ë°ì´í„°]\n",
            "  Shape: (8349, 300)\n",
            "  Label ë¶„í¬: Class 0 = 3807, Class 1 = 4542\n",
            "\n",
            "[Test ë°ì´í„°]\n",
            "  Shape: (927, 300)\n",
            "\n",
            "======================================================================\n",
            "ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ (5-Fold CV)\n",
            "======================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 1/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 300), ê²€ì¦: (1670, 300)\n",
            "  [1/3] LightGBM... F1: 0.8388, Iter: 655\n",
            "  [2/3] XGBoost... F1: 0.8507, Iter: 956\n",
            "  [3/3] CatBoost... F1: 0.8385, Iter: 994\n",
            "\n",
            "  [Ensemble] F1: 0.8464, AUC: 0.9065, FPR: 0.2313\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 2/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 300), ê²€ì¦: (1670, 300)\n",
            "  [1/3] LightGBM... F1: 0.8199, Iter: 583\n",
            "  [2/3] XGBoost... F1: 0.8189, Iter: 686\n",
            "  [3/3] CatBoost... F1: 0.8271, Iter: 929\n",
            "\n",
            "  [Ensemble] F1: 0.8230, AUC: 0.8859, FPR: 0.2694\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 3/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 300), ê²€ì¦: (1670, 300)\n",
            "  [1/3] LightGBM... F1: 0.8070, Iter: 562\n",
            "  [2/3] XGBoost... F1: 0.8026, Iter: 641\n",
            "  [3/3] CatBoost... F1: 0.8056, Iter: 976\n",
            "\n",
            "  [Ensemble] F1: 0.8033, AUC: 0.8738, FPR: 0.2454\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 4/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6679, 300), ê²€ì¦: (1670, 300)\n",
            "  [1/3] LightGBM... F1: 0.8318, Iter: 581\n",
            "  [2/3] XGBoost... F1: 0.8321, Iter: 738\n",
            "  [3/3] CatBoost... F1: 0.8280, Iter: 998\n",
            "\n",
            "  [Ensemble] F1: 0.8334, AUC: 0.8934, FPR: 0.2651\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Fold 5/5\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  í•™ìŠµ: (6680, 300), ê²€ì¦: (1669, 300)\n",
            "  [1/3] LightGBM... F1: 0.8391, Iter: 675\n",
            "  [2/3] XGBoost... F1: 0.8430, Iter: 980\n",
            "  [3/3] CatBoost... F1: 0.8378, Iter: 999\n",
            "\n",
            "  [Ensemble] F1: 0.8449, AUC: 0.9023, FPR: 0.2260\n",
            "\n",
            "======================================================================\n",
            "ìµœì¢… ê²€ì¦ ì„±ëŠ¥ (OOF)\n",
            "======================================================================\n",
            "\n",
            "[Ensemble OOF ì„±ëŠ¥]\n",
            "  F1 Score:  0.8303\n",
            "  AUC Score: 0.8925\n",
            "  Precision: 0.8052\n",
            "  Recall:    0.8571\n",
            "  FPR:       0.2474 (24.74%)\n",
            "\n",
            "[OOF í˜¼ë™ í–‰ë ¬]\n",
            "              ì˜ˆì¸¡: 0    ì˜ˆì¸¡: 1\n",
            "  ì‹¤ì œ: 0  |   2865       942\n",
            "  ì‹¤ì œ: 1  |    649      3893\n",
            "\n",
            "[Low Confidence ìƒ˜í”Œ]\n",
            "  ê°œìˆ˜: 972ê°œ (11.64%)\n",
            "  ì •í™•ë„: 0.5514\n",
            "\n",
            "======================================================================\n",
            "Test ë°ì´í„° ì˜ˆì¸¡\n",
            "======================================================================\n",
            "\n",
            "  Test ë°ì´í„° shape: (927, 300)\n",
            "  âœ“ 5-Fold ì˜ˆì¸¡ ì™„ë£Œ\n",
            "\n",
            "[Test ì˜ˆì¸¡ ê²°ê³¼]\n",
            "  ì˜ˆì¸¡ Class 0: 378ê°œ\n",
            "  ì˜ˆì¸¡ Class 1: 549ê°œ\n",
            "  í‰ê·  Confidence: 0.3078\n",
            "  Low Confidence (<0.1): 128ê°œ (13.81%)\n",
            "\n",
            "======================================================================\n",
            "Submission íŒŒì¼ ìƒì„±\n",
            "======================================================================\n",
            "\n",
            "âœ“ ê¸°ë³¸ ì œì¶œ íŒŒì¼: submission_final_top300.csv\n",
            "âœ“ ìƒì„¸ ì œì¶œ íŒŒì¼: submission_detailed_final_top300.csv\n",
            "\n",
            "[ì œì¶œ íŒŒì¼ í†µê³„]\n",
            "  ì „ì²´ ìƒ˜í”Œ: 927ê°œ\n",
            "  Class 0 (ë…ì„±): 378ê°œ (40.78%)\n",
            "  Class 1 (ë¬´ë…ì„±): 549ê°œ (59.22%)\n",
            "\n",
            "======================================================================\n",
            "ìµœì¢… ëª¨ë¸ ì €ì¥\n",
            "======================================================================\n",
            "\n",
            "âœ“ ëª¨ë¸ ì €ì¥: final_model_top300.pkl\n",
            "  - 5-Fold Ã— 3-Model = 15ê°œ ëª¨ë¸\n",
            "  - Top 300 í”¼ì²˜ ë¦¬ìŠ¤íŠ¸\n",
            "  - ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
            "  - ìµœì  Threshold: 0.39\n",
            "âœ“ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸: selected_features_top300.csv\n",
            "\n",
            "======================================================================\n",
            "ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸\n",
            "======================================================================\n",
            "\n",
            "[ëª¨ë¸ ì‚¬ì–‘]\n",
            "  í”¼ì²˜ ìˆ˜: 300ê°œ (ì••ì¶•ë¥  90.2%)\n",
            "  Threshold: 0.39\n",
            "  Ensemble: LGBM(25%) + XGB(50%) + CAT(25%)\n",
            "\n",
            "[ê²€ì¦ ì„±ëŠ¥ (5-Fold CV OOF)]\n",
            "  F1 Score:  0.8303\n",
            "  AUC Score: 0.8925\n",
            "  Precision: 0.8052\n",
            "  Recall:    0.8571\n",
            "  FPR:       24.74%\n",
            "\n",
            "[Foldë³„ ì„±ëŠ¥]\n",
            "Fold   LGBM F1    XGB F1     CAT F1     Ensemble   FPR     \n",
            "------------------------------------------------------------\n",
            "1      0.8388     0.8507     0.8385     0.8464     23.13   %\n",
            "2      0.8199     0.8189     0.8271     0.8230     26.94   %\n",
            "3      0.8070     0.8026     0.8056     0.8033     24.54   %\n",
            "4      0.8318     0.8321     0.8280     0.8334     26.51   %\n",
            "5      0.8391     0.8430     0.8378     0.8449     22.60   %\n",
            "\n",
            "[2ë‹¨ê³„ (3076ê°œ í”¼ì²˜) ëŒ€ë¹„]\n",
            "  F1 Score:  0.8317 â†’ 0.8303 (-0.14%p)\n",
            "  FPR:       25.14% â†’ 24.74% (-0.40%p)\n",
            "  í”¼ì²˜ ìˆ˜:    3076ê°œ â†’ 300ê°œ (-90.2%)\n",
            "\n",
            "[Test ì˜ˆì¸¡]\n",
            "  ì˜ˆì¸¡ ì™„ë£Œ: 927ê°œ\n",
            "  Class 1 ë¹„ìœ¨: 59.22%\n",
            "  ì œì¶œ íŒŒì¼: submission_final_top300.csv\n",
            "\n",
            "======================================================================\n",
            "âœ“ ë‹¨ê³„ 6 ì™„ë£Œ - ìµœì¢… ëª¨ë¸ êµ¬ì„± ì™„ë£Œ\n",
            "======================================================================\n",
            "\n",
            "[ë‹¤ìŒ ë‹¨ê³„]\n",
            "  1. submission_final_top300.csv ì œì¶œ\n",
            "  2. ì„±ëŠ¥ í™•ì¸ í›„ í”¼ë“œë°±\n",
            "  3. í•„ìš” ì‹œ Threshold ì¬ì¡°ì •\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Submission íŒŒì¼ í˜•ì‹ ë³€í™˜ (SMILES + output)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Submission íŒŒì¼ í˜•ì‹ ë³€í™˜\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================================\n",
        "# 1. ê¸°ì¡´ submission íŒŒì¼ ë¡œë“œ\n",
        "# ============================================================\n",
        "print(\"\\n[1ë‹¨ê³„] íŒŒì¼ ë¡œë“œ\")\n",
        "\n",
        "# 6ë‹¨ê³„ì—ì„œ ìƒì„±í•œ submission íŒŒì¼\n",
        "submission = pd.read_csv('submission_final_top300.csv')\n",
        "print(f\"  ê¸°ì¡´ submission: {submission.shape}\")\n",
        "print(f\"  ì»¬ëŸ¼: {submission.columns.tolist()}\")\n",
        "\n",
        "# Test ë°ì´í„°ì—ì„œ SMILES ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "test_data = pd.read_csv('predict_input.csv')\n",
        "\n",
        "# SMILES ì»¬ëŸ¼ í™•ì¸\n",
        "if 'SMILES' in test_data.columns:\n",
        "    smiles_col = 'SMILES'\n",
        "elif 'smiles' in test_data.columns:\n",
        "    smiles_col = 'smiles'\n",
        "else:\n",
        "    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ SMILESì¼ ê°€ëŠ¥ì„±\n",
        "    smiles_col = test_data.columns[0]\n",
        "    print(f\"  âš ï¸  'SMILES' ì»¬ëŸ¼ ì—†ìŒ, '{smiles_col}' ì‚¬ìš©\")\n",
        "\n",
        "print(f\"  Test ë°ì´í„°: {test_data.shape}\")\n",
        "print(f\"  SMILES ì»¬ëŸ¼: {smiles_col}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. í˜•ì‹ ë³€í™˜\n",
        "# ============================================================\n",
        "print(f\"\\n[2ë‹¨ê³„] í˜•ì‹ ë³€í™˜\")\n",
        "\n",
        "# ìƒˆë¡œìš´ submission í˜•ì‹ ìƒì„±\n",
        "submission_final = pd.DataFrame({\n",
        "    'SMILES': test_data[smiles_col],\n",
        "    'output': submission['label']\n",
        "})\n",
        "\n",
        "print(f\"  ë³€í™˜ ì™„ë£Œ: {submission_final.shape}\")\n",
        "print(f\"  ì»¬ëŸ¼: {submission_final.columns.tolist()}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. í†µê³„ í™•ì¸\n",
        "# ============================================================\n",
        "print(f\"\\n[3ë‹¨ê³„] í†µê³„ í™•ì¸\")\n",
        "\n",
        "print(f\"\\n  ì˜ˆì¸¡ ë¶„í¬:\")\n",
        "print(f\"    output=0 (ë…ì„±): {sum(submission_final['output'] == 0)}ê°œ ({sum(submission_final['output'] == 0)/len(submission_final)*100:.2f}%)\")\n",
        "print(f\"    output=1 (ë¬´ë…ì„±): {sum(submission_final['output'] == 1)}ê°œ ({sum(submission_final['output'] == 1)/len(submission_final)*100:.2f}%)\")\n",
        "\n",
        "# ìƒ˜í”Œ í™•ì¸\n",
        "print(f\"\\n  ì²˜ìŒ 10ê°œ ìƒ˜í”Œ:\")\n",
        "print(submission_final.head(10).to_string(index=False))\n",
        "\n",
        "# ============================================================\n",
        "# 4. íŒŒì¼ ì €ì¥\n",
        "# ============================================================\n",
        "print(f\"\\n[4ë‹¨ê³„] íŒŒì¼ ì €ì¥\")\n",
        "\n",
        "# ìµœì¢… ì œì¶œ íŒŒì¼\n",
        "submission_final.to_csv('submission_final_format.csv', index=False)\n",
        "print(f\"  âœ“ ì €ì¥ ì™„ë£Œ: submission_final_format.csv\")\n",
        "\n",
        "# ê²€ì¦: íŒŒì¼ ë‹¤ì‹œ ì½ì–´ì„œ í™•ì¸\n",
        "verify = pd.read_csv('submission_final_format.csv')\n",
        "print(f\"\\n[ê²€ì¦]\")\n",
        "print(f\"  íŒŒì¼ í¬ê¸°: {len(verify)}í–‰ Ã— {len(verify.columns)}ì—´\")\n",
        "print(f\"  ì»¬ëŸ¼: {verify.columns.tolist()}\")\n",
        "print(f\"  ì²« 5í–‰:\")\n",
        "print(verify.head().to_string(index=False))\n",
        "\n",
        "# ============================================================\n",
        "# 5. ì˜ˆìƒ ì œì¶œ í˜•ì‹ ë§¤ì¹­ í™•ì¸\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ì œì¶œ í˜•ì‹ ìµœì¢… í™•ì¸\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[ìš”êµ¬ í˜•ì‹ (ì‚¬ì§„ ê¸°ì¤€)]\")\n",
        "print(f\"  ì»¬ëŸ¼ 1: SMILES (ë¶„ì êµ¬ì¡°)\")\n",
        "print(f\"  ì»¬ëŸ¼ 2: output (ì˜ˆì¸¡ê°’ 0 ë˜ëŠ” 1)\")\n",
        "\n",
        "print(f\"\\n[ìƒì„±ëœ íŒŒì¼]\")\n",
        "print(f\"  ì»¬ëŸ¼ 1: {verify.columns[0]} âœ“\")\n",
        "print(f\"  ì»¬ëŸ¼ 2: {verify.columns[1]} âœ“\")\n",
        "\n",
        "if verify.columns[0] == 'SMILES' and verify.columns[1] == 'output':\n",
        "    print(f\"\\nâœ“âœ“âœ“ í˜•ì‹ ë§¤ì¹­ ì™„ë£Œ - ì œì¶œ ê°€ëŠ¥!\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  ì»¬ëŸ¼ëª… í™•ì¸ í•„ìš”\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ“ ë³€í™˜ ì™„ë£Œ - submission_final_format.csv ì œì¶œí•˜ì„¸ìš”!\")\n",
        "print(f\"{'='*70}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnF3UMxtwLwH",
        "outputId": "64448a6d-5710-4aa7-be20-2cdfb87a8693"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Submission íŒŒì¼ í˜•ì‹ ë³€í™˜\n",
            "======================================================================\n",
            "\n",
            "[1ë‹¨ê³„] íŒŒì¼ ë¡œë“œ\n",
            "  ê¸°ì¡´ submission: (927, 2)\n",
            "  ì»¬ëŸ¼: ['id', 'label']\n",
            "  Test ë°ì´í„°: (927, 3077)\n",
            "  SMILES ì»¬ëŸ¼: SMILES\n",
            "\n",
            "[2ë‹¨ê³„] í˜•ì‹ ë³€í™˜\n",
            "  ë³€í™˜ ì™„ë£Œ: (927, 2)\n",
            "  ì»¬ëŸ¼: ['SMILES', 'output']\n",
            "\n",
            "[3ë‹¨ê³„] í†µê³„ í™•ì¸\n",
            "\n",
            "  ì˜ˆì¸¡ ë¶„í¬:\n",
            "    output=0 (ë…ì„±): 378ê°œ (40.78%)\n",
            "    output=1 (ë¬´ë…ì„±): 549ê°œ (59.22%)\n",
            "\n",
            "  ì²˜ìŒ 10ê°œ ìƒ˜í”Œ:\n",
            "                                          SMILES  output\n",
            "                           OC(=O)c1cc2sccc2[nH]1       1\n",
            "                     [O-][n+]1onc(c2ccccc2)c1C#N       1\n",
            "                      CN1C(=O)N(C)c2ncn(C)c2C1=O       1\n",
            "                          Clc1cccc(c1)C2CNCC=CC2       1\n",
            "                      CCN(CC)CC(=O)Nc1c(C)cccc1C       1\n",
            "                      CCN(CC)CCNC(=O)c1ccc(N)cc1       1\n",
            "NC[C@H]1C[C@@H]1c2cc(Cl)ccc2OCC=C.OC(=O)C(F)(F)F       1\n",
            "                         Clc1ccc(cc1Cl)C2CCCCNC2       1\n",
            "                NC(=O)C1CCCc2c1[nH]c3ccc(Cl)cc23       1\n",
            "        O=C1NOC(=C1)[C@H]2CCN[C@@H](Cc3ccccc3)C2       1\n",
            "\n",
            "[4ë‹¨ê³„] íŒŒì¼ ì €ì¥\n",
            "  âœ“ ì €ì¥ ì™„ë£Œ: submission_final_format.csv\n",
            "\n",
            "[ê²€ì¦]\n",
            "  íŒŒì¼ í¬ê¸°: 927í–‰ Ã— 2ì—´\n",
            "  ì»¬ëŸ¼: ['SMILES', 'output']\n",
            "  ì²« 5í–‰:\n",
            "                     SMILES  output\n",
            "      OC(=O)c1cc2sccc2[nH]1       1\n",
            "[O-][n+]1onc(c2ccccc2)c1C#N       1\n",
            " CN1C(=O)N(C)c2ncn(C)c2C1=O       1\n",
            "     Clc1cccc(c1)C2CNCC=CC2       1\n",
            " CCN(CC)CC(=O)Nc1c(C)cccc1C       1\n",
            "\n",
            "======================================================================\n",
            "ì œì¶œ í˜•ì‹ ìµœì¢… í™•ì¸\n",
            "======================================================================\n",
            "\n",
            "[ìš”êµ¬ í˜•ì‹ (ì‚¬ì§„ ê¸°ì¤€)]\n",
            "  ì»¬ëŸ¼ 1: SMILES (ë¶„ì êµ¬ì¡°)\n",
            "  ì»¬ëŸ¼ 2: output (ì˜ˆì¸¡ê°’ 0 ë˜ëŠ” 1)\n",
            "\n",
            "[ìƒì„±ëœ íŒŒì¼]\n",
            "  ì»¬ëŸ¼ 1: SMILES âœ“\n",
            "  ì»¬ëŸ¼ 2: output âœ“\n",
            "\n",
            "âœ“âœ“âœ“ í˜•ì‹ ë§¤ì¹­ ì™„ë£Œ - ì œì¶œ ê°€ëŠ¥!\n",
            "\n",
            "======================================================================\n",
            "âœ“ ë³€í™˜ ì™„ë£Œ - submission_final_format.csv ì œì¶œí•˜ì„¸ìš”!\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}