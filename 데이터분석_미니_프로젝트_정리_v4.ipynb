{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoYCzyZM6C9-",
        "outputId": "84c2eaec-bf92-4690-9d24-56c73d097cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Top 500 ÌîºÏ≤ò Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Top 500 ÌîºÏ≤ò Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================================\n",
        "# 1. Top 500 ÌîºÏ≤ò ÏÑ†Ï†ï\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Top 500 ÌîºÏ≤ò ÏÑ†Ï†ï\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Feature importance Î°úÎìú\n",
        "importance_df = pd.read_csv('feature_importance_ensemble_cv.csv')\n",
        "\n",
        "# Top 500 ÏÑ†ÌÉù\n",
        "N_FEATURES = 500\n",
        "selected_features = importance_df.head(N_FEATURES)['feature'].tolist()\n",
        "\n",
        "print(f\"\\n[ÏÑ†Ï†ïÎêú ÌîºÏ≤ò]\")\n",
        "print(f\"  Ï¥ù ÌîºÏ≤ò: {len(selected_features)}Í∞ú\")\n",
        "\n",
        "# ÌÉÄÏûÖÎ≥Ñ Î∂ÑÌè¨\n",
        "fp_cols = [f for f in selected_features if f.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = [f for f in selected_features if f in ['MolWt', 'clogp', 'sa_score', 'qed']]\n",
        "\n",
        "ecfp_count = len([f for f in fp_cols if f.startswith('ecfp_')])\n",
        "fcfp_count = len([f for f in fp_cols if f.startswith('fcfp_')])\n",
        "ptfp_count = len([f for f in fp_cols if f.startswith('ptfp_')])\n",
        "\n",
        "print(f\"  - Descriptor: {len(desc_cols)}Í∞ú\")\n",
        "print(f\"  - Fingerprint: {len(fp_cols)}Í∞ú\")\n",
        "print(f\"    ¬∑ ECFP: {ecfp_count}Í∞ú\")\n",
        "print(f\"    ¬∑ FCFP: {fcfp_count}Í∞ú\")\n",
        "print(f\"    ¬∑ PTFP: {ptfp_count}Í∞ú\")\n",
        "\n",
        "# ÎàÑÏ†Å Ï§ëÏöîÎèÑ Í≥ÑÏÇ∞\n",
        "cumsum_importance = importance_df.head(N_FEATURES)['ensemble_mean'].sum()\n",
        "total_importance = importance_df['ensemble_mean'].sum()\n",
        "cumsum_pct = cumsum_importance / total_importance * 100\n",
        "\n",
        "print(f\"\\n[ÎàÑÏ†Å Ï§ëÏöîÎèÑ]\")\n",
        "print(f\"  Top 500 Ï§ëÏöîÎèÑ Ìï©: {cumsum_importance:.2f}\")\n",
        "print(f\"  Ï†ÑÏ≤¥ Ï§ëÏöîÎèÑ Ìï©: {total_importance:.2f}\")\n",
        "print(f\"  ÎàÑÏ†Å ÎπÑÏú®: {cumsum_pct:.2f}%\")\n",
        "\n",
        "# CSV Ï†ÄÏû•\n",
        "pd.DataFrame({'feature': selected_features}).to_csv('selected_features_top500.csv', index=False)\n",
        "print(f\"\\n‚úì ÌîºÏ≤ò Î¶¨Ïä§Ìä∏ Ï†ÄÏû•: selected_features_top500.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Îç∞Ïù¥ÌÑ∞ Î°úÎìú\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Train Îç∞Ïù¥ÌÑ∞\n",
        "df_train = pd.read_csv('train.csv')\n",
        "X_train = df_train[selected_features]\n",
        "y_train = df_train['label'].astype(int)\n",
        "\n",
        "print(f\"\\n[Train Îç∞Ïù¥ÌÑ∞]\")\n",
        "print(f\"  Shape: {X_train.shape}\")\n",
        "print(f\"  Label Î∂ÑÌè¨: Class 0 = {sum(y_train==0)}, Class 1 = {sum(y_train==1)}\")\n",
        "\n",
        "# Test Îç∞Ïù¥ÌÑ∞\n",
        "try:\n",
        "    df_test = pd.read_csv('predict_input.csv')\n",
        "    X_test = df_test[selected_features]\n",
        "    print(f\"\\n[Test Îç∞Ïù¥ÌÑ∞]\")\n",
        "    print(f\"  Shape: {X_test.shape}\")\n",
        "    test_available = True\n",
        "except:\n",
        "    print(f\"\\n‚ö†Ô∏è  Test Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå\")\n",
        "    test_available = False\n",
        "\n",
        "# Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 3. 5-Fold CV ÌïôÏäµ\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"5-Fold Cross-Validation ÌïôÏäµ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# Í≤∞Í≥º Ï†ÄÏû•\n",
        "results = {\n",
        "    'lgbm': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'xgb': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'catboost': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'fold_details': []\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"üìä Fold {fold}/5\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
        "    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
        "\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    print(f\"  ÌïôÏäµ: {Xt_tr.shape}, Í≤ÄÏ¶ù: {Xt_va.shape}\")\n",
        "\n",
        "    # LightGBM\n",
        "    print(f\"  [1/3] LightGBM...\", end=' ')\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "        num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "        colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)],\n",
        "                   callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "\n",
        "    lgbm_proba = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "    results['lgbm']['models'].append(lgbm_model)\n",
        "    results['lgbm']['oof_probabilities'][va_idx] = lgbm_proba\n",
        "\n",
        "    print(f\"F1: {f1_score(y_va, (lgbm_proba >= 0.39).astype(int)):.4f}, Iter: {lgbm_model.best_iteration_}\")\n",
        "\n",
        "    # XGBoost\n",
        "    print(f\"  [2/3] XGBoost...\", end=' ')\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "        gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1,\n",
        "        early_stopping_rounds=100, eval_metric='logloss', verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)], verbose=False)\n",
        "\n",
        "    xgb_proba = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "    results['xgb']['models'].append(xgb_model)\n",
        "    results['xgb']['oof_probabilities'][va_idx] = xgb_proba\n",
        "\n",
        "    print(f\"F1: {f1_score(y_va, (xgb_proba >= 0.39).astype(int)):.4f}, Iter: {xgb_model.best_iteration}\")\n",
        "\n",
        "    # CatBoost\n",
        "    print(f\"  [3/3] CatBoost...\", end=' ')\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000, learning_rate=0.03, depth=7,\n",
        "        l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE, verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "    cat_model.fit(Xt_tr, y_tr, eval_set=(Xt_va, y_va), verbose=False)\n",
        "\n",
        "    cat_proba = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "    results['catboost']['models'].append(cat_model)\n",
        "    results['catboost']['oof_probabilities'][va_idx] = cat_proba\n",
        "\n",
        "    print(f\"F1: {f1_score(y_va, (cat_proba >= 0.39).astype(int)):.4f}, Iter: {cat_model.best_iteration_}\")\n",
        "\n",
        "    # Ensemble\n",
        "    ensemble_proba = 0.20 * lgbm_proba + 0.60 * xgb_proba + 0.20 * cat_proba\n",
        "\n",
        "    # Adaptive Threshold\n",
        "    confidence = np.abs(ensemble_proba - 0.5)\n",
        "    adaptive_thresholds = np.where(confidence < 0.05, 0.42,\n",
        "                                    np.where(confidence < 0.10, 0.40, 0.39))\n",
        "    ensemble_pred = (ensemble_proba >= adaptive_thresholds).astype(int)\n",
        "\n",
        "    ensemble_f1 = f1_score(y_va, ensemble_pred)\n",
        "    ensemble_auc = roc_auc_score(y_va, ensemble_proba)\n",
        "\n",
        "    cm = confusion_matrix(y_va, ensemble_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "\n",
        "    print(f\"\\n  [Ensemble] F1: {ensemble_f1:.4f}, AUC: {ensemble_auc:.4f}, FPR: {fpr:.4f}\")\n",
        "\n",
        "    results['fold_details'].append({\n",
        "        'fold': fold,\n",
        "        'ensemble_f1': ensemble_f1,\n",
        "        'ensemble_auc': ensemble_auc,\n",
        "        'fpr': fpr\n",
        "    })\n",
        "\n",
        "# ============================================================\n",
        "# 4. OOF ÏÑ±Îä• ÌèâÍ∞Ä\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"OOF ÏÑ±Îä• ÌèâÍ∞Ä\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Ensemble ÌôïÎ•†\n",
        "ensemble_oof_proba = (\n",
        "    0.20 * results['lgbm']['oof_probabilities'] +\n",
        "    0.60 * results['xgb']['oof_probabilities'] +\n",
        "    0.20 * results['catboost']['oof_probabilities']\n",
        ")\n",
        "\n",
        "# Adaptive Threshold Ï†ÅÏö©\n",
        "confidence_oof = np.abs(ensemble_oof_proba - 0.5)\n",
        "\n",
        "def get_adaptive_threshold(confidence):\n",
        "    if confidence < 0.05:\n",
        "        return 0.42\n",
        "    elif confidence < 0.10:\n",
        "        return 0.40\n",
        "    else:\n",
        "        return 0.39\n",
        "\n",
        "adaptive_thresholds_oof = np.array([get_adaptive_threshold(c) for c in confidence_oof])\n",
        "predictions_oof = (ensemble_oof_proba >= adaptive_thresholds_oof).astype(int)\n",
        "\n",
        "# ÏÑ±Îä• Í≥ÑÏÇ∞\n",
        "oof_f1 = f1_score(y_train, predictions_oof)\n",
        "oof_auc = roc_auc_score(y_train, ensemble_oof_proba)\n",
        "oof_cm = confusion_matrix(y_train, predictions_oof)\n",
        "tn, fp, fn, tp = oof_cm.ravel()\n",
        "oof_fpr = fp / (fp + tn)\n",
        "oof_precision = precision_score(y_train, predictions_oof)\n",
        "oof_recall = recall_score(y_train, predictions_oof)\n",
        "\n",
        "print(f\"\\n[Top 500 OOF ÏÑ±Îä•]\")\n",
        "print(f\"  F1 Score:  {oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {oof_auc:.4f}\")\n",
        "print(f\"  Precision: {oof_precision:.4f}\")\n",
        "print(f\"  Recall:    {oof_recall:.4f}\")\n",
        "print(f\"  FPR:       {oof_fpr:.4f} ({oof_fpr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[OOF ÌòºÎèô ÌñâÎ†¨]\")\n",
        "print(f\"  TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
        "\n",
        "# Low Confidence\n",
        "low_conf_mask = confidence_oof < 0.1\n",
        "n_low_conf = low_conf_mask.sum()\n",
        "low_conf_acc = (predictions_oof[low_conf_mask] == y_train[low_conf_mask]).mean() if n_low_conf > 0 else 0\n",
        "\n",
        "print(f\"\\n[Low Confidence]\")\n",
        "print(f\"  Í∞úÏàò: {n_low_conf}Í∞ú ({n_low_conf/len(y_train)*100:.2f}%)\")\n",
        "print(f\"  Ï†ïÌôïÎèÑ: {low_conf_acc:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. Top 300Í≥º ÎπÑÍµê\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Top 300 vs Top 500 ÎπÑÍµê\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Top 300 ÏÑ±Îä• (Í∏∞Ï§Ä)\n",
        "baseline_f1 = 0.8300\n",
        "baseline_fpr = 0.2467\n",
        "\n",
        "f1_improvement = oof_f1 - baseline_f1\n",
        "fpr_improvement = oof_fpr - baseline_fpr\n",
        "\n",
        "print(f\"\\n[ÏÑ±Îä• ÎπÑÍµê]\")\n",
        "print(f\"{'ÏßÄÌëú':<15} {'Top 300':<12} {'Top 500':<12} {'Î≥ÄÌôî':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'F1 Score':<15} {baseline_f1:<12.4f} {oof_f1:<12.4f} {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\")\n",
        "print(f\"{'FPR':<15} {baseline_fpr*100:<12.2f}% {oof_fpr*100:<12.2f}% {fpr_improvement*100:+.2f}%p\")\n",
        "print(f\"{'ÌîºÏ≤ò Ïàò':<15} {'300':<12} {'500':<12} {'+200'}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. Test ÏòàÏ∏° (Test Îç∞Ïù¥ÌÑ∞ ÏûàÏùÑ Í≤ΩÏö∞)\n",
        "# ============================================================\n",
        "if test_available:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Test Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Ï†ÑÏ≤¥ Train Îç∞Ïù¥ÌÑ∞Î°ú Ï†ÑÏ≤òÎ¶¨Í∏∞ ÌïôÏäµ\n",
        "    Xt_train_full = preprocessor.fit_transform(X_train)\n",
        "    Xt_test = preprocessor.transform(X_test)\n",
        "\n",
        "    print(f\"\\n  Test shape: {Xt_test.shape}\")\n",
        "\n",
        "    # Í∞Å Fold Î™®Îç∏Î°ú ÏòàÏ∏°\n",
        "    test_predictions = {\n",
        "        'lgbm': np.zeros((len(X_test), 5)),\n",
        "        'xgb': np.zeros((len(X_test), 5)),\n",
        "        'catboost': np.zeros((len(X_test), 5))\n",
        "    }\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"\\r  Fold {fold+1}/5 ÏòàÏ∏° Ï§ë...\", end='')\n",
        "        test_predictions['lgbm'][:, fold] = results['lgbm']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "        test_predictions['xgb'][:, fold] = results['xgb']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "        test_predictions['catboost'][:, fold] = results['catboost']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "    print(f\"\\r  ‚úì 5-Fold ÏòàÏ∏° ÏôÑÎ£å\")\n",
        "\n",
        "    # ÌèâÍ∑† ÌôïÎ•†\n",
        "    lgbm_proba_test = test_predictions['lgbm'].mean(axis=1)\n",
        "    xgb_proba_test = test_predictions['xgb'].mean(axis=1)\n",
        "    cat_proba_test = test_predictions['catboost'].mean(axis=1)\n",
        "\n",
        "    # Ensemble\n",
        "    ensemble_proba_test = 0.20 * lgbm_proba_test + 0.60 * xgb_proba_test + 0.20 * cat_proba_test\n",
        "\n",
        "    # Adaptive Threshold Ï†ÅÏö©\n",
        "    confidence_test = np.abs(ensemble_proba_test - 0.5)\n",
        "    adaptive_thresholds_test = np.array([get_adaptive_threshold(c) for c in confidence_test])\n",
        "    predictions_test = (ensemble_proba_test >= adaptive_thresholds_test).astype(int)\n",
        "\n",
        "    print(f\"\\n[Test ÏòàÏ∏° Í≤∞Í≥º]\")\n",
        "    print(f\"  ÏòàÏ∏° Class 0: {sum(predictions_test == 0)}Í∞ú\")\n",
        "    print(f\"  ÏòàÏ∏° Class 1: {sum(predictions_test == 1)}Í∞ú\")\n",
        "    print(f\"  ÌèâÍ∑† Confidence: {confidence_test.mean():.4f}\")\n",
        "    print(f\"  Low Confidence (<0.1): {sum(confidence_test < 0.1)}Í∞ú\")\n",
        "\n",
        "    # ============================================================\n",
        "    # 7. Submission ÌååÏùº ÏÉùÏÑ±\n",
        "    # ============================================================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Submission ÌååÏùº ÏÉùÏÑ±\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # SMILES Ïª¨Îüº ÌôïÏù∏\n",
        "    if 'SMILES' in df_test.columns:\n",
        "        smiles_col = 'SMILES'\n",
        "    elif 'smiles' in df_test.columns:\n",
        "        smiles_col = 'smiles'\n",
        "    else:\n",
        "        smiles_col = df_test.columns[0]\n",
        "\n",
        "    # Í∏∞Î≥∏ Ï†úÏ∂ú ÌååÏùº\n",
        "    submission = pd.DataFrame({\n",
        "        'SMILES': df_test[smiles_col],\n",
        "        'output': predictions_test\n",
        "    })\n",
        "    submission.to_csv('submission_top500.csv', index=False)\n",
        "    print(f\"\\n‚úì Í∏∞Î≥∏ Ï†úÏ∂ú ÌååÏùº: submission_top500.csv\")\n",
        "\n",
        "    # ÏÉÅÏÑ∏ Ï†úÏ∂ú ÌååÏùº\n",
        "    submission_detailed = pd.DataFrame({\n",
        "        'id': range(len(predictions_test)),\n",
        "        'label': predictions_test,\n",
        "        'probability': ensemble_proba_test,\n",
        "        'confidence': confidence_test,\n",
        "        'adaptive_threshold': adaptive_thresholds_test,\n",
        "        'lgbm_proba': lgbm_proba_test,\n",
        "        'xgb_proba': xgb_proba_test,\n",
        "        'catboost_proba': cat_proba_test\n",
        "    })\n",
        "    submission_detailed.to_csv('submission_detailed_top500.csv', index=False)\n",
        "    print(f\"‚úì ÏÉÅÏÑ∏ Ï†úÏ∂ú ÌååÏùº: submission_detailed_top500.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 8. ÏãúÍ∞ÅÌôî\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ÏãúÍ∞ÅÌôî ÏÉùÏÑ±\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Feature Count Comparison\n",
        "ax = axes[0, 0]\n",
        "feature_counts = [300, 500]\n",
        "f1_scores = [baseline_f1, oof_f1]\n",
        "colors = ['lightblue', 'darkgreen']\n",
        "\n",
        "bars = ax.bar([str(x) for x in feature_counts], f1_scores,\n",
        "              color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('F1 Score', fontsize=11)\n",
        "ax.set_title('F1 Score by Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.set_ylim([0.825, 0.835])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, f1_scores):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.4f}',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. FPR Comparison\n",
        "ax = axes[0, 1]\n",
        "fpr_values = [baseline_fpr * 100, oof_fpr * 100]\n",
        "bars = ax.bar([str(x) for x in feature_counts], fpr_values,\n",
        "              color=['lightcoral', 'lightgreen'], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.axhline(25, color='r', linestyle='--', alpha=0.5, label='Target: 25%')\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('FPR (%)', fontsize=11)\n",
        "ax.set_title('FPR by Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, fpr_values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.2f}%',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3. Confusion Matrix\n",
        "ax = axes[1, 0]\n",
        "sns.heatmap(oof_cm, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={'size': 14})\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix (Top 500)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 4. Performance Summary\n",
        "ax = axes[1, 1]\n",
        "ax.axis('off')\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        " Top 500 ÏÑ±Îä• ÏöîÏïΩ\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "F1 Score:  {oof_f1:.4f}\n",
        "AUC:       {oof_auc:.4f}\n",
        "Precision: {oof_precision:.4f}\n",
        "Recall:    {oof_recall:.4f}\n",
        "FPR:       {oof_fpr*100:.2f}%\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Top 300 ÎåÄÎπÑ\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "F1:   {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\n",
        "FPR:  {fpr_improvement*100:+.2f}%p\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "ÌîºÏ≤ò: 500Í∞ú (16.3%)\n",
        "ÏïïÏ∂ïÎ•†: 83.7%\n",
        "\"\"\"\n",
        "\n",
        "ax.text(0.5, 0.5, summary_text, ha='center', va='center',\n",
        "        fontsize=11, family='monospace',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('top500_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"\\n‚úì ÏãúÍ∞ÅÌôî Ï†ÄÏû•: top500_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[Î™®Îç∏ ÏÇ¨Ïñë]\")\n",
        "print(f\"  ÌîºÏ≤ò: Top 500Í∞ú (ÏïïÏ∂ïÎ•† 83.7%)\")\n",
        "print(f\"  Ensemble: LGBM(20%) + XGB(60%) + CAT(20%)\")\n",
        "print(f\"  Threshold: Adaptive (0.42/0.40/0.39)\")\n",
        "\n",
        "print(f\"\\n[ÏÑ±Îä•]\")\n",
        "print(f\"  F1 Score:  {oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {oof_auc:.4f}\")\n",
        "print(f\"  FPR:       {oof_fpr*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n[Top 300 ÎåÄÎπÑ]\")\n",
        "if f1_improvement > 0:\n",
        "    print(f\"  ‚úì‚úì F1 Ìñ•ÏÉÅ: {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\")\n",
        "else:\n",
        "    print(f\"  ‚ñ≥ F1 Î≥ÄÌôî: {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\")\n",
        "\n",
        "if fpr_improvement < 0:\n",
        "    print(f\"  ‚úì‚úì FPR Í∞úÏÑ†: {fpr_improvement*100:+.2f}%p\")\n",
        "else:\n",
        "    print(f\"  ‚ñ≥ FPR Î≥ÄÌôî: {fpr_improvement*100:+.2f}%p\")\n",
        "\n",
        "if test_available:\n",
        "    print(f\"\\n[Ï†úÏ∂ú ÌååÏùº]\")\n",
        "    print(f\"  Î©îÏù∏: submission_top500.csv\")\n",
        "    print(f\"  ÏÉÅÏÑ∏: submission_detailed_top500.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úì Top 500 Î™®Îç∏ ÏôÑÏÑ±!\")\n",
        "print(f\"{'='*70}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hYoPl3I6Vul",
        "outputId": "9610bf5c-cb25-4212-ea87-e2513a5fb534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Top 500 ÌîºÏ≤ò Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Top 500 ÌîºÏ≤ò ÏÑ†Ï†ï\n",
            "======================================================================\n",
            "\n",
            "[ÏÑ†Ï†ïÎêú ÌîºÏ≤ò]\n",
            "  Ï¥ù ÌîºÏ≤ò: 500Í∞ú\n",
            "  - Descriptor: 4Í∞ú\n",
            "  - Fingerprint: 496Í∞ú\n",
            "    ¬∑ ECFP: 158Í∞ú\n",
            "    ¬∑ FCFP: 110Í∞ú\n",
            "    ¬∑ PTFP: 228Í∞ú\n",
            "\n",
            "[ÎàÑÏ†Å Ï§ëÏöîÎèÑ]\n",
            "  Top 500 Ï§ëÏöîÎèÑ Ìï©: 28620.44\n",
            "  Ï†ÑÏ≤¥ Ï§ëÏöîÎèÑ Ìï©: 36487.29\n",
            "  ÎàÑÏ†Å ÎπÑÏú®: 78.44%\n",
            "\n",
            "‚úì ÌîºÏ≤ò Î¶¨Ïä§Ìä∏ Ï†ÄÏû•: selected_features_top500.csv\n",
            "\n",
            "======================================================================\n",
            "Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
            "======================================================================\n",
            "\n",
            "[Train Îç∞Ïù¥ÌÑ∞]\n",
            "  Shape: (8349, 500)\n",
            "  Label Î∂ÑÌè¨: Class 0 = 3807, Class 1 = 4542\n",
            "\n",
            "[Test Îç∞Ïù¥ÌÑ∞]\n",
            "  Shape: (927, 500)\n",
            "\n",
            "======================================================================\n",
            "5-Fold Cross-Validation ÌïôÏäµ\n",
            "======================================================================\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 1/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 500), Í≤ÄÏ¶ù: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8387, Iter: 827\n",
            "  [2/3] XGBoost... F1: 0.8470, Iter: 997\n",
            "  [3/3] CatBoost... F1: 0.8376, Iter: 996\n",
            "\n",
            "  [Ensemble] F1: 0.8429, AUC: 0.9072, FPR: 0.2378\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 2/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 500), Í≤ÄÏ¶ù: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8225, Iter: 637\n",
            "  [2/3] XGBoost... F1: 0.8191, Iter: 698\n",
            "  [3/3] CatBoost... F1: 0.8209, Iter: 990\n",
            "\n",
            "  [Ensemble] F1: 0.8203, AUC: 0.8868, FPR: 0.2681\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 3/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 500), Í≤ÄÏ¶ù: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8105, Iter: 626\n",
            "  [2/3] XGBoost... F1: 0.8096, Iter: 661\n",
            "  [3/3] CatBoost... F1: 0.8057, Iter: 999\n",
            "\n",
            "  [Ensemble] F1: 0.8098, AUC: 0.8772, FPR: 0.2454\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 4/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 500), Í≤ÄÏ¶ù: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8269, Iter: 732\n",
            "  [2/3] XGBoost... F1: 0.8314, Iter: 721\n",
            "  [3/3] CatBoost... F1: 0.8305, Iter: 988\n",
            "\n",
            "  [Ensemble] F1: 0.8350, AUC: 0.8944, FPR: 0.2677\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 5/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6680, 500), Í≤ÄÏ¶ù: (1669, 500)\n",
            "  [1/3] LightGBM... F1: 0.8474, Iter: 838\n",
            "  [2/3] XGBoost... F1: 0.8461, Iter: 986\n",
            "  [3/3] CatBoost... F1: 0.8381, Iter: 997\n",
            "\n",
            "  [Ensemble] F1: 0.8448, AUC: 0.9040, FPR: 0.2247\n",
            "\n",
            "======================================================================\n",
            "OOF ÏÑ±Îä• ÌèâÍ∞Ä\n",
            "======================================================================\n",
            "\n",
            "[Top 500 OOF ÏÑ±Îä•]\n",
            "  F1 Score:  0.8306\n",
            "  AUC Score: 0.8940\n",
            "  Precision: 0.8046\n",
            "  Recall:    0.8584\n",
            "  FPR:       0.2488 (24.88%)\n",
            "\n",
            "[OOF ÌòºÎèô ÌñâÎ†¨]\n",
            "  TN: 2860, FP: 947, FN: 643, TP: 3899\n",
            "\n",
            "[Low Confidence]\n",
            "  Í∞úÏàò: 970Í∞ú (11.62%)\n",
            "  Ï†ïÌôïÎèÑ: 0.5412\n",
            "\n",
            "======================================================================\n",
            "Top 300 vs Top 500 ÎπÑÍµê\n",
            "======================================================================\n",
            "\n",
            "[ÏÑ±Îä• ÎπÑÍµê]\n",
            "ÏßÄÌëú              Top 300      Top 500      Î≥ÄÌôî             \n",
            "-------------------------------------------------------\n",
            "F1 Score        0.8300       0.8306       +0.0006 (+0.08%)\n",
            "FPR             24.67       % 24.88       % +0.21%p\n",
            "ÌîºÏ≤ò Ïàò            300          500          +200\n",
            "\n",
            "======================================================================\n",
            "Test Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\n",
            "======================================================================\n",
            "\n",
            "  Test shape: (927, 500)\n",
            "  ‚úì 5-Fold ÏòàÏ∏° ÏôÑÎ£å\n",
            "\n",
            "[Test ÏòàÏ∏° Í≤∞Í≥º]\n",
            "  ÏòàÏ∏° Class 0: 382Í∞ú\n",
            "  ÏòàÏ∏° Class 1: 545Í∞ú\n",
            "  ÌèâÍ∑† Confidence: 0.3115\n",
            "  Low Confidence (<0.1): 122Í∞ú\n",
            "\n",
            "======================================================================\n",
            "Submission ÌååÏùº ÏÉùÏÑ±\n",
            "======================================================================\n",
            "\n",
            "‚úì Í∏∞Î≥∏ Ï†úÏ∂ú ÌååÏùº: submission_top500.csv\n",
            "‚úì ÏÉÅÏÑ∏ Ï†úÏ∂ú ÌååÏùº: submission_detailed_top500.csv\n",
            "\n",
            "======================================================================\n",
            "ÏãúÍ∞ÅÌôî ÏÉùÏÑ±\n",
            "======================================================================\n",
            "\n",
            "‚úì ÏãúÍ∞ÅÌôî Ï†ÄÏû•: top500_analysis.png\n",
            "\n",
            "======================================================================\n",
            "ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏\n",
            "======================================================================\n",
            "\n",
            "[Î™®Îç∏ ÏÇ¨Ïñë]\n",
            "  ÌîºÏ≤ò: Top 500Í∞ú (ÏïïÏ∂ïÎ•† 83.7%)\n",
            "  Ensemble: LGBM(20%) + XGB(60%) + CAT(20%)\n",
            "  Threshold: Adaptive (0.42/0.40/0.39)\n",
            "\n",
            "[ÏÑ±Îä•]\n",
            "  F1 Score:  0.8306\n",
            "  AUC Score: 0.8940\n",
            "  FPR:       24.88%\n",
            "\n",
            "[Top 300 ÎåÄÎπÑ]\n",
            "  ‚úì‚úì F1 Ìñ•ÏÉÅ: +0.0006 (+0.08%)\n",
            "  ‚ñ≥ FPR Î≥ÄÌôî: +0.21%p\n",
            "\n",
            "[Ï†úÏ∂ú ÌååÏùº]\n",
            "  Î©îÏù∏: submission_top500.csv\n",
            "  ÏÉÅÏÑ∏: submission_detailed_top500.csv\n",
            "\n",
            "======================================================================\n",
            "‚úì Top 500 Î™®Îç∏ ÏôÑÏÑ±!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏: Ï†ÑÏ≤¥ ÌîºÏ≤ò + 2-Layer Stacking\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏: Ï†ÑÏ≤¥ ÌîºÏ≤ò + 2-Layer Stacking\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[Ï†ÑÎûµ]\")\n",
        "print(\"  Layer 1: LGBM, XGB, CAT (Base Models)\")\n",
        "print(\"  Layer 2: Logistic Regression (Meta-Learner)\")\n",
        "print(\"  ÌîºÏ≤ò: Ï†ÑÏ≤¥ 3076Í∞ú (ÏïïÏ∂ï ÏóÜÏùå)\")\n",
        "print(\"  Î™©Ìëú: F1 0.835+\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (Ï†ÑÏ≤¥ ÌîºÏ≤ò)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Îç∞Ïù¥ÌÑ∞ Î°úÎìú (Ï†ÑÏ≤¥ ÌîºÏ≤ò)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Train Îç∞Ïù¥ÌÑ∞\n",
        "df_train = pd.read_csv('train.csv')\n",
        "\n",
        "# Ï†ÑÏ≤¥ ÌîºÏ≤ò ÏÇ¨Ïö© (label Ï†úÏô∏)\n",
        "feature_columns = [col for col in df_train.columns if col != 'label']\n",
        "X_train = df_train[feature_columns]\n",
        "y_train = df_train['label'].astype(int)\n",
        "\n",
        "print(f\"\\n[Train Îç∞Ïù¥ÌÑ∞]\")\n",
        "print(f\"  Shape: {X_train.shape}\")\n",
        "print(f\"  ÌîºÏ≤ò: {len(feature_columns)}Í∞ú (Ï†ÑÏ≤¥)\")\n",
        "print(f\"  Label Î∂ÑÌè¨: Class 0 = {sum(y_train==0)}, Class 1 = {sum(y_train==1)}\")\n",
        "\n",
        "# Test Îç∞Ïù¥ÌÑ∞\n",
        "try:\n",
        "    df_test = pd.read_csv('predict_input.csv')\n",
        "    X_test = df_test[feature_columns]\n",
        "    print(f\"\\n[Test Îç∞Ïù¥ÌÑ∞]\")\n",
        "    print(f\"  Shape: {X_test.shape}\")\n",
        "    test_available = True\n",
        "except:\n",
        "    print(f\"\\n‚ö†Ô∏è  Test Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå\")\n",
        "    test_available = False\n",
        "\n",
        "# ÌîºÏ≤ò ÌÉÄÏûÖ Î∂ÑÎ•ò\n",
        "fp_cols = [col for col in feature_columns if col.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "\n",
        "print(f\"\\n[ÌîºÏ≤ò Íµ¨ÏÑ±]\")\n",
        "print(f\"  Fingerprint: {len(fp_cols)}Í∞ú\")\n",
        "print(f\"  Descriptor: {len(desc_cols)}Í∞ú\")\n",
        "\n",
        "# Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Layer 1: Base Models (5-Fold OOF)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Layer 1: Base Models ÌïôÏäµ (5-Fold CV)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# OOF ÌôïÎ•† Ï†ÄÏû•\n",
        "oof_probabilities = {\n",
        "    'lgbm': np.zeros(len(X_train)),\n",
        "    'xgb': np.zeros(len(X_train)),\n",
        "    'catboost': np.zeros(len(X_train))\n",
        "}\n",
        "\n",
        "# Test ÏòàÏ∏° Ï†ÄÏû• (Í∞Å Fold)\n",
        "if test_available:\n",
        "    test_predictions = {\n",
        "        'lgbm': np.zeros((len(X_test), 5)),\n",
        "        'xgb': np.zeros((len(X_test), 5)),\n",
        "        'catboost': np.zeros((len(X_test), 5))\n",
        "    }\n",
        "\n",
        "# Î™®Îç∏ Ï†ÄÏû•\n",
        "models = {\n",
        "    'lgbm': [],\n",
        "    'xgb': [],\n",
        "    'catboost': []\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"üìä Fold {fold}/5\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
        "    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
        "\n",
        "    # Ï†ÑÏ≤òÎ¶¨\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    print(f\"  ÌïôÏäµ: {Xt_tr.shape}, Í≤ÄÏ¶ù: {Xt_va.shape}\")\n",
        "\n",
        "    # ========================================\n",
        "    # LightGBM\n",
        "    # ========================================\n",
        "    print(f\"  [1/3] LightGBM...\", end=' ')\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=8,\n",
        "        num_leaves=63,\n",
        "        min_child_samples=30,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    )\n",
        "\n",
        "    lgbm_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "\n",
        "    lgbm_proba_va = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "    oof_probabilities['lgbm'][va_idx] = lgbm_proba_va\n",
        "    models['lgbm'].append((lgbm_model, preprocessor))\n",
        "\n",
        "    print(f\"ÏôÑÎ£å (Iter: {lgbm_model.best_iteration_})\")\n",
        "\n",
        "    # Test ÏòàÏ∏°\n",
        "    if test_available:\n",
        "        Xt_test = preprocessor.transform(X_test)\n",
        "        test_predictions['lgbm'][:, fold-1] = lgbm_model.predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "    # ========================================\n",
        "    # XGBoost\n",
        "    # ========================================\n",
        "    print(f\"  [2/3] XGBoost...\", end=' ')\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=7,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        gamma=0.1,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        early_stopping_rounds=100,\n",
        "        eval_metric='logloss',\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    xgb_proba_va = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "    oof_probabilities['xgb'][va_idx] = xgb_proba_va\n",
        "    models['xgb'].append((xgb_model, preprocessor))\n",
        "\n",
        "    print(f\"ÏôÑÎ£å (Iter: {xgb_model.best_iteration})\")\n",
        "\n",
        "    # Test ÏòàÏ∏°\n",
        "    if test_available:\n",
        "        test_predictions['xgb'][:, fold-1] = xgb_model.predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "    # ========================================\n",
        "    # CatBoost\n",
        "    # ========================================\n",
        "    print(f\"  [3/3] CatBoost...\", end=' ')\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.03,\n",
        "        depth=7,\n",
        "        l2_leaf_reg=3,\n",
        "        class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE,\n",
        "        verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "\n",
        "    cat_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=(Xt_va, y_va),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    cat_proba_va = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "    oof_probabilities['catboost'][va_idx] = cat_proba_va\n",
        "    models['catboost'].append((cat_model, preprocessor))\n",
        "\n",
        "    print(f\"ÏôÑÎ£å (Iter: {cat_model.best_iteration_})\")\n",
        "\n",
        "    # Test ÏòàÏ∏°\n",
        "    if test_available:\n",
        "        test_predictions['catboost'][:, fold-1] = cat_model.predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "print(f\"\\n‚úì Layer 1 ÏôÑÎ£å: 3Í∞ú Base Models √ó 5 Folds = 15Í∞ú Î™®Îç∏ ÌïôÏäµ\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Layer 1 Base Performance\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Layer 1 Base Models ÏÑ±Îä• (Simple Ensemble)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Simple weighted ensemble (Í∏∞Ï§ÄÏÑ†)\n",
        "simple_ensemble_proba = (\n",
        "    0.20 * oof_probabilities['lgbm'] +\n",
        "    0.60 * oof_probabilities['xgb'] +\n",
        "    0.20 * oof_probabilities['catboost']\n",
        ")\n",
        "\n",
        "# Adaptive Threshold\n",
        "def get_adaptive_threshold(confidence):\n",
        "    if confidence < 0.05:\n",
        "        return 0.42\n",
        "    elif confidence < 0.10:\n",
        "        return 0.40\n",
        "    else:\n",
        "        return 0.39\n",
        "\n",
        "confidence_simple = np.abs(simple_ensemble_proba - 0.5)\n",
        "adaptive_thresholds_simple = np.array([get_adaptive_threshold(c) for c in confidence_simple])\n",
        "predictions_simple = (simple_ensemble_proba >= adaptive_thresholds_simple).astype(int)\n",
        "\n",
        "# ÏÑ±Îä•\n",
        "simple_f1 = f1_score(y_train, predictions_simple)\n",
        "simple_auc = roc_auc_score(y_train, simple_ensemble_proba)\n",
        "simple_cm = confusion_matrix(y_train, predictions_simple)\n",
        "tn, fp, fn, tp = simple_cm.ravel()\n",
        "simple_fpr = fp / (fp + tn)\n",
        "\n",
        "print(f\"\\n[Simple Ensemble (Weighted Average)]\")\n",
        "print(f\"  F1 Score:  {simple_f1:.4f}\")\n",
        "print(f\"  AUC Score: {simple_auc:.4f}\")\n",
        "print(f\"  FPR:       {simple_fpr:.4f} ({simple_fpr*100:.2f}%)\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. Layer 2: Stacking (Meta-Learner)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Layer 2: Stacking Meta-Learner ÌïôÏäµ\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Meta-features (Layer 1Ïùò OOF ÌôïÎ•†)\n",
        "meta_features_train = np.column_stack([\n",
        "    oof_probabilities['lgbm'],\n",
        "    oof_probabilities['xgb'],\n",
        "    oof_probabilities['catboost']\n",
        "])\n",
        "\n",
        "print(f\"\\n[Meta-features]\")\n",
        "print(f\"  Shape: {meta_features_train.shape}\")\n",
        "print(f\"  Feature 1: LGBM ÌôïÎ•†\")\n",
        "print(f\"  Feature 2: XGBoost ÌôïÎ•†\")\n",
        "print(f\"  Feature 3: CatBoost ÌôïÎ•†\")\n",
        "\n",
        "# Meta-Learner ÌïôÏäµ (Logistic Regression)\n",
        "print(f\"\\n[Meta-Learner ÌïôÏäµ]\")\n",
        "meta_model = LogisticRegression(\n",
        "    C=0.1,\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "meta_model.fit(meta_features_train, y_train)\n",
        "\n",
        "print(f\"‚úì Meta-Learner ÌïôÏäµ ÏôÑÎ£å\")\n",
        "print(f\"\\n[Meta-Learner Í∞ÄÏ§ëÏπò]\")\n",
        "print(f\"  LGBM:    {meta_model.coef_[0][0]:.4f}\")\n",
        "print(f\"  XGBoost: {meta_model.coef_[0][1]:.4f}\")\n",
        "print(f\"  CatBoost: {meta_model.coef_[0][2]:.4f}\")\n",
        "print(f\"  Intercept: {meta_model.intercept_[0]:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. Stacking Performance (OOF)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Stacking ÏÑ±Îä• ÌèâÍ∞Ä (OOF)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Stacking ÌôïÎ•†\n",
        "stacking_proba = meta_model.predict_proba(meta_features_train)[:, 1]\n",
        "\n",
        "# Adaptive Threshold Ï†ÅÏö©\n",
        "confidence_stacking = np.abs(stacking_proba - 0.5)\n",
        "adaptive_thresholds_stacking = np.array([get_adaptive_threshold(c) for c in confidence_stacking])\n",
        "predictions_stacking = (stacking_proba >= adaptive_thresholds_stacking).astype(int)\n",
        "\n",
        "# ÏÑ±Îä•\n",
        "stacking_f1 = f1_score(y_train, predictions_stacking)\n",
        "stacking_auc = roc_auc_score(y_train, stacking_proba)\n",
        "stacking_cm = confusion_matrix(y_train, predictions_stacking)\n",
        "tn_s, fp_s, fn_s, tp_s = stacking_cm.ravel()\n",
        "stacking_fpr = fp_s / (fp_s + tn_s)\n",
        "stacking_precision = precision_score(y_train, predictions_stacking)\n",
        "stacking_recall = recall_score(y_train, predictions_stacking)\n",
        "\n",
        "print(f\"\\n[Stacking OOF ÏÑ±Îä•]\")\n",
        "print(f\"  F1 Score:  {stacking_f1:.4f}\")\n",
        "print(f\"  AUC Score: {stacking_auc:.4f}\")\n",
        "print(f\"  Precision: {stacking_precision:.4f}\")\n",
        "print(f\"  Recall:    {stacking_recall:.4f}\")\n",
        "print(f\"  FPR:       {stacking_fpr:.4f} ({stacking_fpr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[OOF ÌòºÎèô ÌñâÎ†¨]\")\n",
        "print(f\"  TN: {tn_s}, FP: {fp_s}, FN: {fn_s}, TP: {tp_s}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. ÏÑ±Îä• ÎπÑÍµê (Simple vs Stacking)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Simple Ensemble vs Stacking ÎπÑÍµê\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "f1_improvement = stacking_f1 - simple_f1\n",
        "auc_improvement = stacking_auc - simple_auc\n",
        "fpr_improvement = stacking_fpr - simple_fpr\n",
        "\n",
        "print(f\"\\n[ÏÑ±Îä• Î≥ÄÌôî]\")\n",
        "print(f\"{'ÏßÄÌëú':<15} {'Simple':<12} {'Stacking':<12} {'Î≥ÄÌôî':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'F1 Score':<15} {simple_f1:<12.4f} {stacking_f1:<12.4f} {f1_improvement:+.4f} ({f1_improvement/simple_f1*100:+.2f}%)\")\n",
        "print(f\"{'AUC':<15} {simple_auc:<12.4f} {stacking_auc:<12.4f} {auc_improvement:+.4f}\")\n",
        "print(f\"{'FPR':<15} {simple_fpr*100:<12.2f}% {stacking_fpr*100:<12.2f}% {fpr_improvement*100:+.2f}%p\")\n",
        "\n",
        "# Top 300 ÎåÄÎπÑ\n",
        "baseline_f1 = 0.8300\n",
        "baseline_fpr = 0.2467\n",
        "\n",
        "print(f\"\\n[Top 300 ÎåÄÎπÑ]\")\n",
        "print(f\"  F1:  {baseline_f1:.4f} ‚Üí {stacking_f1:.4f} ({(stacking_f1-baseline_f1)*100:+.2f}%p)\")\n",
        "print(f\"  FPR: {baseline_fpr*100:.2f}% ‚Üí {stacking_fpr*100:.2f}% ({(stacking_fpr-baseline_fpr)*100:+.2f}%p)\")\n",
        "\n",
        "# ============================================================\n",
        "# 7. Test ÏòàÏ∏° (Stacking)\n",
        "# ============================================================\n",
        "if test_available:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Test Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏° (Stacking)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Test Meta-features (Í∞Å Fold ÌèâÍ∑†)\n",
        "    meta_features_test = np.column_stack([\n",
        "        test_predictions['lgbm'].mean(axis=1),\n",
        "        test_predictions['xgb'].mean(axis=1),\n",
        "        test_predictions['catboost'].mean(axis=1)\n",
        "    ])\n",
        "\n",
        "    print(f\"\\n  Test Meta-features shape: {meta_features_test.shape}\")\n",
        "\n",
        "    # Stacking ÏòàÏ∏°\n",
        "    stacking_proba_test = meta_model.predict_proba(meta_features_test)[:, 1]\n",
        "\n",
        "    # Adaptive Threshold Ï†ÅÏö©\n",
        "    confidence_test = np.abs(stacking_proba_test - 0.5)\n",
        "    adaptive_thresholds_test = np.array([get_adaptive_threshold(c) for c in confidence_test])\n",
        "    predictions_test = (stacking_proba_test >= adaptive_thresholds_test).astype(int)\n",
        "\n",
        "    print(f\"\\n[Test ÏòàÏ∏° Í≤∞Í≥º]\")\n",
        "    print(f\"  ÏòàÏ∏° Class 0: {sum(predictions_test == 0)}Í∞ú ({sum(predictions_test == 0)/len(predictions_test)*100:.2f}%)\")\n",
        "    print(f\"  ÏòàÏ∏° Class 1: {sum(predictions_test == 1)}Í∞ú ({sum(predictions_test == 1)/len(predictions_test)*100:.2f}%)\")\n",
        "    print(f\"  ÌèâÍ∑† Confidence: {confidence_test.mean():.4f}\")\n",
        "    print(f\"  Low Confidence (<0.1): {sum(confidence_test < 0.1)}Í∞ú\")\n",
        "\n",
        "    # ============================================================\n",
        "    # 8. Submission ÌååÏùº ÏÉùÏÑ±\n",
        "    # ============================================================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Submission ÌååÏùº ÏÉùÏÑ±\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # SMILES Ïª¨Îüº ÌôïÏù∏\n",
        "    if 'SMILES' in df_test.columns:\n",
        "        smiles_col = 'SMILES'\n",
        "    elif 'smiles' in df_test.columns:\n",
        "        smiles_col = 'smiles'\n",
        "    else:\n",
        "        smiles_col = df_test.columns[0]\n",
        "\n",
        "    # Í∏∞Î≥∏ Ï†úÏ∂ú ÌååÏùº\n",
        "    submission = pd.DataFrame({\n",
        "        'SMILES': df_test[smiles_col],\n",
        "        'output': predictions_test\n",
        "    })\n",
        "    submission.to_csv('submission_stacking_final.csv', index=False)\n",
        "    print(f\"\\n‚úì Í∏∞Î≥∏ Ï†úÏ∂ú ÌååÏùº: submission_stacking_final.csv\")\n",
        "\n",
        "    # ÏÉÅÏÑ∏ Ï†úÏ∂ú ÌååÏùº\n",
        "    submission_detailed = pd.DataFrame({\n",
        "        'id': range(len(predictions_test)),\n",
        "        'label': predictions_test,\n",
        "        'probability': stacking_proba_test,\n",
        "        'confidence': confidence_test,\n",
        "        'adaptive_threshold': adaptive_thresholds_test,\n",
        "        'lgbm_proba': test_predictions['lgbm'].mean(axis=1),\n",
        "        'xgb_proba': test_predictions['xgb'].mean(axis=1),\n",
        "        'catboost_proba': test_predictions['catboost'].mean(axis=1)\n",
        "    })\n",
        "    submission_detailed.to_csv('submission_detailed_stacking_final.csv', index=False)\n",
        "    print(f\"‚úì ÏÉÅÏÑ∏ Ï†úÏ∂ú ÌååÏùº: submission_detailed_stacking_final.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. Î™®Îç∏ Ï†ÄÏû•\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Î™®Îç∏ Ï†ÄÏû•\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "final_model_package = {\n",
        "    'layer1_models': models,\n",
        "    'meta_model': meta_model,\n",
        "    'feature_columns': feature_columns,\n",
        "    'oof_f1': stacking_f1,\n",
        "    'oof_auc': stacking_auc,\n",
        "    'oof_fpr': stacking_fpr\n",
        "}\n",
        "\n",
        "with open('final_stacking_model.pkl', 'wb') as f:\n",
        "    pickle.dump(final_model_package, f)\n",
        "\n",
        "print(f\"\\n‚úì Î™®Îç∏ Ï†ÄÏû•: final_stacking_model.pkl\")\n",
        "print(f\"  - Layer 1: 15Í∞ú Base Models\")\n",
        "print(f\"  - Layer 2: Meta-Learner\")\n",
        "print(f\"  - Ï†ÑÏ≤¥ ÌîºÏ≤ò Î¶¨Ïä§Ìä∏\")\n",
        "\n",
        "# ============================================================\n",
        "# 10. ÏãúÍ∞ÅÌôî\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ÏãúÍ∞ÅÌôî ÏÉùÏÑ±\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# 1. F1 Score Comparison\n",
        "ax = axes[0, 0]\n",
        "models_compare = ['Top 300\\n(Adaptive)', 'Simple\\nEnsemble\\n(3076Í∞ú)', 'Stacking\\n(3076Í∞ú)']\n",
        "f1_scores = [baseline_f1, simple_f1, stacking_f1]\n",
        "colors = ['lightblue', 'orange', 'darkgreen']\n",
        "\n",
        "bars = ax.bar(models_compare, f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.set_ylabel('F1 Score', fontsize=11)\n",
        "ax.set_title('F1 Score Comparison', fontsize=12, fontweight='bold')\n",
        "ax.set_ylim([0.825, 0.840])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, f1_scores):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.4f}',\n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 2. FPR Comparison\n",
        "ax = axes[0, 1]\n",
        "fpr_values = [baseline_fpr * 100, simple_fpr * 100, stacking_fpr * 100]\n",
        "bars = ax.bar(models_compare, fpr_values, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.axhline(25, color='r', linestyle='--', alpha=0.5, label='Target: 25%')\n",
        "ax.set_ylabel('FPR (%)', fontsize=11)\n",
        "ax.set_title('FPR Comparison', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, fpr_values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.2f}%',\n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 3. Confusion Matrix (Stacking)\n",
        "ax = axes[0, 2]\n",
        "sns.heatmap(stacking_cm, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={'size': 14})\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix (Stacking)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 4. Meta-Learner Weights\n",
        "ax = axes[1, 0]\n",
        "model_names = ['LGBM', 'XGBoost', 'CatBoost']\n",
        "weights = meta_model.coef_[0]\n",
        "colors_weights = ['steelblue', 'orange', 'green']\n",
        "\n",
        "bars = ax.bar(model_names, weights, color=colors_weights, alpha=0.7, edgecolor='black')\n",
        "ax.set_ylabel('Weight', fontsize=11)\n",
        "ax.set_title('Meta-Learner Weights', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, weight in zip(bars, weights):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{weight:.3f}',\n",
        "            ha='center', va='bottom' if height > 0 else 'top', fontweight='bold')\n",
        "\n",
        "# 5. Performance Improvement\n",
        "ax = axes[1, 1]\n",
        "metrics = ['F1\\nScore', 'AUC', 'Precision', 'Recall']\n",
        "simple_vals = [simple_f1, simple_auc,\n",
        "               precision_score(y_train, predictions_simple),\n",
        "               recall_score(y_train, predictions_simple)]\n",
        "stacking_vals = [stacking_f1, stacking_auc, stacking_precision, stacking_recall]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, simple_vals, width, label='Simple', alpha=0.8)\n",
        "ax.bar(x + width/2, stacking_vals, width, label='Stacking', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('All Metrics Comparison', fontsize=12, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 6. Summary\n",
        "ax = axes[1, 2]\n",
        "ax.axis('off')\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "  Stacking ÏµúÏ¢Ö ÏÑ±Îä•\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "F1 Score:  {stacking_f1:.4f}\n",
        "AUC:       {stacking_auc:.4f}\n",
        "Precision: {stacking_precision:.4f}\n",
        "Recall:    {stacking_recall:.4f}\n",
        "FPR:       {stacking_fpr*100:.2f}%\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Top 300 ÎåÄÎπÑ\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "F1:   {(stacking_f1-baseline_f1)*100:+.2f}%p\n",
        "FPR:  {(stacking_fpr-baseline_fpr)*100:+.2f}%p\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Simple ÎåÄÎπÑ\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "F1:   {f1_improvement:+.4f}\n",
        "FPR:  {fpr_improvement*100:+.2f}%p\n",
        "\n",
        "ÌîºÏ≤ò: 3076Í∞ú (Ï†ÑÏ≤¥)\n",
        "Layer 1: 15Í∞ú Î™®Îç∏\n",
        "Layer 2: Meta-Learner\n",
        "\"\"\"\n",
        "\n",
        "ax.text(0.5, 0.5, summary_text, ha='center', va='center',\n",
        "        fontsize=10, family='monospace',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('stacking_final_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"\\n‚úì ÏãúÍ∞ÅÌôî Ï†ÄÏû•: stacking_final_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 11. ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ÏµúÏ¢Ö ÏÑ±Îä• Î¶¨Ìè¨Ìä∏\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[Î™®Îç∏ ÏÇ¨Ïñë]\")\n",
        "print(f\"  Architecture: 2-Layer Stacking\")\n",
        "print(f\"    - Layer 1: LGBM + XGB + CAT\")\n",
        "print(f\"    - Layer 2: Logistic Regression\")\n",
        "print(f\"  ÌîºÏ≤ò: Ï†ÑÏ≤¥ 3076Í∞ú\")\n",
        "print(f\"  Threshold: Adaptive (0.42/0.40/0.39)\")\n",
        "\n",
        "print(f\"\\n[ÏµúÏ¢Ö ÏÑ±Îä•]\")\n",
        "print(f\"  F1 Score:  {stacking_f1:.4f}\")\n",
        "print(f\"  AUC Score: {stacking_auc:.4f}\")\n",
        "print(f\"  Precision: {stacking_precision:.4f}\")\n",
        "print(f\"  Recall:    {stacking_recall:.4f}\")\n",
        "print(f\"  FPR:       {stacking_fpr*100:.2f}%\")\n",
        "\n",
        "if stacking_f1 > baseline_f1:\n",
        "    print(f\"\\n‚úì‚úì‚úì ÏÑ±Í≥µ! Top 300 ÎåÄÎπÑ F1 {(stacking_f1-baseline_f1)*100:+.2f}%p Ìñ•ÏÉÅ\")\n",
        "else:\n",
        "    print(f\"\\n‚ñ≥ Top 300Í≥º Ïú†ÏÇ¨Ìïú ÏÑ±Îä•\")\n",
        "\n",
        "if test_available:\n",
        "    print(f\"\\n[Ï†úÏ∂ú ÌååÏùº]\")\n",
        "    print(f\"  Î©îÏù∏: submission_stacking_final.csv\")\n",
        "    print(f\"  ÏÉÅÏÑ∏: submission_detailed_stacking_final.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úì Stacking Î™®Îç∏ ÏôÑÏÑ±!\")\n",
        "print(f\"{'='*70}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UY5KF0z-bMA",
        "outputId": "7b76136f-b3ae-43c2-c160-c469d98f6d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏: Ï†ÑÏ≤¥ ÌîºÏ≤ò + 2-Layer Stacking\n",
            "======================================================================\n",
            "\n",
            "[Ï†ÑÎûµ]\n",
            "  Layer 1: LGBM, XGB, CAT (Base Models)\n",
            "  Layer 2: Logistic Regression (Meta-Learner)\n",
            "  ÌîºÏ≤ò: Ï†ÑÏ≤¥ 3076Í∞ú (ÏïïÏ∂ï ÏóÜÏùå)\n",
            "  Î™©Ìëú: F1 0.835+\n",
            "\n",
            "======================================================================\n",
            "Îç∞Ïù¥ÌÑ∞ Î°úÎìú (Ï†ÑÏ≤¥ ÌîºÏ≤ò)\n",
            "======================================================================\n",
            "\n",
            "[Train Îç∞Ïù¥ÌÑ∞]\n",
            "  Shape: (8349, 3077)\n",
            "  ÌîºÏ≤ò: 3077Í∞ú (Ï†ÑÏ≤¥)\n",
            "  Label Î∂ÑÌè¨: Class 0 = 3807, Class 1 = 4542\n",
            "\n",
            "[Test Îç∞Ïù¥ÌÑ∞]\n",
            "  Shape: (927, 3077)\n",
            "\n",
            "[ÌîºÏ≤ò Íµ¨ÏÑ±]\n",
            "  Fingerprint: 3072Í∞ú\n",
            "  Descriptor: 4Í∞ú\n",
            "\n",
            "======================================================================\n",
            "Layer 1: Base Models ÌïôÏäµ (5-Fold CV)\n",
            "======================================================================\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 1/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 3076), Í≤ÄÏ¶ù: (1670, 3076)\n",
            "  [1/3] LightGBM... ÏôÑÎ£å (Iter: 874)\n",
            "  [2/3] XGBoost... ÏôÑÎ£å (Iter: 993)\n",
            "  [3/3] CatBoost... ÏôÑÎ£å (Iter: 999)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 2/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 3076), Í≤ÄÏ¶ù: (1670, 3076)\n",
            "  [1/3] LightGBM... ÏôÑÎ£å (Iter: 760)\n",
            "  [2/3] XGBoost... ÏôÑÎ£å (Iter: 731)\n",
            "  [3/3] CatBoost... ÏôÑÎ£å (Iter: 956)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 3/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 3076), Í≤ÄÏ¶ù: (1670, 3076)\n",
            "  [1/3] LightGBM... ÏôÑÎ£å (Iter: 762)\n",
            "  [2/3] XGBoost... ÏôÑÎ£å (Iter: 774)\n",
            "  [3/3] CatBoost... ÏôÑÎ£å (Iter: 987)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 4/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6679, 3076), Í≤ÄÏ¶ù: (1670, 3076)\n",
            "  [1/3] LightGBM... ÏôÑÎ£å (Iter: 845)\n",
            "  [2/3] XGBoost... ÏôÑÎ£å (Iter: 904)\n",
            "  [3/3] CatBoost... ÏôÑÎ£å (Iter: 999)\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìä Fold 5/5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ÌïôÏäµ: (6680, 3076), Í≤ÄÏ¶ù: (1669, 3076)\n",
            "  [1/3] LightGBM... ÏôÑÎ£å (Iter: 996)\n",
            "  [2/3] XGBoost... ÏôÑÎ£å (Iter: 993)\n",
            "  [3/3] CatBoost... ÏôÑÎ£å (Iter: 997)\n",
            "\n",
            "‚úì Layer 1 ÏôÑÎ£å: 3Í∞ú Base Models √ó 5 Folds = 15Í∞ú Î™®Îç∏ ÌïôÏäµ\n",
            "\n",
            "======================================================================\n",
            "Layer 1 Base Models ÏÑ±Îä• (Simple Ensemble)\n",
            "======================================================================\n",
            "\n",
            "[Simple Ensemble (Weighted Average)]\n",
            "  F1 Score:  0.8225\n",
            "  AUC Score: 0.8862\n",
            "  FPR:       0.2590 (25.90%)\n",
            "\n",
            "======================================================================\n",
            "Layer 2: Stacking Meta-Learner ÌïôÏäµ\n",
            "======================================================================\n",
            "\n",
            "[Meta-features]\n",
            "  Shape: (8349, 3)\n",
            "  Feature 1: LGBM ÌôïÎ•†\n",
            "  Feature 2: XGBoost ÌôïÎ•†\n",
            "  Feature 3: CatBoost ÌôïÎ•†\n",
            "\n",
            "[Meta-Learner ÌïôÏäµ]\n",
            "‚úì Meta-Learner ÌïôÏäµ ÏôÑÎ£å\n",
            "\n",
            "[Meta-Learner Í∞ÄÏ§ëÏπò]\n",
            "  LGBM:    1.6204\n",
            "  XGBoost: 1.9808\n",
            "  CatBoost: 1.7538\n",
            "  Intercept: -2.6061\n",
            "\n",
            "======================================================================\n",
            "Stacking ÏÑ±Îä• ÌèâÍ∞Ä (OOF)\n",
            "======================================================================\n",
            "\n",
            "[Stacking OOF ÏÑ±Îä•]\n",
            "  F1 Score:  0.8239\n",
            "  AUC Score: 0.8860\n",
            "  Precision: 0.8031\n",
            "  Recall:    0.8459\n",
            "  FPR:       0.2474 (24.74%)\n",
            "\n",
            "[OOF ÌòºÎèô ÌñâÎ†¨]\n",
            "  TN: 2865, FP: 942, FN: 700, TP: 3842\n",
            "\n",
            "======================================================================\n",
            "Simple Ensemble vs Stacking ÎπÑÍµê\n",
            "======================================================================\n",
            "\n",
            "[ÏÑ±Îä• Î≥ÄÌôî]\n",
            "ÏßÄÌëú              Simple       Stacking     Î≥ÄÌôî             \n",
            "-------------------------------------------------------\n",
            "F1 Score        0.8225       0.8239       +0.0015 (+0.18%)\n",
            "AUC             0.8862       0.8860       -0.0002\n",
            "FPR             25.90       % 24.74       % -1.16%p\n",
            "\n",
            "[Top 300 ÎåÄÎπÑ]\n",
            "  F1:  0.8300 ‚Üí 0.8239 (-0.61%p)\n",
            "  FPR: 24.67% ‚Üí 24.74% (+0.07%p)\n",
            "\n",
            "======================================================================\n",
            "Test Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏° (Stacking)\n",
            "======================================================================\n",
            "\n",
            "  Test Meta-features shape: (927, 3)\n",
            "\n",
            "[Test ÏòàÏ∏° Í≤∞Í≥º]\n",
            "  ÏòàÏ∏° Class 0: 392Í∞ú (42.29%)\n",
            "  ÏòàÏ∏° Class 1: 535Í∞ú (57.71%)\n",
            "  ÌèâÍ∑† Confidence: 0.3038\n",
            "  Low Confidence (<0.1): 94Í∞ú\n",
            "\n",
            "======================================================================\n",
            "Submission ÌååÏùº ÏÉùÏÑ±\n",
            "======================================================================\n",
            "\n",
            "‚úì Í∏∞Î≥∏ Ï†úÏ∂ú ÌååÏùº: submission_stacking_final.csv\n",
            "‚úì ÏÉÅÏÑ∏ Ï†úÏ∂ú ÌååÏùº: submission_detailed_stacking_final.csv\n",
            "\n",
            "======================================================================\n",
            "Î™®Îç∏ Ï†ÄÏû•\n",
            "======================================================================\n",
            "\n",
            "‚úì Î™®Îç∏ Ï†ÄÏû•: final_stacking_model.pkl\n",
            "  - Layer 1: 15Í∞ú Base Models\n",
            "  - Layer 2: Meta-Learner\n",
            "  - Ï†ÑÏ≤¥ ÌîºÏ≤ò Î¶¨Ïä§Ìä∏\n",
            "\n",
            "======================================================================\n",
            "ÏãúÍ∞ÅÌôî ÏÉùÏÑ±\n",
            "======================================================================\n",
            "\n",
            "‚úì ÏãúÍ∞ÅÌôî Ï†ÄÏû•: stacking_final_analysis.png\n",
            "\n",
            "======================================================================\n",
            "ÏµúÏ¢Ö ÏÑ±Îä• Î¶¨Ìè¨Ìä∏\n",
            "======================================================================\n",
            "\n",
            "[Î™®Îç∏ ÏÇ¨Ïñë]\n",
            "  Architecture: 2-Layer Stacking\n",
            "    - Layer 1: LGBM + XGB + CAT\n",
            "    - Layer 2: Logistic Regression\n",
            "  ÌîºÏ≤ò: Ï†ÑÏ≤¥ 3076Í∞ú\n",
            "  Threshold: Adaptive (0.42/0.40/0.39)\n",
            "\n",
            "[ÏµúÏ¢Ö ÏÑ±Îä•]\n",
            "  F1 Score:  0.8239\n",
            "  AUC Score: 0.8860\n",
            "  Precision: 0.8031\n",
            "  Recall:    0.8459\n",
            "  FPR:       24.74%\n",
            "\n",
            "‚ñ≥ Top 300Í≥º Ïú†ÏÇ¨Ìïú ÏÑ±Îä•\n",
            "\n",
            "[Ï†úÏ∂ú ÌååÏùº]\n",
            "  Î©îÏù∏: submission_stacking_final.csv\n",
            "  ÏÉÅÏÑ∏: submission_detailed_stacking_final.csv\n",
            "\n",
            "======================================================================\n",
            "‚úì Stacking Î™®Îç∏ ÏôÑÏÑ±!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# RDKit Descriptor Ï∂îÍ∞Ä ÌîºÏ≤ò ÏÉùÏÑ±\n",
        "# ============================================================\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RDKit Descriptor ÌîºÏ≤ò ÏÉùÏÑ±\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def calculate_rdkit_descriptors(smiles):\n",
        "    \"\"\"\n",
        "    SMILESÏóêÏÑú RDKit Descriptor Í≥ÑÏÇ∞\n",
        "\n",
        "    ÎèÖÏÑ± ÏòàÏ∏° ÌäπÌôî Descriptors:\n",
        "    - Lipinski descriptors (ÏïΩÎ¨ºÏÑ±)\n",
        "    - Íµ¨Ï°∞Ï†Å ÌäπÏßï (Í≥†Î¶¨, Í≤∞Ìï©)\n",
        "    - Ï†ÑÏûêÏ†Å ÌäπÏÑ±\n",
        "    \"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        descriptors = {\n",
        "            # Lipinski Descriptors (ÏïΩÎ¨ºÏÑ±)\n",
        "            'NumHDonors': Descriptors.NumHDonors(mol),\n",
        "            'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
        "            'MolLogP': Descriptors.MolLogP(mol),\n",
        "            'TPSA': Descriptors.TPSA(mol),\n",
        "\n",
        "            # Íµ¨Ï°∞Ï†Å ÌäπÏßï\n",
        "            'NumAromaticRings': Descriptors.NumAromaticRings(mol),\n",
        "            'NumAliphaticRings': Descriptors.NumAliphaticRings(mol),\n",
        "            'NumSaturatedRings': Descriptors.NumSaturatedRings(mol),\n",
        "            'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'NumHeteroatoms': Descriptors.NumHeteroatoms(mol),\n",
        "\n",
        "            # Ï†ÑÏûêÏ†Å ÌäπÏÑ±\n",
        "            'FractionCsp3': Descriptors.FractionCsp3(mol),\n",
        "            'Chi0v': Descriptors.Chi0v(mol),\n",
        "            'HallKierAlpha': Descriptors.HallKierAlpha(mol),\n",
        "\n",
        "            # ÌëúÎ©¥Ï†Å Î∞è Î∂ÄÌîº\n",
        "            'LabuteASA': Descriptors.LabuteASA(mol),\n",
        "            'PEOE_VSA1': Descriptors.PEOE_VSA1(mol),\n",
        "\n",
        "            # Ï∂îÍ∞Ä (ÏÑ†ÌÉù)\n",
        "            'BertzCT': Descriptors.BertzCT(mol),  # Î≥µÏû°ÎèÑ\n",
        "            'Ipc': Descriptors.Ipc(mol),          # Ï†ïÎ≥¥ Ìï®Îüâ\n",
        "            'RingCount': Descriptors.RingCount(mol),\n",
        "            'NumBridgeheadAtoms': Descriptors.NumBridgeheadAtoms(mol),\n",
        "            'NumSpiroAtoms': Descriptors.NumSpiroAtoms(mol)\n",
        "        }\n",
        "\n",
        "        return descriptors\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {smiles}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ============================================================\n",
        "# 1. Train Îç∞Ïù¥ÌÑ∞Ïóê RDKit Descriptor Ï∂îÍ∞Ä\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Train Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Train Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "df_train = pd.read_csv('train.csv')\n",
        "\n",
        "# SMILES Ïª¨Îüº ÌôïÏù∏ (ÏóÜÏúºÎ©¥ Î≥ÑÎèÑ ÌååÏùºÏóêÏÑú Î°úÎìú ÌïÑÏöî)\n",
        "if 'SMILES' in df_train.columns:\n",
        "    train_smiles = df_train['SMILES']\n",
        "elif 'smiles' in df_train.columns:\n",
        "    train_smiles = df_train['smiles']\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Train Îç∞Ïù¥ÌÑ∞Ïóê SMILES ÏóÜÏùå\")\n",
        "    print(\"   Ìï¥Í≤∞Ï±Ö:\")\n",
        "    print(\"   1. Train SMILES ÌååÏùº Î≥ÑÎèÑ Ï†úÍ≥µ ÌïÑÏöî\")\n",
        "    print(\"   2. ÎòêÎäî Í∏∞Ï°¥ SMILESÎ°úÎ∂ÄÌÑ∞ Ïû¨ÏÉùÏÑ±\")\n",
        "    train_smiles = None\n",
        "\n",
        "if train_smiles is not None:\n",
        "    print(f\"\\n  SMILES Î∞úÍ≤¨: {len(train_smiles)}Í∞ú\")\n",
        "\n",
        "    # RDKit Descriptor Í≥ÑÏÇ∞\n",
        "    print(f\"  RDKit Descriptor Í≥ÑÏÇ∞ Ï§ë...\")\n",
        "    rdkit_descriptors_list = []\n",
        "\n",
        "    for idx, smiles in enumerate(train_smiles):\n",
        "        if idx % 1000 == 0:\n",
        "            print(f\"\\r    ÏßÑÌñâ: {idx}/{len(train_smiles)}\", end='')\n",
        "\n",
        "        desc = calculate_rdkit_descriptors(smiles)\n",
        "        rdkit_descriptors_list.append(desc)\n",
        "\n",
        "    print(f\"\\r    ‚úì ÏôÑÎ£å: {len(train_smiles)}Í∞ú\")\n",
        "\n",
        "    # DataFrame Î≥ÄÌôò\n",
        "    rdkit_df = pd.DataFrame(rdkit_descriptors_list)\n",
        "\n",
        "    print(f\"\\n  ÏÉùÏÑ±Îêú RDKit Descriptor: {len(rdkit_df.columns)}Í∞ú\")\n",
        "    print(f\"  Ïª¨Îüº: {list(rdkit_df.columns)}\")\n",
        "\n",
        "    # Í≤∞Ï∏°Ïπò ÌôïÏù∏\n",
        "    missing_count = rdkit_df.isnull().sum().sum()\n",
        "    if missing_count > 0:\n",
        "        print(f\"\\n  ‚ö†Ô∏è  Í≤∞Ï∏°Ïπò: {missing_count}Í∞ú\")\n",
        "        print(f\"     Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨: median imputation\")\n",
        "        rdkit_df = rdkit_df.fillna(rdkit_df.median())\n",
        "\n",
        "    # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Ïóê Ï∂îÍ∞Ä\n",
        "    df_train_enhanced = pd.concat([df_train, rdkit_df], axis=1)\n",
        "\n",
        "    print(f\"\\n  ÏµúÏ¢Ö Train Îç∞Ïù¥ÌÑ∞: {df_train_enhanced.shape}\")\n",
        "    print(f\"    Í∏∞Ï°¥: {df_train.shape}\")\n",
        "    print(f\"    Ï∂îÍ∞Ä: {rdkit_df.shape[1]}Í∞ú Ïª¨Îüº\")\n",
        "\n",
        "    # Ï†ÄÏû•\n",
        "    df_train_enhanced.to_csv('train_with_rdkit.csv', index=False)\n",
        "    print(f\"\\n‚úì Ï†ÄÏû•: train_with_rdkit.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Test Îç∞Ïù¥ÌÑ∞Ïóê RDKit Descriptor Ï∂îÍ∞Ä\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Test Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "df_test = pd.read_csv('predict_input.csv')\n",
        "\n",
        "# SMILES Ïª¨Îüº ÌôïÏù∏\n",
        "if 'SMILES' in df_test.columns:\n",
        "    test_smiles = df_test['SMILES']\n",
        "elif 'smiles' in df_test.columns:\n",
        "    test_smiles = df_test['smiles']\n",
        "else:\n",
        "    test_smiles = df_test[df_test.columns[0]]  # Ï≤´ Î≤àÏß∏ Ïª¨Îüº ÏãúÎèÑ\n",
        "\n",
        "print(f\"\\n  SMILES: {len(test_smiles)}Í∞ú\")\n",
        "\n",
        "# RDKit Descriptor Í≥ÑÏÇ∞\n",
        "print(f\"  RDKit Descriptor Í≥ÑÏÇ∞ Ï§ë...\")\n",
        "rdkit_descriptors_test_list = []\n",
        "\n",
        "for idx, smiles in enumerate(test_smiles):\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"\\r    ÏßÑÌñâ: {idx}/{len(test_smiles)}\", end='')\n",
        "\n",
        "    desc = calculate_rdkit_descriptors(smiles)\n",
        "    rdkit_descriptors_test_list.append(desc)\n",
        "\n",
        "print(f\"\\r    ‚úì ÏôÑÎ£å: {len(test_smiles)}Í∞ú\")\n",
        "\n",
        "# DataFrame Î≥ÄÌôò\n",
        "rdkit_test_df = pd.DataFrame(rdkit_descriptors_test_list)\n",
        "\n",
        "# Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨\n",
        "if rdkit_test_df.isnull().sum().sum() > 0:\n",
        "    rdkit_test_df = rdkit_test_df.fillna(rdkit_test_df.median())\n",
        "\n",
        "# Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Ïóê Ï∂îÍ∞Ä\n",
        "df_test_enhanced = pd.concat([df_test, rdkit_test_df], axis=1)\n",
        "\n",
        "print(f\"\\n  ÏµúÏ¢Ö Test Îç∞Ïù¥ÌÑ∞: {df_test_enhanced.shape}\")\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "df_test_enhanced.to_csv('predict_input_with_rdkit.csv', index=False)\n",
        "print(f\"\\n‚úì Ï†ÄÏû•: predict_input_with_rdkit.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù (Ï§ëÎ≥µ ÌôïÏù∏)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Í∏∞Ï°¥ DescriptorÏôÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if train_smiles is not None:\n",
        "    existing_desc = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "\n",
        "    print(f\"\\n[ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Îß§Ìä∏Î¶≠Ïä§]\")\n",
        "\n",
        "    # Í∏∞Ï°¥ + ÏÉàÎ°úÏö¥ Descriptor\n",
        "    all_descriptors = existing_desc + list(rdkit_df.columns)\n",
        "    correlation_df = df_train_enhanced[all_descriptors].corr()\n",
        "\n",
        "    # ÎÜíÏùÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Ï∞æÍ∏∞ (|r| > 0.8)\n",
        "    high_corr_pairs = []\n",
        "\n",
        "    for i, col1 in enumerate(all_descriptors):\n",
        "        for col2 in all_descriptors[i+1:]:\n",
        "            corr = correlation_df.loc[col1, col2]\n",
        "            if abs(corr) > 0.8:\n",
        "                high_corr_pairs.append((col1, col2, corr))\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(f\"\\n  ‚ö†Ô∏è  ÎÜíÏùÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∞úÍ≤¨ (|r| > 0.8):\")\n",
        "        for col1, col2, corr in high_corr_pairs[:10]:  # ÏÉÅÏúÑ 10Í∞ú\n",
        "            print(f\"    {col1} ‚Üî {col2}: {corr:.3f}\")\n",
        "        print(f\"\\n  Ï¥ù {len(high_corr_pairs)}Ïåç\")\n",
        "        print(f\"  Í∂åÏû•: Ìïú Ï™Ω Ï†úÍ±∞ÌïòÏó¨ Îã§Ï§ëÍ≥µÏÑ†ÏÑ± Î∞©ÏßÄ\")\n",
        "    else:\n",
        "        print(f\"\\n  ‚úì ÎÜíÏùÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÏóÜÏùå - Î™®Îëê ÎèÖÎ¶ΩÏ†Å\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. ÏòàÏÉÅ ÏÑ±Îä• Í∞úÏÑ†\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ÏòàÏÉÅ Ìö®Í≥º\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[RDKit Descriptor Ï∂îÍ∞Ä Ìö®Í≥º]\")\n",
        "print(f\"  Í∏∞Ï°¥ ÌîºÏ≤ò: 3076Í∞ú (Fingerprint 3072 + Descriptor 4)\")\n",
        "print(f\"  Ï∂îÍ∞Ä ÌîºÏ≤ò: {len(rdkit_df.columns)}Í∞ú (RDKit Descriptors)\")\n",
        "print(f\"  ÏµúÏ¢Ö ÌîºÏ≤ò: {3076 + len(rdkit_df.columns)}Í∞ú\")\n",
        "\n",
        "print(f\"\\n[ÏòàÏÉÅ ÏÑ±Îä• Ìñ•ÏÉÅ]\")\n",
        "print(f\"  ÌòÑÏû¨ F1: 0.8300~0.8306\")\n",
        "print(f\"  ÏòàÏÉÅ F1: 0.8350~0.8400 (+0.5~1.0%)\")\n",
        "print(f\"  Í∑ºÍ±∞:\")\n",
        "print(f\"    - Domain knowledge ÌôúÏö©\")\n",
        "print(f\"    - ÌôîÌïôÏ†Å ÏùòÎØ∏ ÏûàÎäî ÌîºÏ≤ò\")\n",
        "print(f\"    - ÎèÖÏÑ±Í≥º ÏßÅÏ†ë Í¥ÄÎ†®\")\n",
        "\n",
        "print(f\"\\n‚úì RDKit Descriptor ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "print(f\"  Îã§Ïùå: train_with_rdkit.csvÎ°ú Î™®Îç∏ Ïû¨ÌïôÏäµ\")\n"
      ],
      "metadata": {
        "id": "8ICEa6HZDqJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}