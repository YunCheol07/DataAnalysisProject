{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoYCzyZM6C9-",
        "outputId": "84c2eaec-bf92-4690-9d24-56c73d097cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Top 500 피처 모델 학습 및 평가\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Top 500 피처 모델 학습 및 평가\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================================\n",
        "# 1. Top 500 피처 선정\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Top 500 피처 선정\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Feature importance 로드\n",
        "importance_df = pd.read_csv('feature_importance_ensemble_cv.csv')\n",
        "\n",
        "# Top 500 선택\n",
        "N_FEATURES = 500\n",
        "selected_features = importance_df.head(N_FEATURES)['feature'].tolist()\n",
        "\n",
        "print(f\"\\n[선정된 피처]\")\n",
        "print(f\"  총 피처: {len(selected_features)}개\")\n",
        "\n",
        "# 타입별 분포\n",
        "fp_cols = [f for f in selected_features if f.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = [f for f in selected_features if f in ['MolWt', 'clogp', 'sa_score', 'qed']]\n",
        "\n",
        "ecfp_count = len([f for f in fp_cols if f.startswith('ecfp_')])\n",
        "fcfp_count = len([f for f in fp_cols if f.startswith('fcfp_')])\n",
        "ptfp_count = len([f for f in fp_cols if f.startswith('ptfp_')])\n",
        "\n",
        "print(f\"  - Descriptor: {len(desc_cols)}개\")\n",
        "print(f\"  - Fingerprint: {len(fp_cols)}개\")\n",
        "print(f\"    · ECFP: {ecfp_count}개\")\n",
        "print(f\"    · FCFP: {fcfp_count}개\")\n",
        "print(f\"    · PTFP: {ptfp_count}개\")\n",
        "\n",
        "# 누적 중요도 계산\n",
        "cumsum_importance = importance_df.head(N_FEATURES)['ensemble_mean'].sum()\n",
        "total_importance = importance_df['ensemble_mean'].sum()\n",
        "cumsum_pct = cumsum_importance / total_importance * 100\n",
        "\n",
        "print(f\"\\n[누적 중요도]\")\n",
        "print(f\"  Top 500 중요도 합: {cumsum_importance:.2f}\")\n",
        "print(f\"  전체 중요도 합: {total_importance:.2f}\")\n",
        "print(f\"  누적 비율: {cumsum_pct:.2f}%\")\n",
        "\n",
        "# CSV 저장\n",
        "pd.DataFrame({'feature': selected_features}).to_csv('selected_features_top500.csv', index=False)\n",
        "print(f\"\\n✓ 피처 리스트 저장: selected_features_top500.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. 데이터 준비\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"데이터 로드\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Train 데이터\n",
        "df_train = pd.read_csv('train.csv')\n",
        "X_train = df_train[selected_features]\n",
        "y_train = df_train['label'].astype(int)\n",
        "\n",
        "print(f\"\\n[Train 데이터]\")\n",
        "print(f\"  Shape: {X_train.shape}\")\n",
        "print(f\"  Label 분포: Class 0 = {sum(y_train==0)}, Class 1 = {sum(y_train==1)}\")\n",
        "\n",
        "# Test 데이터\n",
        "try:\n",
        "    df_test = pd.read_csv('predict_input.csv')\n",
        "    X_test = df_test[selected_features]\n",
        "    print(f\"\\n[Test 데이터]\")\n",
        "    print(f\"  Shape: {X_test.shape}\")\n",
        "    test_available = True\n",
        "except:\n",
        "    print(f\"\\n⚠️  Test 데이터 없음\")\n",
        "    test_available = False\n",
        "\n",
        "# 전처리 파이프라인\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 3. 5-Fold CV 학습\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"5-Fold Cross-Validation 학습\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# 결과 저장\n",
        "results = {\n",
        "    'lgbm': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'xgb': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'catboost': {'models': [], 'oof_probabilities': np.zeros(len(X_train))},\n",
        "    'fold_details': []\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print(f\"\\n{'─'*70}\")\n",
        "    print(f\"📊 Fold {fold}/5\")\n",
        "    print(f\"{'─'*70}\")\n",
        "\n",
        "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
        "    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
        "\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    print(f\"  학습: {Xt_tr.shape}, 검증: {Xt_va.shape}\")\n",
        "\n",
        "    # LightGBM\n",
        "    print(f\"  [1/3] LightGBM...\", end=' ')\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "        num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "        colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)],\n",
        "                   callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "\n",
        "    lgbm_proba = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "    results['lgbm']['models'].append(lgbm_model)\n",
        "    results['lgbm']['oof_probabilities'][va_idx] = lgbm_proba\n",
        "\n",
        "    print(f\"F1: {f1_score(y_va, (lgbm_proba >= 0.39).astype(int)):.4f}, Iter: {lgbm_model.best_iteration_}\")\n",
        "\n",
        "    # XGBoost\n",
        "    print(f\"  [2/3] XGBoost...\", end=' ')\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "        gamma=0.1, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1,\n",
        "        early_stopping_rounds=100, eval_metric='logloss', verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)], verbose=False)\n",
        "\n",
        "    xgb_proba = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "    results['xgb']['models'].append(xgb_model)\n",
        "    results['xgb']['oof_probabilities'][va_idx] = xgb_proba\n",
        "\n",
        "    print(f\"F1: {f1_score(y_va, (xgb_proba >= 0.39).astype(int)):.4f}, Iter: {xgb_model.best_iteration}\")\n",
        "\n",
        "    # CatBoost\n",
        "    print(f\"  [3/3] CatBoost...\", end=' ')\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000, learning_rate=0.03, depth=7,\n",
        "        l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE, verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "    cat_model.fit(Xt_tr, y_tr, eval_set=(Xt_va, y_va), verbose=False)\n",
        "\n",
        "    cat_proba = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "    results['catboost']['models'].append(cat_model)\n",
        "    results['catboost']['oof_probabilities'][va_idx] = cat_proba\n",
        "\n",
        "    print(f\"F1: {f1_score(y_va, (cat_proba >= 0.39).astype(int)):.4f}, Iter: {cat_model.best_iteration_}\")\n",
        "\n",
        "    # Ensemble\n",
        "    ensemble_proba = 0.20 * lgbm_proba + 0.60 * xgb_proba + 0.20 * cat_proba\n",
        "\n",
        "    # Adaptive Threshold\n",
        "    confidence = np.abs(ensemble_proba - 0.5)\n",
        "    adaptive_thresholds = np.where(confidence < 0.05, 0.42,\n",
        "                                    np.where(confidence < 0.10, 0.40, 0.39))\n",
        "    ensemble_pred = (ensemble_proba >= adaptive_thresholds).astype(int)\n",
        "\n",
        "    ensemble_f1 = f1_score(y_va, ensemble_pred)\n",
        "    ensemble_auc = roc_auc_score(y_va, ensemble_proba)\n",
        "\n",
        "    cm = confusion_matrix(y_va, ensemble_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "\n",
        "    print(f\"\\n  [Ensemble] F1: {ensemble_f1:.4f}, AUC: {ensemble_auc:.4f}, FPR: {fpr:.4f}\")\n",
        "\n",
        "    results['fold_details'].append({\n",
        "        'fold': fold,\n",
        "        'ensemble_f1': ensemble_f1,\n",
        "        'ensemble_auc': ensemble_auc,\n",
        "        'fpr': fpr\n",
        "    })\n",
        "\n",
        "# ============================================================\n",
        "# 4. OOF 성능 평가\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"OOF 성능 평가\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Ensemble 확률\n",
        "ensemble_oof_proba = (\n",
        "    0.20 * results['lgbm']['oof_probabilities'] +\n",
        "    0.60 * results['xgb']['oof_probabilities'] +\n",
        "    0.20 * results['catboost']['oof_probabilities']\n",
        ")\n",
        "\n",
        "# Adaptive Threshold 적용\n",
        "confidence_oof = np.abs(ensemble_oof_proba - 0.5)\n",
        "\n",
        "def get_adaptive_threshold(confidence):\n",
        "    if confidence < 0.05:\n",
        "        return 0.42\n",
        "    elif confidence < 0.10:\n",
        "        return 0.40\n",
        "    else:\n",
        "        return 0.39\n",
        "\n",
        "adaptive_thresholds_oof = np.array([get_adaptive_threshold(c) for c in confidence_oof])\n",
        "predictions_oof = (ensemble_oof_proba >= adaptive_thresholds_oof).astype(int)\n",
        "\n",
        "# 성능 계산\n",
        "oof_f1 = f1_score(y_train, predictions_oof)\n",
        "oof_auc = roc_auc_score(y_train, ensemble_oof_proba)\n",
        "oof_cm = confusion_matrix(y_train, predictions_oof)\n",
        "tn, fp, fn, tp = oof_cm.ravel()\n",
        "oof_fpr = fp / (fp + tn)\n",
        "oof_precision = precision_score(y_train, predictions_oof)\n",
        "oof_recall = recall_score(y_train, predictions_oof)\n",
        "\n",
        "print(f\"\\n[Top 500 OOF 성능]\")\n",
        "print(f\"  F1 Score:  {oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {oof_auc:.4f}\")\n",
        "print(f\"  Precision: {oof_precision:.4f}\")\n",
        "print(f\"  Recall:    {oof_recall:.4f}\")\n",
        "print(f\"  FPR:       {oof_fpr:.4f} ({oof_fpr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[OOF 혼동 행렬]\")\n",
        "print(f\"  TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
        "\n",
        "# Low Confidence\n",
        "low_conf_mask = confidence_oof < 0.1\n",
        "n_low_conf = low_conf_mask.sum()\n",
        "low_conf_acc = (predictions_oof[low_conf_mask] == y_train[low_conf_mask]).mean() if n_low_conf > 0 else 0\n",
        "\n",
        "print(f\"\\n[Low Confidence]\")\n",
        "print(f\"  개수: {n_low_conf}개 ({n_low_conf/len(y_train)*100:.2f}%)\")\n",
        "print(f\"  정확도: {low_conf_acc:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. Top 300과 비교\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Top 300 vs Top 500 비교\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Top 300 성능 (기준)\n",
        "baseline_f1 = 0.8300\n",
        "baseline_fpr = 0.2467\n",
        "\n",
        "f1_improvement = oof_f1 - baseline_f1\n",
        "fpr_improvement = oof_fpr - baseline_fpr\n",
        "\n",
        "print(f\"\\n[성능 비교]\")\n",
        "print(f\"{'지표':<15} {'Top 300':<12} {'Top 500':<12} {'변화':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'F1 Score':<15} {baseline_f1:<12.4f} {oof_f1:<12.4f} {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\")\n",
        "print(f\"{'FPR':<15} {baseline_fpr*100:<12.2f}% {oof_fpr*100:<12.2f}% {fpr_improvement*100:+.2f}%p\")\n",
        "print(f\"{'피처 수':<15} {'300':<12} {'500':<12} {'+200'}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. Test 예측 (Test 데이터 있을 경우)\n",
        "# ============================================================\n",
        "if test_available:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Test 데이터 예측\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # 전체 Train 데이터로 전처리기 학습\n",
        "    Xt_train_full = preprocessor.fit_transform(X_train)\n",
        "    Xt_test = preprocessor.transform(X_test)\n",
        "\n",
        "    print(f\"\\n  Test shape: {Xt_test.shape}\")\n",
        "\n",
        "    # 각 Fold 모델로 예측\n",
        "    test_predictions = {\n",
        "        'lgbm': np.zeros((len(X_test), 5)),\n",
        "        'xgb': np.zeros((len(X_test), 5)),\n",
        "        'catboost': np.zeros((len(X_test), 5))\n",
        "    }\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"\\r  Fold {fold+1}/5 예측 중...\", end='')\n",
        "        test_predictions['lgbm'][:, fold] = results['lgbm']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "        test_predictions['xgb'][:, fold] = results['xgb']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "        test_predictions['catboost'][:, fold] = results['catboost']['models'][fold].predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "    print(f\"\\r  ✓ 5-Fold 예측 완료\")\n",
        "\n",
        "    # 평균 확률\n",
        "    lgbm_proba_test = test_predictions['lgbm'].mean(axis=1)\n",
        "    xgb_proba_test = test_predictions['xgb'].mean(axis=1)\n",
        "    cat_proba_test = test_predictions['catboost'].mean(axis=1)\n",
        "\n",
        "    # Ensemble\n",
        "    ensemble_proba_test = 0.20 * lgbm_proba_test + 0.60 * xgb_proba_test + 0.20 * cat_proba_test\n",
        "\n",
        "    # Adaptive Threshold 적용\n",
        "    confidence_test = np.abs(ensemble_proba_test - 0.5)\n",
        "    adaptive_thresholds_test = np.array([get_adaptive_threshold(c) for c in confidence_test])\n",
        "    predictions_test = (ensemble_proba_test >= adaptive_thresholds_test).astype(int)\n",
        "\n",
        "    print(f\"\\n[Test 예측 결과]\")\n",
        "    print(f\"  예측 Class 0: {sum(predictions_test == 0)}개\")\n",
        "    print(f\"  예측 Class 1: {sum(predictions_test == 1)}개\")\n",
        "    print(f\"  평균 Confidence: {confidence_test.mean():.4f}\")\n",
        "    print(f\"  Low Confidence (<0.1): {sum(confidence_test < 0.1)}개\")\n",
        "\n",
        "    # ============================================================\n",
        "    # 7. Submission 파일 생성\n",
        "    # ============================================================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Submission 파일 생성\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # SMILES 컬럼 확인\n",
        "    if 'SMILES' in df_test.columns:\n",
        "        smiles_col = 'SMILES'\n",
        "    elif 'smiles' in df_test.columns:\n",
        "        smiles_col = 'smiles'\n",
        "    else:\n",
        "        smiles_col = df_test.columns[0]\n",
        "\n",
        "    # 기본 제출 파일\n",
        "    submission = pd.DataFrame({\n",
        "        'SMILES': df_test[smiles_col],\n",
        "        'output': predictions_test\n",
        "    })\n",
        "    submission.to_csv('submission_top500.csv', index=False)\n",
        "    print(f\"\\n✓ 기본 제출 파일: submission_top500.csv\")\n",
        "\n",
        "    # 상세 제출 파일\n",
        "    submission_detailed = pd.DataFrame({\n",
        "        'id': range(len(predictions_test)),\n",
        "        'label': predictions_test,\n",
        "        'probability': ensemble_proba_test,\n",
        "        'confidence': confidence_test,\n",
        "        'adaptive_threshold': adaptive_thresholds_test,\n",
        "        'lgbm_proba': lgbm_proba_test,\n",
        "        'xgb_proba': xgb_proba_test,\n",
        "        'catboost_proba': cat_proba_test\n",
        "    })\n",
        "    submission_detailed.to_csv('submission_detailed_top500.csv', index=False)\n",
        "    print(f\"✓ 상세 제출 파일: submission_detailed_top500.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 8. 시각화\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"시각화 생성\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Feature Count Comparison\n",
        "ax = axes[0, 0]\n",
        "feature_counts = [300, 500]\n",
        "f1_scores = [baseline_f1, oof_f1]\n",
        "colors = ['lightblue', 'darkgreen']\n",
        "\n",
        "bars = ax.bar([str(x) for x in feature_counts], f1_scores,\n",
        "              color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('F1 Score', fontsize=11)\n",
        "ax.set_title('F1 Score by Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.set_ylim([0.825, 0.835])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, f1_scores):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.4f}',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. FPR Comparison\n",
        "ax = axes[0, 1]\n",
        "fpr_values = [baseline_fpr * 100, oof_fpr * 100]\n",
        "bars = ax.bar([str(x) for x in feature_counts], fpr_values,\n",
        "              color=['lightcoral', 'lightgreen'], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.axhline(25, color='r', linestyle='--', alpha=0.5, label='Target: 25%')\n",
        "ax.set_xlabel('Number of Features', fontsize=11)\n",
        "ax.set_ylabel('FPR (%)', fontsize=11)\n",
        "ax.set_title('FPR by Feature Count', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, fpr_values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.2f}%',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3. Confusion Matrix\n",
        "ax = axes[1, 0]\n",
        "sns.heatmap(oof_cm, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={'size': 14})\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix (Top 500)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 4. Performance Summary\n",
        "ax = axes[1, 1]\n",
        "ax.axis('off')\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "═══════════════════════\n",
        " Top 500 성능 요약\n",
        "═══════════════════════\n",
        "\n",
        "F1 Score:  {oof_f1:.4f}\n",
        "AUC:       {oof_auc:.4f}\n",
        "Precision: {oof_precision:.4f}\n",
        "Recall:    {oof_recall:.4f}\n",
        "FPR:       {oof_fpr*100:.2f}%\n",
        "\n",
        "─────────────────────\n",
        "Top 300 대비\n",
        "─────────────────────\n",
        "F1:   {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\n",
        "FPR:  {fpr_improvement*100:+.2f}%p\n",
        "\n",
        "─────────────────────\n",
        "피처: 500개 (16.3%)\n",
        "압축률: 83.7%\n",
        "\"\"\"\n",
        "\n",
        "ax.text(0.5, 0.5, summary_text, ha='center', va='center',\n",
        "        fontsize=11, family='monospace',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('top500_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"\\n✓ 시각화 저장: top500_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. 최종 리포트\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"최종 리포트\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[모델 사양]\")\n",
        "print(f\"  피처: Top 500개 (압축률 83.7%)\")\n",
        "print(f\"  Ensemble: LGBM(20%) + XGB(60%) + CAT(20%)\")\n",
        "print(f\"  Threshold: Adaptive (0.42/0.40/0.39)\")\n",
        "\n",
        "print(f\"\\n[성능]\")\n",
        "print(f\"  F1 Score:  {oof_f1:.4f}\")\n",
        "print(f\"  AUC Score: {oof_auc:.4f}\")\n",
        "print(f\"  FPR:       {oof_fpr*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n[Top 300 대비]\")\n",
        "if f1_improvement > 0:\n",
        "    print(f\"  ✓✓ F1 향상: {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\")\n",
        "else:\n",
        "    print(f\"  △ F1 변화: {f1_improvement:+.4f} ({f1_improvement/baseline_f1*100:+.2f}%)\")\n",
        "\n",
        "if fpr_improvement < 0:\n",
        "    print(f\"  ✓✓ FPR 개선: {fpr_improvement*100:+.2f}%p\")\n",
        "else:\n",
        "    print(f\"  △ FPR 변화: {fpr_improvement*100:+.2f}%p\")\n",
        "\n",
        "if test_available:\n",
        "    print(f\"\\n[제출 파일]\")\n",
        "    print(f\"  메인: submission_top500.csv\")\n",
        "    print(f\"  상세: submission_detailed_top500.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✓ Top 500 모델 완성!\")\n",
        "print(f\"{'='*70}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hYoPl3I6Vul",
        "outputId": "9610bf5c-cb25-4212-ea87-e2513a5fb534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Top 500 피처 모델 학습 및 평가\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Top 500 피처 선정\n",
            "======================================================================\n",
            "\n",
            "[선정된 피처]\n",
            "  총 피처: 500개\n",
            "  - Descriptor: 4개\n",
            "  - Fingerprint: 496개\n",
            "    · ECFP: 158개\n",
            "    · FCFP: 110개\n",
            "    · PTFP: 228개\n",
            "\n",
            "[누적 중요도]\n",
            "  Top 500 중요도 합: 28620.44\n",
            "  전체 중요도 합: 36487.29\n",
            "  누적 비율: 78.44%\n",
            "\n",
            "✓ 피처 리스트 저장: selected_features_top500.csv\n",
            "\n",
            "======================================================================\n",
            "데이터 로드\n",
            "======================================================================\n",
            "\n",
            "[Train 데이터]\n",
            "  Shape: (8349, 500)\n",
            "  Label 분포: Class 0 = 3807, Class 1 = 4542\n",
            "\n",
            "[Test 데이터]\n",
            "  Shape: (927, 500)\n",
            "\n",
            "======================================================================\n",
            "5-Fold Cross-Validation 학습\n",
            "======================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 1/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 500), 검증: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8387, Iter: 827\n",
            "  [2/3] XGBoost... F1: 0.8470, Iter: 997\n",
            "  [3/3] CatBoost... F1: 0.8376, Iter: 996\n",
            "\n",
            "  [Ensemble] F1: 0.8429, AUC: 0.9072, FPR: 0.2378\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 2/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 500), 검증: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8225, Iter: 637\n",
            "  [2/3] XGBoost... F1: 0.8191, Iter: 698\n",
            "  [3/3] CatBoost... F1: 0.8209, Iter: 990\n",
            "\n",
            "  [Ensemble] F1: 0.8203, AUC: 0.8868, FPR: 0.2681\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 3/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 500), 검증: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8105, Iter: 626\n",
            "  [2/3] XGBoost... F1: 0.8096, Iter: 661\n",
            "  [3/3] CatBoost... F1: 0.8057, Iter: 999\n",
            "\n",
            "  [Ensemble] F1: 0.8098, AUC: 0.8772, FPR: 0.2454\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 4/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 500), 검증: (1670, 500)\n",
            "  [1/3] LightGBM... F1: 0.8269, Iter: 732\n",
            "  [2/3] XGBoost... F1: 0.8314, Iter: 721\n",
            "  [3/3] CatBoost... F1: 0.8305, Iter: 988\n",
            "\n",
            "  [Ensemble] F1: 0.8350, AUC: 0.8944, FPR: 0.2677\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 5/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6680, 500), 검증: (1669, 500)\n",
            "  [1/3] LightGBM... F1: 0.8474, Iter: 838\n",
            "  [2/3] XGBoost... F1: 0.8461, Iter: 986\n",
            "  [3/3] CatBoost... F1: 0.8381, Iter: 997\n",
            "\n",
            "  [Ensemble] F1: 0.8448, AUC: 0.9040, FPR: 0.2247\n",
            "\n",
            "======================================================================\n",
            "OOF 성능 평가\n",
            "======================================================================\n",
            "\n",
            "[Top 500 OOF 성능]\n",
            "  F1 Score:  0.8306\n",
            "  AUC Score: 0.8940\n",
            "  Precision: 0.8046\n",
            "  Recall:    0.8584\n",
            "  FPR:       0.2488 (24.88%)\n",
            "\n",
            "[OOF 혼동 행렬]\n",
            "  TN: 2860, FP: 947, FN: 643, TP: 3899\n",
            "\n",
            "[Low Confidence]\n",
            "  개수: 970개 (11.62%)\n",
            "  정확도: 0.5412\n",
            "\n",
            "======================================================================\n",
            "Top 300 vs Top 500 비교\n",
            "======================================================================\n",
            "\n",
            "[성능 비교]\n",
            "지표              Top 300      Top 500      변화             \n",
            "-------------------------------------------------------\n",
            "F1 Score        0.8300       0.8306       +0.0006 (+0.08%)\n",
            "FPR             24.67       % 24.88       % +0.21%p\n",
            "피처 수            300          500          +200\n",
            "\n",
            "======================================================================\n",
            "Test 데이터 예측\n",
            "======================================================================\n",
            "\n",
            "  Test shape: (927, 500)\n",
            "  ✓ 5-Fold 예측 완료\n",
            "\n",
            "[Test 예측 결과]\n",
            "  예측 Class 0: 382개\n",
            "  예측 Class 1: 545개\n",
            "  평균 Confidence: 0.3115\n",
            "  Low Confidence (<0.1): 122개\n",
            "\n",
            "======================================================================\n",
            "Submission 파일 생성\n",
            "======================================================================\n",
            "\n",
            "✓ 기본 제출 파일: submission_top500.csv\n",
            "✓ 상세 제출 파일: submission_detailed_top500.csv\n",
            "\n",
            "======================================================================\n",
            "시각화 생성\n",
            "======================================================================\n",
            "\n",
            "✓ 시각화 저장: top500_analysis.png\n",
            "\n",
            "======================================================================\n",
            "최종 리포트\n",
            "======================================================================\n",
            "\n",
            "[모델 사양]\n",
            "  피처: Top 500개 (압축률 83.7%)\n",
            "  Ensemble: LGBM(20%) + XGB(60%) + CAT(20%)\n",
            "  Threshold: Adaptive (0.42/0.40/0.39)\n",
            "\n",
            "[성능]\n",
            "  F1 Score:  0.8306\n",
            "  AUC Score: 0.8940\n",
            "  FPR:       24.88%\n",
            "\n",
            "[Top 300 대비]\n",
            "  ✓✓ F1 향상: +0.0006 (+0.08%)\n",
            "  △ FPR 변화: +0.21%p\n",
            "\n",
            "[제출 파일]\n",
            "  메인: submission_top500.csv\n",
            "  상세: submission_detailed_top500.csv\n",
            "\n",
            "======================================================================\n",
            "✓ Top 500 모델 완성!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 최고 성능 모델: 전체 피처 + 2-Layer Stacking\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"최고 성능 모델: 전체 피처 + 2-Layer Stacking\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[전략]\")\n",
        "print(\"  Layer 1: LGBM, XGB, CAT (Base Models)\")\n",
        "print(\"  Layer 2: Logistic Regression (Meta-Learner)\")\n",
        "print(\"  피처: 전체 3076개 (압축 없음)\")\n",
        "print(\"  목표: F1 0.835+\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. 데이터 준비 (전체 피처)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"데이터 로드 (전체 피처)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Train 데이터\n",
        "df_train = pd.read_csv('train.csv')\n",
        "\n",
        "# 전체 피처 사용 (label 제외)\n",
        "feature_columns = [col for col in df_train.columns if col != 'label']\n",
        "X_train = df_train[feature_columns]\n",
        "y_train = df_train['label'].astype(int)\n",
        "\n",
        "print(f\"\\n[Train 데이터]\")\n",
        "print(f\"  Shape: {X_train.shape}\")\n",
        "print(f\"  피처: {len(feature_columns)}개 (전체)\")\n",
        "print(f\"  Label 분포: Class 0 = {sum(y_train==0)}, Class 1 = {sum(y_train==1)}\")\n",
        "\n",
        "# Test 데이터\n",
        "try:\n",
        "    df_test = pd.read_csv('predict_input.csv')\n",
        "    X_test = df_test[feature_columns]\n",
        "    print(f\"\\n[Test 데이터]\")\n",
        "    print(f\"  Shape: {X_test.shape}\")\n",
        "    test_available = True\n",
        "except:\n",
        "    print(f\"\\n⚠️  Test 데이터 없음\")\n",
        "    test_available = False\n",
        "\n",
        "# 피처 타입 분류\n",
        "fp_cols = [col for col in feature_columns if col.startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "\n",
        "print(f\"\\n[피처 구성]\")\n",
        "print(f\"  Fingerprint: {len(fp_cols)}개\")\n",
        "print(f\"  Descriptor: {len(desc_cols)}개\")\n",
        "\n",
        "# 전처리 파이프라인\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Layer 1: Base Models (5-Fold OOF)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Layer 1: Base Models 학습 (5-Fold CV)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# OOF 확률 저장\n",
        "oof_probabilities = {\n",
        "    'lgbm': np.zeros(len(X_train)),\n",
        "    'xgb': np.zeros(len(X_train)),\n",
        "    'catboost': np.zeros(len(X_train))\n",
        "}\n",
        "\n",
        "# Test 예측 저장 (각 Fold)\n",
        "if test_available:\n",
        "    test_predictions = {\n",
        "        'lgbm': np.zeros((len(X_test), 5)),\n",
        "        'xgb': np.zeros((len(X_test), 5)),\n",
        "        'catboost': np.zeros((len(X_test), 5))\n",
        "    }\n",
        "\n",
        "# 모델 저장\n",
        "models = {\n",
        "    'lgbm': [],\n",
        "    'xgb': [],\n",
        "    'catboost': []\n",
        "}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print(f\"\\n{'─'*70}\")\n",
        "    print(f\"📊 Fold {fold}/5\")\n",
        "    print(f\"{'─'*70}\")\n",
        "\n",
        "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
        "    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
        "\n",
        "    # 전처리\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "\n",
        "    print(f\"  학습: {Xt_tr.shape}, 검증: {Xt_va.shape}\")\n",
        "\n",
        "    # ========================================\n",
        "    # LightGBM\n",
        "    # ========================================\n",
        "    print(f\"  [1/3] LightGBM...\", end=' ')\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=8,\n",
        "        num_leaves=63,\n",
        "        min_child_samples=30,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=0.3,\n",
        "        class_weight={0: 1.5, 1: 1.0},\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    )\n",
        "\n",
        "    lgbm_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "\n",
        "    lgbm_proba_va = lgbm_model.predict_proba(Xt_va)[:, 1]\n",
        "    oof_probabilities['lgbm'][va_idx] = lgbm_proba_va\n",
        "    models['lgbm'].append((lgbm_model, preprocessor))\n",
        "\n",
        "    print(f\"완료 (Iter: {lgbm_model.best_iteration_})\")\n",
        "\n",
        "    # Test 예측\n",
        "    if test_available:\n",
        "        Xt_test = preprocessor.transform(X_test)\n",
        "        test_predictions['lgbm'][:, fold-1] = lgbm_model.predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "    # ========================================\n",
        "    # XGBoost\n",
        "    # ========================================\n",
        "    print(f\"  [2/3] XGBoost...\", end=' ')\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=7,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        gamma=0.1,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=0.3,\n",
        "        scale_pos_weight=0.67,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        early_stopping_rounds=100,\n",
        "        eval_metric='logloss',\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=[(Xt_va, y_va)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    xgb_proba_va = xgb_model.predict_proba(Xt_va)[:, 1]\n",
        "    oof_probabilities['xgb'][va_idx] = xgb_proba_va\n",
        "    models['xgb'].append((xgb_model, preprocessor))\n",
        "\n",
        "    print(f\"완료 (Iter: {xgb_model.best_iteration})\")\n",
        "\n",
        "    # Test 예측\n",
        "    if test_available:\n",
        "        test_predictions['xgb'][:, fold-1] = xgb_model.predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "    # ========================================\n",
        "    # CatBoost\n",
        "    # ========================================\n",
        "    print(f\"  [3/3] CatBoost...\", end=' ')\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.03,\n",
        "        depth=7,\n",
        "        l2_leaf_reg=3,\n",
        "        class_weights=[1.5, 1.0],\n",
        "        random_seed=RANDOM_STATE,\n",
        "        verbose=0,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "\n",
        "    cat_model.fit(\n",
        "        Xt_tr, y_tr,\n",
        "        eval_set=(Xt_va, y_va),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    cat_proba_va = cat_model.predict_proba(Xt_va)[:, 1]\n",
        "    oof_probabilities['catboost'][va_idx] = cat_proba_va\n",
        "    models['catboost'].append((cat_model, preprocessor))\n",
        "\n",
        "    print(f\"완료 (Iter: {cat_model.best_iteration_})\")\n",
        "\n",
        "    # Test 예측\n",
        "    if test_available:\n",
        "        test_predictions['catboost'][:, fold-1] = cat_model.predict_proba(Xt_test)[:, 1]\n",
        "\n",
        "print(f\"\\n✓ Layer 1 완료: 3개 Base Models × 5 Folds = 15개 모델 학습\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Layer 1 Base Performance\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Layer 1 Base Models 성능 (Simple Ensemble)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Simple weighted ensemble (기준선)\n",
        "simple_ensemble_proba = (\n",
        "    0.20 * oof_probabilities['lgbm'] +\n",
        "    0.60 * oof_probabilities['xgb'] +\n",
        "    0.20 * oof_probabilities['catboost']\n",
        ")\n",
        "\n",
        "# Adaptive Threshold\n",
        "def get_adaptive_threshold(confidence):\n",
        "    if confidence < 0.05:\n",
        "        return 0.42\n",
        "    elif confidence < 0.10:\n",
        "        return 0.40\n",
        "    else:\n",
        "        return 0.39\n",
        "\n",
        "confidence_simple = np.abs(simple_ensemble_proba - 0.5)\n",
        "adaptive_thresholds_simple = np.array([get_adaptive_threshold(c) for c in confidence_simple])\n",
        "predictions_simple = (simple_ensemble_proba >= adaptive_thresholds_simple).astype(int)\n",
        "\n",
        "# 성능\n",
        "simple_f1 = f1_score(y_train, predictions_simple)\n",
        "simple_auc = roc_auc_score(y_train, simple_ensemble_proba)\n",
        "simple_cm = confusion_matrix(y_train, predictions_simple)\n",
        "tn, fp, fn, tp = simple_cm.ravel()\n",
        "simple_fpr = fp / (fp + tn)\n",
        "\n",
        "print(f\"\\n[Simple Ensemble (Weighted Average)]\")\n",
        "print(f\"  F1 Score:  {simple_f1:.4f}\")\n",
        "print(f\"  AUC Score: {simple_auc:.4f}\")\n",
        "print(f\"  FPR:       {simple_fpr:.4f} ({simple_fpr*100:.2f}%)\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. Layer 2: Stacking (Meta-Learner)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Layer 2: Stacking Meta-Learner 학습\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Meta-features (Layer 1의 OOF 확률)\n",
        "meta_features_train = np.column_stack([\n",
        "    oof_probabilities['lgbm'],\n",
        "    oof_probabilities['xgb'],\n",
        "    oof_probabilities['catboost']\n",
        "])\n",
        "\n",
        "print(f\"\\n[Meta-features]\")\n",
        "print(f\"  Shape: {meta_features_train.shape}\")\n",
        "print(f\"  Feature 1: LGBM 확률\")\n",
        "print(f\"  Feature 2: XGBoost 확률\")\n",
        "print(f\"  Feature 3: CatBoost 확률\")\n",
        "\n",
        "# Meta-Learner 학습 (Logistic Regression)\n",
        "print(f\"\\n[Meta-Learner 학습]\")\n",
        "meta_model = LogisticRegression(\n",
        "    C=0.1,\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "meta_model.fit(meta_features_train, y_train)\n",
        "\n",
        "print(f\"✓ Meta-Learner 학습 완료\")\n",
        "print(f\"\\n[Meta-Learner 가중치]\")\n",
        "print(f\"  LGBM:    {meta_model.coef_[0][0]:.4f}\")\n",
        "print(f\"  XGBoost: {meta_model.coef_[0][1]:.4f}\")\n",
        "print(f\"  CatBoost: {meta_model.coef_[0][2]:.4f}\")\n",
        "print(f\"  Intercept: {meta_model.intercept_[0]:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. Stacking Performance (OOF)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Stacking 성능 평가 (OOF)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Stacking 확률\n",
        "stacking_proba = meta_model.predict_proba(meta_features_train)[:, 1]\n",
        "\n",
        "# Adaptive Threshold 적용\n",
        "confidence_stacking = np.abs(stacking_proba - 0.5)\n",
        "adaptive_thresholds_stacking = np.array([get_adaptive_threshold(c) for c in confidence_stacking])\n",
        "predictions_stacking = (stacking_proba >= adaptive_thresholds_stacking).astype(int)\n",
        "\n",
        "# 성능\n",
        "stacking_f1 = f1_score(y_train, predictions_stacking)\n",
        "stacking_auc = roc_auc_score(y_train, stacking_proba)\n",
        "stacking_cm = confusion_matrix(y_train, predictions_stacking)\n",
        "tn_s, fp_s, fn_s, tp_s = stacking_cm.ravel()\n",
        "stacking_fpr = fp_s / (fp_s + tn_s)\n",
        "stacking_precision = precision_score(y_train, predictions_stacking)\n",
        "stacking_recall = recall_score(y_train, predictions_stacking)\n",
        "\n",
        "print(f\"\\n[Stacking OOF 성능]\")\n",
        "print(f\"  F1 Score:  {stacking_f1:.4f}\")\n",
        "print(f\"  AUC Score: {stacking_auc:.4f}\")\n",
        "print(f\"  Precision: {stacking_precision:.4f}\")\n",
        "print(f\"  Recall:    {stacking_recall:.4f}\")\n",
        "print(f\"  FPR:       {stacking_fpr:.4f} ({stacking_fpr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[OOF 혼동 행렬]\")\n",
        "print(f\"  TN: {tn_s}, FP: {fp_s}, FN: {fn_s}, TP: {tp_s}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. 성능 비교 (Simple vs Stacking)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Simple Ensemble vs Stacking 비교\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "f1_improvement = stacking_f1 - simple_f1\n",
        "auc_improvement = stacking_auc - simple_auc\n",
        "fpr_improvement = stacking_fpr - simple_fpr\n",
        "\n",
        "print(f\"\\n[성능 변화]\")\n",
        "print(f\"{'지표':<15} {'Simple':<12} {'Stacking':<12} {'변화':<15}\")\n",
        "print(f\"{'-'*55}\")\n",
        "print(f\"{'F1 Score':<15} {simple_f1:<12.4f} {stacking_f1:<12.4f} {f1_improvement:+.4f} ({f1_improvement/simple_f1*100:+.2f}%)\")\n",
        "print(f\"{'AUC':<15} {simple_auc:<12.4f} {stacking_auc:<12.4f} {auc_improvement:+.4f}\")\n",
        "print(f\"{'FPR':<15} {simple_fpr*100:<12.2f}% {stacking_fpr*100:<12.2f}% {fpr_improvement*100:+.2f}%p\")\n",
        "\n",
        "# Top 300 대비\n",
        "baseline_f1 = 0.8300\n",
        "baseline_fpr = 0.2467\n",
        "\n",
        "print(f\"\\n[Top 300 대비]\")\n",
        "print(f\"  F1:  {baseline_f1:.4f} → {stacking_f1:.4f} ({(stacking_f1-baseline_f1)*100:+.2f}%p)\")\n",
        "print(f\"  FPR: {baseline_fpr*100:.2f}% → {stacking_fpr*100:.2f}% ({(stacking_fpr-baseline_fpr)*100:+.2f}%p)\")\n",
        "\n",
        "# ============================================================\n",
        "# 7. Test 예측 (Stacking)\n",
        "# ============================================================\n",
        "if test_available:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Test 데이터 예측 (Stacking)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Test Meta-features (각 Fold 평균)\n",
        "    meta_features_test = np.column_stack([\n",
        "        test_predictions['lgbm'].mean(axis=1),\n",
        "        test_predictions['xgb'].mean(axis=1),\n",
        "        test_predictions['catboost'].mean(axis=1)\n",
        "    ])\n",
        "\n",
        "    print(f\"\\n  Test Meta-features shape: {meta_features_test.shape}\")\n",
        "\n",
        "    # Stacking 예측\n",
        "    stacking_proba_test = meta_model.predict_proba(meta_features_test)[:, 1]\n",
        "\n",
        "    # Adaptive Threshold 적용\n",
        "    confidence_test = np.abs(stacking_proba_test - 0.5)\n",
        "    adaptive_thresholds_test = np.array([get_adaptive_threshold(c) for c in confidence_test])\n",
        "    predictions_test = (stacking_proba_test >= adaptive_thresholds_test).astype(int)\n",
        "\n",
        "    print(f\"\\n[Test 예측 결과]\")\n",
        "    print(f\"  예측 Class 0: {sum(predictions_test == 0)}개 ({sum(predictions_test == 0)/len(predictions_test)*100:.2f}%)\")\n",
        "    print(f\"  예측 Class 1: {sum(predictions_test == 1)}개 ({sum(predictions_test == 1)/len(predictions_test)*100:.2f}%)\")\n",
        "    print(f\"  평균 Confidence: {confidence_test.mean():.4f}\")\n",
        "    print(f\"  Low Confidence (<0.1): {sum(confidence_test < 0.1)}개\")\n",
        "\n",
        "    # ============================================================\n",
        "    # 8. Submission 파일 생성\n",
        "    # ============================================================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Submission 파일 생성\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # SMILES 컬럼 확인\n",
        "    if 'SMILES' in df_test.columns:\n",
        "        smiles_col = 'SMILES'\n",
        "    elif 'smiles' in df_test.columns:\n",
        "        smiles_col = 'smiles'\n",
        "    else:\n",
        "        smiles_col = df_test.columns[0]\n",
        "\n",
        "    # 기본 제출 파일\n",
        "    submission = pd.DataFrame({\n",
        "        'SMILES': df_test[smiles_col],\n",
        "        'output': predictions_test\n",
        "    })\n",
        "    submission.to_csv('submission_stacking_final.csv', index=False)\n",
        "    print(f\"\\n✓ 기본 제출 파일: submission_stacking_final.csv\")\n",
        "\n",
        "    # 상세 제출 파일\n",
        "    submission_detailed = pd.DataFrame({\n",
        "        'id': range(len(predictions_test)),\n",
        "        'label': predictions_test,\n",
        "        'probability': stacking_proba_test,\n",
        "        'confidence': confidence_test,\n",
        "        'adaptive_threshold': adaptive_thresholds_test,\n",
        "        'lgbm_proba': test_predictions['lgbm'].mean(axis=1),\n",
        "        'xgb_proba': test_predictions['xgb'].mean(axis=1),\n",
        "        'catboost_proba': test_predictions['catboost'].mean(axis=1)\n",
        "    })\n",
        "    submission_detailed.to_csv('submission_detailed_stacking_final.csv', index=False)\n",
        "    print(f\"✓ 상세 제출 파일: submission_detailed_stacking_final.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. 모델 저장\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"모델 저장\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "final_model_package = {\n",
        "    'layer1_models': models,\n",
        "    'meta_model': meta_model,\n",
        "    'feature_columns': feature_columns,\n",
        "    'oof_f1': stacking_f1,\n",
        "    'oof_auc': stacking_auc,\n",
        "    'oof_fpr': stacking_fpr\n",
        "}\n",
        "\n",
        "with open('final_stacking_model.pkl', 'wb') as f:\n",
        "    pickle.dump(final_model_package, f)\n",
        "\n",
        "print(f\"\\n✓ 모델 저장: final_stacking_model.pkl\")\n",
        "print(f\"  - Layer 1: 15개 Base Models\")\n",
        "print(f\"  - Layer 2: Meta-Learner\")\n",
        "print(f\"  - 전체 피처 리스트\")\n",
        "\n",
        "# ============================================================\n",
        "# 10. 시각화\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"시각화 생성\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# 1. F1 Score Comparison\n",
        "ax = axes[0, 0]\n",
        "models_compare = ['Top 300\\n(Adaptive)', 'Simple\\nEnsemble\\n(3076개)', 'Stacking\\n(3076개)']\n",
        "f1_scores = [baseline_f1, simple_f1, stacking_f1]\n",
        "colors = ['lightblue', 'orange', 'darkgreen']\n",
        "\n",
        "bars = ax.bar(models_compare, f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.set_ylabel('F1 Score', fontsize=11)\n",
        "ax.set_title('F1 Score Comparison', fontsize=12, fontweight='bold')\n",
        "ax.set_ylim([0.825, 0.840])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, f1_scores):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.4f}',\n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 2. FPR Comparison\n",
        "ax = axes[0, 1]\n",
        "fpr_values = [baseline_fpr * 100, simple_fpr * 100, stacking_fpr * 100]\n",
        "bars = ax.bar(models_compare, fpr_values, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.axhline(25, color='r', linestyle='--', alpha=0.5, label='Target: 25%')\n",
        "ax.set_ylabel('FPR (%)', fontsize=11)\n",
        "ax.set_title('FPR Comparison', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, val in zip(bars, fpr_values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.2f}%',\n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 3. Confusion Matrix (Stacking)\n",
        "ax = axes[0, 2]\n",
        "sns.heatmap(stacking_cm, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={'size': 14})\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix (Stacking)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 4. Meta-Learner Weights\n",
        "ax = axes[1, 0]\n",
        "model_names = ['LGBM', 'XGBoost', 'CatBoost']\n",
        "weights = meta_model.coef_[0]\n",
        "colors_weights = ['steelblue', 'orange', 'green']\n",
        "\n",
        "bars = ax.bar(model_names, weights, color=colors_weights, alpha=0.7, edgecolor='black')\n",
        "ax.set_ylabel('Weight', fontsize=11)\n",
        "ax.set_title('Meta-Learner Weights', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, weight in zip(bars, weights):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{weight:.3f}',\n",
        "            ha='center', va='bottom' if height > 0 else 'top', fontweight='bold')\n",
        "\n",
        "# 5. Performance Improvement\n",
        "ax = axes[1, 1]\n",
        "metrics = ['F1\\nScore', 'AUC', 'Precision', 'Recall']\n",
        "simple_vals = [simple_f1, simple_auc,\n",
        "               precision_score(y_train, predictions_simple),\n",
        "               recall_score(y_train, predictions_simple)]\n",
        "stacking_vals = [stacking_f1, stacking_auc, stacking_precision, stacking_recall]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, simple_vals, width, label='Simple', alpha=0.8)\n",
        "ax.bar(x + width/2, stacking_vals, width, label='Stacking', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('All Metrics Comparison', fontsize=12, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 6. Summary\n",
        "ax = axes[1, 2]\n",
        "ax.axis('off')\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "═══════════════════════\n",
        "  Stacking 최종 성능\n",
        "═══════════════════════\n",
        "\n",
        "F1 Score:  {stacking_f1:.4f}\n",
        "AUC:       {stacking_auc:.4f}\n",
        "Precision: {stacking_precision:.4f}\n",
        "Recall:    {stacking_recall:.4f}\n",
        "FPR:       {stacking_fpr*100:.2f}%\n",
        "\n",
        "─────────────────────\n",
        "Top 300 대비\n",
        "─────────────────────\n",
        "F1:   {(stacking_f1-baseline_f1)*100:+.2f}%p\n",
        "FPR:  {(stacking_fpr-baseline_fpr)*100:+.2f}%p\n",
        "\n",
        "─────────────────────\n",
        "Simple 대비\n",
        "─────────────────────\n",
        "F1:   {f1_improvement:+.4f}\n",
        "FPR:  {fpr_improvement*100:+.2f}%p\n",
        "\n",
        "피처: 3076개 (전체)\n",
        "Layer 1: 15개 모델\n",
        "Layer 2: Meta-Learner\n",
        "\"\"\"\n",
        "\n",
        "ax.text(0.5, 0.5, summary_text, ha='center', va='center',\n",
        "        fontsize=10, family='monospace',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('stacking_final_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"\\n✓ 시각화 저장: stacking_final_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 11. 최종 리포트\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"최종 성능 리포트\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[모델 사양]\")\n",
        "print(f\"  Architecture: 2-Layer Stacking\")\n",
        "print(f\"    - Layer 1: LGBM + XGB + CAT\")\n",
        "print(f\"    - Layer 2: Logistic Regression\")\n",
        "print(f\"  피처: 전체 3076개\")\n",
        "print(f\"  Threshold: Adaptive (0.42/0.40/0.39)\")\n",
        "\n",
        "print(f\"\\n[최종 성능]\")\n",
        "print(f\"  F1 Score:  {stacking_f1:.4f}\")\n",
        "print(f\"  AUC Score: {stacking_auc:.4f}\")\n",
        "print(f\"  Precision: {stacking_precision:.4f}\")\n",
        "print(f\"  Recall:    {stacking_recall:.4f}\")\n",
        "print(f\"  FPR:       {stacking_fpr*100:.2f}%\")\n",
        "\n",
        "if stacking_f1 > baseline_f1:\n",
        "    print(f\"\\n✓✓✓ 성공! Top 300 대비 F1 {(stacking_f1-baseline_f1)*100:+.2f}%p 향상\")\n",
        "else:\n",
        "    print(f\"\\n△ Top 300과 유사한 성능\")\n",
        "\n",
        "if test_available:\n",
        "    print(f\"\\n[제출 파일]\")\n",
        "    print(f\"  메인: submission_stacking_final.csv\")\n",
        "    print(f\"  상세: submission_detailed_stacking_final.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✓ Stacking 모델 완성!\")\n",
        "print(f\"{'='*70}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UY5KF0z-bMA",
        "outputId": "7b76136f-b3ae-43c2-c160-c469d98f6d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "최고 성능 모델: 전체 피처 + 2-Layer Stacking\n",
            "======================================================================\n",
            "\n",
            "[전략]\n",
            "  Layer 1: LGBM, XGB, CAT (Base Models)\n",
            "  Layer 2: Logistic Regression (Meta-Learner)\n",
            "  피처: 전체 3076개 (압축 없음)\n",
            "  목표: F1 0.835+\n",
            "\n",
            "======================================================================\n",
            "데이터 로드 (전체 피처)\n",
            "======================================================================\n",
            "\n",
            "[Train 데이터]\n",
            "  Shape: (8349, 3077)\n",
            "  피처: 3077개 (전체)\n",
            "  Label 분포: Class 0 = 3807, Class 1 = 4542\n",
            "\n",
            "[Test 데이터]\n",
            "  Shape: (927, 3077)\n",
            "\n",
            "[피처 구성]\n",
            "  Fingerprint: 3072개\n",
            "  Descriptor: 4개\n",
            "\n",
            "======================================================================\n",
            "Layer 1: Base Models 학습 (5-Fold CV)\n",
            "======================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 1/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 3076), 검증: (1670, 3076)\n",
            "  [1/3] LightGBM... 완료 (Iter: 874)\n",
            "  [2/3] XGBoost... 완료 (Iter: 993)\n",
            "  [3/3] CatBoost... 완료 (Iter: 999)\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 2/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 3076), 검증: (1670, 3076)\n",
            "  [1/3] LightGBM... 완료 (Iter: 760)\n",
            "  [2/3] XGBoost... 완료 (Iter: 731)\n",
            "  [3/3] CatBoost... 완료 (Iter: 956)\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 3/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 3076), 검증: (1670, 3076)\n",
            "  [1/3] LightGBM... 완료 (Iter: 762)\n",
            "  [2/3] XGBoost... 완료 (Iter: 774)\n",
            "  [3/3] CatBoost... 완료 (Iter: 987)\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 4/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6679, 3076), 검증: (1670, 3076)\n",
            "  [1/3] LightGBM... 완료 (Iter: 845)\n",
            "  [2/3] XGBoost... 완료 (Iter: 904)\n",
            "  [3/3] CatBoost... 완료 (Iter: 999)\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📊 Fold 5/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  학습: (6680, 3076), 검증: (1669, 3076)\n",
            "  [1/3] LightGBM... 완료 (Iter: 996)\n",
            "  [2/3] XGBoost... 완료 (Iter: 993)\n",
            "  [3/3] CatBoost... 완료 (Iter: 997)\n",
            "\n",
            "✓ Layer 1 완료: 3개 Base Models × 5 Folds = 15개 모델 학습\n",
            "\n",
            "======================================================================\n",
            "Layer 1 Base Models 성능 (Simple Ensemble)\n",
            "======================================================================\n",
            "\n",
            "[Simple Ensemble (Weighted Average)]\n",
            "  F1 Score:  0.8225\n",
            "  AUC Score: 0.8862\n",
            "  FPR:       0.2590 (25.90%)\n",
            "\n",
            "======================================================================\n",
            "Layer 2: Stacking Meta-Learner 학습\n",
            "======================================================================\n",
            "\n",
            "[Meta-features]\n",
            "  Shape: (8349, 3)\n",
            "  Feature 1: LGBM 확률\n",
            "  Feature 2: XGBoost 확률\n",
            "  Feature 3: CatBoost 확률\n",
            "\n",
            "[Meta-Learner 학습]\n",
            "✓ Meta-Learner 학습 완료\n",
            "\n",
            "[Meta-Learner 가중치]\n",
            "  LGBM:    1.6204\n",
            "  XGBoost: 1.9808\n",
            "  CatBoost: 1.7538\n",
            "  Intercept: -2.6061\n",
            "\n",
            "======================================================================\n",
            "Stacking 성능 평가 (OOF)\n",
            "======================================================================\n",
            "\n",
            "[Stacking OOF 성능]\n",
            "  F1 Score:  0.8239\n",
            "  AUC Score: 0.8860\n",
            "  Precision: 0.8031\n",
            "  Recall:    0.8459\n",
            "  FPR:       0.2474 (24.74%)\n",
            "\n",
            "[OOF 혼동 행렬]\n",
            "  TN: 2865, FP: 942, FN: 700, TP: 3842\n",
            "\n",
            "======================================================================\n",
            "Simple Ensemble vs Stacking 비교\n",
            "======================================================================\n",
            "\n",
            "[성능 변화]\n",
            "지표              Simple       Stacking     변화             \n",
            "-------------------------------------------------------\n",
            "F1 Score        0.8225       0.8239       +0.0015 (+0.18%)\n",
            "AUC             0.8862       0.8860       -0.0002\n",
            "FPR             25.90       % 24.74       % -1.16%p\n",
            "\n",
            "[Top 300 대비]\n",
            "  F1:  0.8300 → 0.8239 (-0.61%p)\n",
            "  FPR: 24.67% → 24.74% (+0.07%p)\n",
            "\n",
            "======================================================================\n",
            "Test 데이터 예측 (Stacking)\n",
            "======================================================================\n",
            "\n",
            "  Test Meta-features shape: (927, 3)\n",
            "\n",
            "[Test 예측 결과]\n",
            "  예측 Class 0: 392개 (42.29%)\n",
            "  예측 Class 1: 535개 (57.71%)\n",
            "  평균 Confidence: 0.3038\n",
            "  Low Confidence (<0.1): 94개\n",
            "\n",
            "======================================================================\n",
            "Submission 파일 생성\n",
            "======================================================================\n",
            "\n",
            "✓ 기본 제출 파일: submission_stacking_final.csv\n",
            "✓ 상세 제출 파일: submission_detailed_stacking_final.csv\n",
            "\n",
            "======================================================================\n",
            "모델 저장\n",
            "======================================================================\n",
            "\n",
            "✓ 모델 저장: final_stacking_model.pkl\n",
            "  - Layer 1: 15개 Base Models\n",
            "  - Layer 2: Meta-Learner\n",
            "  - 전체 피처 리스트\n",
            "\n",
            "======================================================================\n",
            "시각화 생성\n",
            "======================================================================\n",
            "\n",
            "✓ 시각화 저장: stacking_final_analysis.png\n",
            "\n",
            "======================================================================\n",
            "최종 성능 리포트\n",
            "======================================================================\n",
            "\n",
            "[모델 사양]\n",
            "  Architecture: 2-Layer Stacking\n",
            "    - Layer 1: LGBM + XGB + CAT\n",
            "    - Layer 2: Logistic Regression\n",
            "  피처: 전체 3076개\n",
            "  Threshold: Adaptive (0.42/0.40/0.39)\n",
            "\n",
            "[최종 성능]\n",
            "  F1 Score:  0.8239\n",
            "  AUC Score: 0.8860\n",
            "  Precision: 0.8031\n",
            "  Recall:    0.8459\n",
            "  FPR:       24.74%\n",
            "\n",
            "△ Top 300과 유사한 성능\n",
            "\n",
            "[제출 파일]\n",
            "  메인: submission_stacking_final.csv\n",
            "  상세: submission_detailed_stacking_final.csv\n",
            "\n",
            "======================================================================\n",
            "✓ Stacking 모델 완성!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# RDKit Descriptor 추가 피처 생성\n",
        "# ============================================================\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RDKit Descriptor 피처 생성\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def calculate_rdkit_descriptors(smiles):\n",
        "    \"\"\"\n",
        "    SMILES에서 RDKit Descriptor 계산\n",
        "\n",
        "    독성 예측 특화 Descriptors:\n",
        "    - Lipinski descriptors (약물성)\n",
        "    - 구조적 특징 (고리, 결합)\n",
        "    - 전자적 특성\n",
        "    \"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        descriptors = {\n",
        "            # Lipinski Descriptors (약물성)\n",
        "            'NumHDonors': Descriptors.NumHDonors(mol),\n",
        "            'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
        "            'MolLogP': Descriptors.MolLogP(mol),\n",
        "            'TPSA': Descriptors.TPSA(mol),\n",
        "\n",
        "            # 구조적 특징\n",
        "            'NumAromaticRings': Descriptors.NumAromaticRings(mol),\n",
        "            'NumAliphaticRings': Descriptors.NumAliphaticRings(mol),\n",
        "            'NumSaturatedRings': Descriptors.NumSaturatedRings(mol),\n",
        "            'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'NumHeteroatoms': Descriptors.NumHeteroatoms(mol),\n",
        "\n",
        "            # 전자적 특성\n",
        "            'FractionCsp3': Descriptors.FractionCsp3(mol),\n",
        "            'Chi0v': Descriptors.Chi0v(mol),\n",
        "            'HallKierAlpha': Descriptors.HallKierAlpha(mol),\n",
        "\n",
        "            # 표면적 및 부피\n",
        "            'LabuteASA': Descriptors.LabuteASA(mol),\n",
        "            'PEOE_VSA1': Descriptors.PEOE_VSA1(mol),\n",
        "\n",
        "            # 추가 (선택)\n",
        "            'BertzCT': Descriptors.BertzCT(mol),  # 복잡도\n",
        "            'Ipc': Descriptors.Ipc(mol),          # 정보 함량\n",
        "            'RingCount': Descriptors.RingCount(mol),\n",
        "            'NumBridgeheadAtoms': Descriptors.NumBridgeheadAtoms(mol),\n",
        "            'NumSpiroAtoms': Descriptors.NumSpiroAtoms(mol)\n",
        "        }\n",
        "\n",
        "        return descriptors\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {smiles}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ============================================================\n",
        "# 1. Train 데이터에 RDKit Descriptor 추가\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Train 데이터 처리\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Train 데이터 로드\n",
        "df_train = pd.read_csv('train.csv')\n",
        "\n",
        "# SMILES 컬럼 확인 (없으면 별도 파일에서 로드 필요)\n",
        "if 'SMILES' in df_train.columns:\n",
        "    train_smiles = df_train['SMILES']\n",
        "elif 'smiles' in df_train.columns:\n",
        "    train_smiles = df_train['smiles']\n",
        "else:\n",
        "    print(\"⚠️  Train 데이터에 SMILES 없음\")\n",
        "    print(\"   해결책:\")\n",
        "    print(\"   1. Train SMILES 파일 별도 제공 필요\")\n",
        "    print(\"   2. 또는 기존 SMILES로부터 재생성\")\n",
        "    train_smiles = None\n",
        "\n",
        "if train_smiles is not None:\n",
        "    print(f\"\\n  SMILES 발견: {len(train_smiles)}개\")\n",
        "\n",
        "    # RDKit Descriptor 계산\n",
        "    print(f\"  RDKit Descriptor 계산 중...\")\n",
        "    rdkit_descriptors_list = []\n",
        "\n",
        "    for idx, smiles in enumerate(train_smiles):\n",
        "        if idx % 1000 == 0:\n",
        "            print(f\"\\r    진행: {idx}/{len(train_smiles)}\", end='')\n",
        "\n",
        "        desc = calculate_rdkit_descriptors(smiles)\n",
        "        rdkit_descriptors_list.append(desc)\n",
        "\n",
        "    print(f\"\\r    ✓ 완료: {len(train_smiles)}개\")\n",
        "\n",
        "    # DataFrame 변환\n",
        "    rdkit_df = pd.DataFrame(rdkit_descriptors_list)\n",
        "\n",
        "    print(f\"\\n  생성된 RDKit Descriptor: {len(rdkit_df.columns)}개\")\n",
        "    print(f\"  컬럼: {list(rdkit_df.columns)}\")\n",
        "\n",
        "    # 결측치 확인\n",
        "    missing_count = rdkit_df.isnull().sum().sum()\n",
        "    if missing_count > 0:\n",
        "        print(f\"\\n  ⚠️  결측치: {missing_count}개\")\n",
        "        print(f\"     결측치 처리: median imputation\")\n",
        "        rdkit_df = rdkit_df.fillna(rdkit_df.median())\n",
        "\n",
        "    # 기존 데이터에 추가\n",
        "    df_train_enhanced = pd.concat([df_train, rdkit_df], axis=1)\n",
        "\n",
        "    print(f\"\\n  최종 Train 데이터: {df_train_enhanced.shape}\")\n",
        "    print(f\"    기존: {df_train.shape}\")\n",
        "    print(f\"    추가: {rdkit_df.shape[1]}개 컬럼\")\n",
        "\n",
        "    # 저장\n",
        "    df_train_enhanced.to_csv('train_with_rdkit.csv', index=False)\n",
        "    print(f\"\\n✓ 저장: train_with_rdkit.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Test 데이터에 RDKit Descriptor 추가\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Test 데이터 처리\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "df_test = pd.read_csv('predict_input.csv')\n",
        "\n",
        "# SMILES 컬럼 확인\n",
        "if 'SMILES' in df_test.columns:\n",
        "    test_smiles = df_test['SMILES']\n",
        "elif 'smiles' in df_test.columns:\n",
        "    test_smiles = df_test['smiles']\n",
        "else:\n",
        "    test_smiles = df_test[df_test.columns[0]]  # 첫 번째 컬럼 시도\n",
        "\n",
        "print(f\"\\n  SMILES: {len(test_smiles)}개\")\n",
        "\n",
        "# RDKit Descriptor 계산\n",
        "print(f\"  RDKit Descriptor 계산 중...\")\n",
        "rdkit_descriptors_test_list = []\n",
        "\n",
        "for idx, smiles in enumerate(test_smiles):\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"\\r    진행: {idx}/{len(test_smiles)}\", end='')\n",
        "\n",
        "    desc = calculate_rdkit_descriptors(smiles)\n",
        "    rdkit_descriptors_test_list.append(desc)\n",
        "\n",
        "print(f\"\\r    ✓ 완료: {len(test_smiles)}개\")\n",
        "\n",
        "# DataFrame 변환\n",
        "rdkit_test_df = pd.DataFrame(rdkit_descriptors_test_list)\n",
        "\n",
        "# 결측치 처리\n",
        "if rdkit_test_df.isnull().sum().sum() > 0:\n",
        "    rdkit_test_df = rdkit_test_df.fillna(rdkit_test_df.median())\n",
        "\n",
        "# 기존 데이터에 추가\n",
        "df_test_enhanced = pd.concat([df_test, rdkit_test_df], axis=1)\n",
        "\n",
        "print(f\"\\n  최종 Test 데이터: {df_test_enhanced.shape}\")\n",
        "\n",
        "# 저장\n",
        "df_test_enhanced.to_csv('predict_input_with_rdkit.csv', index=False)\n",
        "print(f\"\\n✓ 저장: predict_input_with_rdkit.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. 상관관계 분석 (중복 확인)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"기존 Descriptor와 상관관계 분석\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if train_smiles is not None:\n",
        "    existing_desc = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
        "\n",
        "    print(f\"\\n[상관관계 매트릭스]\")\n",
        "\n",
        "    # 기존 + 새로운 Descriptor\n",
        "    all_descriptors = existing_desc + list(rdkit_df.columns)\n",
        "    correlation_df = df_train_enhanced[all_descriptors].corr()\n",
        "\n",
        "    # 높은 상관관계 찾기 (|r| > 0.8)\n",
        "    high_corr_pairs = []\n",
        "\n",
        "    for i, col1 in enumerate(all_descriptors):\n",
        "        for col2 in all_descriptors[i+1:]:\n",
        "            corr = correlation_df.loc[col1, col2]\n",
        "            if abs(corr) > 0.8:\n",
        "                high_corr_pairs.append((col1, col2, corr))\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(f\"\\n  ⚠️  높은 상관관계 발견 (|r| > 0.8):\")\n",
        "        for col1, col2, corr in high_corr_pairs[:10]:  # 상위 10개\n",
        "            print(f\"    {col1} ↔ {col2}: {corr:.3f}\")\n",
        "        print(f\"\\n  총 {len(high_corr_pairs)}쌍\")\n",
        "        print(f\"  권장: 한 쪽 제거하여 다중공선성 방지\")\n",
        "    else:\n",
        "        print(f\"\\n  ✓ 높은 상관관계 없음 - 모두 독립적\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. 예상 성능 개선\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"예상 효과\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[RDKit Descriptor 추가 효과]\")\n",
        "print(f\"  기존 피처: 3076개 (Fingerprint 3072 + Descriptor 4)\")\n",
        "print(f\"  추가 피처: {len(rdkit_df.columns)}개 (RDKit Descriptors)\")\n",
        "print(f\"  최종 피처: {3076 + len(rdkit_df.columns)}개\")\n",
        "\n",
        "print(f\"\\n[예상 성능 향상]\")\n",
        "print(f\"  현재 F1: 0.8300~0.8306\")\n",
        "print(f\"  예상 F1: 0.8350~0.8400 (+0.5~1.0%)\")\n",
        "print(f\"  근거:\")\n",
        "print(f\"    - Domain knowledge 활용\")\n",
        "print(f\"    - 화학적 의미 있는 피처\")\n",
        "print(f\"    - 독성과 직접 관련\")\n",
        "\n",
        "print(f\"\\n✓ RDKit Descriptor 생성 완료\")\n",
        "print(f\"  다음: train_with_rdkit.csv로 모델 재학습\")\n"
      ],
      "metadata": {
        "id": "8ICEa6HZDqJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}