{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 화합물 독성 예측 모델 발표자료 (실제 결과 반영)\n",
        "\n",
        "***\n",
        "\n",
        "## 🎯 프로젝트 개요\n",
        "\n",
        "### 목표\n",
        "- **분자 독성 예측 모델** 개발 (SMILES 기반)\n",
        "- **F1 Score 0.832+** 달성\n",
        "- **높은 Recall** 유지 (88.35%)\n",
        "\n",
        "### 실제 달성 결과\n",
        "```\n",
        "✅ F1 Score: 0.8321 (목표 0.8308 대비 +0.13%p)\n",
        "✅ AUC Score: 0.8953\n",
        "✅ Recall: 0.8835 (민감도 우선)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## 📌 전체 파이프라인\n",
        "\n",
        "```\n",
        "원본 데이터 (8,349개)\n",
        "    ↓\n",
        "Top 300 피처 + RDKit 48개\n",
        "    ↓\n",
        "5-Fold Cross-Validation\n",
        "    ↓\n",
        "3개 Base Models (LGBM, XGB, CatBoost)\n",
        "    ↓\n",
        "Meta-Learner (LightGBM) + 13개 Meta-features\n",
        "    ↓\n",
        "적응형 임계값 (Confidence 기반)\n",
        "    ↓\n",
        "최종 예측 (927개 Test)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## 📊 데이터 구성\n",
        "\n",
        "### Train/Test 분할\n",
        "\n",
        "| 구분 | 샘플 수 | Class 0 (무독성) | Class 1 (독성) | 비율 |\n",
        "|------|---------|------------------|----------------|------|\n",
        "| **Train** | 8,349 | 3,807 (45.6%) | 4,542 (54.4%) | 1.19:1 |\n",
        "| **Test** | 927 | ? | ? | - |\n",
        "\n",
        "**특징**: 약간의 클래스 불균형 (독성 샘플이 9% 많음)\n",
        "\n",
        "***\n",
        "\n",
        "## 🔬 Step 1: 피처 엔지니어링\n",
        "\n",
        "### 1.1 Top 300 기본 피처\n",
        "\n",
        "```python\n",
        "# 사전 선별된 중요 피처 300개\n",
        "├─ Fingerprint (296개): ECFP, FCFP, PTFP 등\n",
        "└─ Descriptor (4개): MolWt, clogp, sa_score, qed\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### 1.2 RDKit Descriptor 48개 추가\n",
        "\n",
        "#### 생성 결과\n",
        "- **총 컬럼**: 48개 (목표 50개 중 2개 누락)\n",
        "- **성공률**: 97.9% (47개 완벽 생성, 1개 완전 실패)\n",
        "- **처리 시간**: Train 50.30초, Test 4.97초\n",
        "\n",
        "#### 실패 컬럼 분석\n",
        "```python\n",
        "완전 실패 (100% 결측):\n",
        "  - rdkit_NumHeteroatoms  # Lipinski 규칙의 헤테로원자 개수\n",
        "  → Median imputation으로 대체\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### 1.3 RDKit Descriptor 통계 (상위 10개)\n",
        "\n",
        "| Descriptor | Mean | Std | Min | Max | 해석 |\n",
        "|-----------|------|-----|-----|-----|------|\n",
        "| **NumHDonors** | 1.31 | 1.10 | 0 | 18 | 수소결합 공여체 (평균 1.3개) |\n",
        "| **NumHAcceptors** | 5.64 | 2.17 | 0 | 17 | 수소결합 수용체 (평균 5.6개) |\n",
        "| **NumRotatableBonds** | 5.74 | 2.40 | 0 | 32 | 회전 가능 결합 (유연성) |\n",
        "| **RingCount** | 4.29 | 1.20 | 0 | 9 | 고리 개수 (평균 4.3개) |\n",
        "| **TPSA** | 78.31 | 32.28 | 0 | 496.68 | 극성 표면적 (세포막 투과성) |\n",
        "| **LabuteASA** | 185.55 | 35.92 | 41.99 | 513.21 | 표면적 |\n",
        "| **NumAromaticRings** | 2.75 | 1.02 | 0 | 7 | 방향족 고리 (안정성) |\n",
        "\n",
        "**독성과의 관계**\n",
        "- 높은 TPSA → 세포막 투과 어려움 → 독성 낮음\n",
        "- 많은 방향족 고리 → 구조 안정 → 대사 느림 → 독성 축적 가능\n",
        "\n",
        "***\n",
        "\n",
        "### 1.4 최종 피처 구성\n",
        "\n",
        "```python\n",
        "총 348개 피처 = Top 300 + RDKit 48\n",
        "\n",
        "[피처 타입별 분류]\n",
        "  ├─ Fingerprint: 296개 (85.1%)\n",
        "  ├─ Descriptor (원본): 4개 (1.1%)\n",
        "  └─ RDKit Descriptor: 48개 (13.8%)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## 🤖 Step 2: Layer 1 - Base Models (5-Fold CV)\n",
        "\n",
        "### 모델 구성\n",
        "\n",
        "| 모델 | n_estimators | learning_rate | max_depth | 주요 특징 |\n",
        "|------|--------------|---------------|-----------|----------|\n",
        "| **LightGBM** | 1000 | 0.03 | 8 | Leaf-wise, class_weight={0:1.5, 1:1.0} |\n",
        "| **XGBoost** | 1000 | 0.03 | 7 | Level-wise, scale_pos_weight=0.67 |\n",
        "| **CatBoost** | 1000 | 0.03 | 7 | Ordered boosting, class_weights=[1.5, 1.0] |\n",
        "\n",
        "***\n",
        "\n",
        "### Fold별 성능 (Valid F1)\n",
        "\n",
        "| Fold | LGBM | XGB | CatBoost | Early Stop (LGBM) | 소요시간 |\n",
        "|------|------|-----|----------|-------------------|----------|\n",
        "| **1** | 0.8323 | 0.8313 | 0.8225 | 727회 | 67.01초 |\n",
        "| **2** | 0.8020 | 0.8061 | 0.7982 | 474회 | 58.63초 |\n",
        "| **3** | 0.7937 | 0.7908 | 0.7864 | 434회 | 56.40초 |\n",
        "| **4** | 0.8117 | 0.8144 | 0.8184 | 510회 | 60.38초 |\n",
        "| **5** | 0.8231 | 0.8327 | 0.8324 | 582회 | 62.29초 |\n",
        "| **평균** | **0.8126** | **0.8151** | **0.8116** | 545회 | 60.94초 |\n",
        "\n",
        "**관찰 포인트**\n",
        "- Fold 1: 가장 높은 성능 (0.83+)\n",
        "- Fold 2-3: 성능 하락 (0.79~0.80) → 데이터 분포 차이\n",
        "- Fold 4-5: 회복 (0.81~0.83)\n",
        "- **XGBoost가 가장 안정적** (평균 0.8151)\n",
        "\n",
        "***\n",
        "\n",
        "### OOF 확률 분포\n",
        "\n",
        "```python\n",
        "lgbm    : Mean=0.5130, Std=0.3480, Min=0.0013, Max=0.9996\n",
        "xgb     : Mean=0.5145, Std=0.3506, Min=0.0013, Max=0.9993\n",
        "catboost: Mean=0.5084, Std=0.3301, Min=0.0032, Max=0.9994\n",
        "```\n",
        "\n",
        "**해석**\n",
        "- 평균 0.51: 독성/무독성 비율(54:46)과 유사\n",
        "- Std 0.33 ~ 0.35: 충분한 변별력 (0 ~ 1 전체 활용)\n",
        "- Min/Max: 극단값 존재 (확신 있는 예측)\n",
        "\n",
        "***\n",
        "\n",
        "## 🧠 Step 3: Layer 2 - Meta-Learner\n",
        "\n",
        "### Meta-Features 13개 구성\n",
        "\n",
        "```python\n",
        "1. 기본 확률 (3개)\n",
        "   - P_lgbm, P_xgb, P_catboost\n",
        "\n",
        "2. 상호작용 (3개)\n",
        "   - P_lgbm × P_xgb         # 두 모델 합의도\n",
        "   - P_xgb × P_catboost\n",
        "   - P_lgbm × P_catboost\n",
        "\n",
        "3. 불일치도 (3개)\n",
        "   - |P_lgbm - P_xgb|       # 예측 차이 (불확실성)\n",
        "   - |P_xgb - P_catboost|\n",
        "   - |P_lgbm - P_catboost|\n",
        "\n",
        "4. 앙상블 통계 (4개)\n",
        "   - max(3 models)          # 가장 높은 확신\n",
        "   - min(3 models)          # 가장 낮은 확신\n",
        "   - mean(3 models)         # 평균 합의\n",
        "   - std(3 models)          # 의견 분산도\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### Meta-Learner 설정\n",
        "\n",
        "```python\n",
        "LGBMClassifier(\n",
        "    n_estimators=100,      # 빠른 학습\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,           # 얕은 트리 (과적합 방지)\n",
        "    num_leaves=7,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "학습 시간: 0.08초 (매우 빠름)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## 🎯 Step 4: 적응형 임계값 전략\n",
        "\n",
        "### Confidence 기반 동적 조정\n",
        "\n",
        "```python\n",
        "confidence = |P - 0.5|  # 0.5에서 멀수록 확신\n",
        "\n",
        "if confidence < 0.05:      # 매우 불확실 (P ≈ 0.45~0.55)\n",
        "    threshold = 0.42       # 보수적\n",
        "elif confidence < 0.10:    # 중간 불확실\n",
        "    threshold = 0.40\n",
        "else:                      # 확실함 (P < 0.4 or P > 0.6)\n",
        "    threshold = 0.39       # 공격적\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### 실제 적용 결과 (Train 8,349개)\n",
        "\n",
        "| Threshold | 샘플 수 | 비율 | 전략 |\n",
        "|-----------|---------|------|------|\n",
        "| **0.42** | 713개 | 8.54% | FPR 최소화 (불확실 샘플) |\n",
        "| **0.40** | 387개 | 4.64% | 균형 |\n",
        "| **0.39** | 7,249개 | 86.82% | Recall 향상 (확실 샘플) |\n",
        "\n",
        "**핵심**\n",
        "- 86.8%는 확실하게 예측 (낮은 임계값)\n",
        "- 13.2%는 신중하게 예측 (높은 임계값)\n",
        "\n",
        "***\n",
        "\n",
        "## 📊 Step 5: 최종 성능 평가 (OOF)\n",
        "\n",
        "### 핵심 지표\n",
        "\n",
        "```\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "  F1 Score:   0.8321  ✅ (목표 0.835 근접)\n",
        "  AUC Score:  0.8953  ✅ (우수)\n",
        "  Precision:  0.7864  \n",
        "  Recall:     0.8835  ✅ (높은 민감도)\n",
        "  FPR:       28.63%   ⚠️ (목표 대비 높음)\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### 혼동 행렬 분석\n",
        "\n",
        "```\n",
        "              실제\n",
        "          무독성   독성\n",
        "예측  무독성  2,717   1,090  ← FP (Type I Error)\n",
        "      독성     529   4,013  ← TP (정확히 독성 예측)\n",
        "```\n",
        "\n",
        "| 지표 | 값 | 설명 |\n",
        "|------|-----|------|\n",
        "| **TN** | 2,717 | 정확히 무독성 예측 (71.3%) |\n",
        "| **FP** | 1,090 | 독성인데 무독성으로 오판 (28.6%) |\n",
        "| **FN** | 529 | 무독성인데 독성으로 오판 (11.6%) |\n",
        "| **TP** | 4,013 | 정확히 독성 예측 (88.4%) |\n",
        "\n",
        "**핵심 문제**\n",
        "- **FPR 28.63%**: 무독성 3,807개 중 1,090개를 독성으로 오판\n",
        "- 목표 3~5%와 큰 차이 → **적응형 임계값이 예상대로 작동 안함**\n",
        "\n",
        "***\n",
        "\n",
        "### 성능 비교\n",
        "\n",
        "| 모델 | F1 Score | 개선폭 | 주요 변화 |\n",
        "|------|----------|--------|----------|\n",
        "| 독립 RDKit 9개 (이전 최고) | 0.8308 | - | - |\n",
        "| **Top 300 + RDKit 48 + Stacking** | **0.8321** | **+0.13%p** | ✅ 신기록 |\n",
        "\n",
        "***\n",
        "\n",
        "### 왜 FPR이 높은가?\n",
        "\n",
        "#### 1. 적응형 임계값 문제\n",
        "```python\n",
        "# 실제 적용\n",
        "Threshold 0.39: 86.82% (대부분 낮은 임계값)\n",
        "→ Recall 우선 전략으로 작동\n",
        "→ FP 증가 (무독성을 독성으로 오판)\n",
        "```\n",
        "\n",
        "#### 2. 클래스 가중치\n",
        "```python\n",
        "class_weight = {0: 1.5, 1: 1.0}\n",
        "→ Class 0 (무독성)에 1.5배 패널티\n",
        "→ 무독성 예측을 신중하게 (독성 쪽으로 편향)\n",
        "```\n",
        "\n",
        "#### 3. Stacking의 Recall 편향\n",
        "```python\n",
        "Meta-features에 min, mean 포함\n",
        "→ 가장 낮은 확률도 고려\n",
        "→ 보수적 판단 (독성으로 분류 경향)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## 📈 Test 예측 결과\n",
        "\n",
        "### 예측 분포 (927개)\n",
        "\n",
        "```\n",
        "Class 0 (독성): 366개 (39.48%)\n",
        "Class 1 (무독성): 561개 (60.52%)\n",
        "```\n",
        "\n",
        "**Train 비율과 비교**\n",
        "- Train: 45.6% vs 54.4%\n",
        "- Test: 39.5% vs 60.5%\n",
        "- → Test에서 독성 비율이 약간 높게 예측됨\n",
        "\n",
        "---\n",
        "\n",
        "### 확률 및 Confidence 분석\n",
        "\n",
        "```python\n",
        "평균 확률: 0.5144\n",
        "평균 Confidence: 0.2931\n",
        "Low Confidence (<0.1): 123개 (13.3%)\n",
        "```\n",
        "\n",
        "**해석**\n",
        "- 평균 확률 0.51: Train과 유사 (일관성 ✓)\n",
        "- Low Confidence 13.3%: Train(13.2%)과 거의 동일\n",
        "- → **모델이 안정적으로 작동** 중\n",
        "\n",
        "***\n",
        "\n",
        "## 🔍 심층 분석\n",
        "\n",
        "### Fold별 성능 변동 원인\n",
        "\n",
        "#### Fold 1 (F1 0.83)\n",
        "- 가장 높은 성능\n",
        "- 데이터 분포가 전체와 유사\n",
        "\n",
        "#### Fold 2-3 (F1 0.79~0.80)\n",
        "- 성능 하락\n",
        "- **가능 원인**: Validation set에 어려운 샘플 집중\n",
        "- Early Stop이 빠름 (434~474회)\n",
        "\n",
        "#### Fold 4-5 (F1 0.81~0.83)\n",
        "- 회복\n",
        "- 균형잡힌 분포\n",
        "\n",
        "**교훈**: 5-Fold CV로 데이터 변동성 포착 성공\n",
        "\n",
        "***\n",
        "\n",
        "### RDKit 효과 분석\n",
        "\n",
        "| 단계 | F1 | 피처 수 | 개선 메커니즘 |\n",
        "|------|-----|---------|--------------|\n",
        "| Top 300만 | 0.8200 | 300 | 기본 지문 |\n",
        "| + RDKit 9개 | 0.8308 | 309 | +1.08%p (화학 특성 추가) |\n",
        "| + RDKit 48개 | 0.8321 | 348 | +0.13%p (심화 특성) |\n",
        "\n",
        "**한계점**\n",
        "- RDKit 9 → 48개로 확장했지만 성능 개선 미미 (+0.13%p)\n",
        "- **가능 원인**:\n",
        "  - 추가 descriptor들이 상관관계 높음 (정보 중복)\n",
        "  - 모델이 이미 포화 상태\n",
        "  - 1개 컬럼 누락 (NumHeteroatoms)\n",
        "\n",
        "***\n",
        "\n",
        "## 💡 핵심 성공 요인\n",
        "\n",
        "### 1. 안정적인 5-Fold CV\n",
        "```\n",
        "15개 독립 모델 (3 × 5)\n",
        "→ 데이터 변동성 흡수\n",
        "→ 평균 F1 0.81~0.82 유지\n",
        "```\n",
        "\n",
        "### 2. 다양한 알고리즘\n",
        "- LightGBM: 빠름, Leaf-wise\n",
        "- XGBoost: 안정적, Level-wise  \n",
        "- CatBoost: 범주형 처리 우수\n",
        "\n",
        "### 3. Meta-Learner의 지능형 결합\n",
        "- 13개 Meta-features로 모델 간 패턴 학습\n",
        "- 불일치도 포착으로 불확실성 관리\n",
        "\n",
        "### 4. 화학 도메인 지식\n",
        "- RDKit 48개로 독성 메커니즘 반영\n",
        "- TPSA, 방향족 고리 등 전문 지식 수치화\n",
        "\n",
        "***\n",
        "\n",
        "## ⚠️ 개선 필요 사항\n",
        "\n",
        "### 1. FPR 문제 해결\n",
        "\n",
        "**현재**: 28.63% (목표: 3~5%)\n",
        "\n",
        "**해결 방안**\n",
        "```python\n",
        "# 1. 임계값 재조정\n",
        "if confidence < 0.05:\n",
        "    threshold = 0.55  # 0.42 → 0.55 (더 보수적)\n",
        "\n",
        "# 2. Class Weight 조정\n",
        "class_weight = {0: 2.0, 1: 1.0}  # 1.5 → 2.0\n",
        "\n",
        "# 3. Precision 중심 Metric\n",
        "- F1 대신 F-beta (β=0.5) 사용\n",
        "- Precision 가중치 증가\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### 2. RDKit Descriptor 최적화\n",
        "\n",
        "**문제**: 48개 중 실제 기여도 낮은 descriptor 존재\n",
        "\n",
        "**해결 방안**\n",
        "```python\n",
        "# Feature Importance 분석\n",
        "importance = meta_model.feature_importances_\n",
        "top_rdkit = select_top_k(rdkit_features, k=20)\n",
        "\n",
        "# 상관관계 높은 피처 제거\n",
        "correlation_matrix = rdkit_df.corr()\n",
        "remove_high_corr(threshold=0.95)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### 3. Fold 간 성능 편차 감소\n",
        "\n",
        "**문제**: Fold 2-3에서 F1 0.79로 하락\n",
        "\n",
        "**해결 방안**\n",
        "```python\n",
        "# Stratified Group K-Fold\n",
        "- SMILES 유사도로 그룹핑\n",
        "- 유사한 분자가 같은 Fold에 포함\n",
        "\n",
        "# 10-Fold로 증가\n",
        "- 더 많은 학습 데이터\n",
        "- Validation 안정성 향상\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## 🎯 결론\n",
        "\n",
        "### 달성 목표 ✅\n",
        "\n",
        "```\n",
        "✅ F1 Score 0.8321 (이전 최고 0.8308 초과)\n",
        "✅ AUC Score 0.8953 (우수한 분류 성능)\n",
        "✅ Recall 0.8835 (민감도 우선 전략 성공)\n",
        "⚠️ FPR 28.63% (목표 미달, 개선 필요)\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "### 핵심 메시지\n",
        "\n",
        "> **\"화학 도메인 지식 + 5-Fold 앙상블 + Meta-Learning\"**  \n",
        "> = **안정적이고 재현 가능한 독성 예측 모델**\n",
        "\n",
        "**강점**\n",
        "- 높은 Recall (88.4%): 독성 물질 잘 포착\n",
        "- 일관성: Train/Test 확률 분포 유사\n",
        "- 해석 가능: RDKit descriptor로 화학적 근거 제공\n",
        "\n",
        "**약점**\n",
        "- 높은 FPR: 무독성을 독성으로 오판 (28.6%)\n",
        "- 개선 여지: 임계값 최적화 필요\n",
        "\n",
        "***\n",
        "\n",
        "### 활용 방안\n",
        "\n",
        "#### 1. 신약 개발 초기 스크리닝\n",
        "```\n",
        "Recall 88.4% → 독성 물질 놓칠 확률 11.6%\n",
        "→ 안전성 우선 전략에 적합\n",
        "```\n",
        "\n",
        "#### 2. 화학물질 안전성 평가\n",
        "```\n",
        "FPR 28.6% → 추가 실험 필요\n",
        "→ 1차 필터링 도구로 활용\n",
        "```\n",
        "\n",
        "#### 3. 규제 기관 참고 자료\n",
        "```\n",
        "AUC 0.8953 → 높은 변별력\n",
        "→ 의사결정 보조 시스템\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## 📁 최종 제출\n",
        "\n",
        "```bash\n",
        "submission_ultimate.csv\n",
        "├─ SMILES: 927개 분자 구조\n",
        "└─ output: 독성 예측 (0=무독성 366개, 1=독성 561개)\n",
        "\n",
        "예상 성능: F1 0.83+ (리더보드 확인 필요)\n",
        "```\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "_m7-GwVAW6zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 최종 최적화 모델: Top 300 + RDKit 50개 + 고급 Stacking\n",
        "# (상세 출력 버전)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski, Crippen, GraphDescriptors, MolSurf\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"최종 최적화 모델: Top 300 + RDKit 50개 + 고급 Stacking\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n[전략]\")\n",
        "print(\"  1. RDKit 50개 확장 (독성 예측 특화)\")\n",
        "print(\"  2. LightGBM Meta-Learner (LogReg 대체)\")\n",
        "print(\"  3. Meta-features 상호작용 추가\")\n",
        "print(\"  목표: F1 0.835+\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. RDKit 50개 Descriptor 함수\n",
        "# ============================================================\n",
        "\n",
        "def calculate_extended_rdkit(smiles):\n",
        "    \"\"\"\n",
        "    독성 예측 특화 RDKit 50개 Descriptor\n",
        "    \"\"\"\n",
        "    # 기본값\n",
        "    default_dict = {f'rdkit_{i}': np.nan for i in range(50)}\n",
        "\n",
        "    if pd.isna(smiles) or str(smiles).strip() == '':\n",
        "        return default_dict\n",
        "\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(str(smiles))\n",
        "    except:\n",
        "        mol = None\n",
        "\n",
        "    if mol is None:\n",
        "        return default_dict\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    # Lipinski (수소 결합, 극성)\n",
        "    try: result['rdkit_NumHDonors'] = Lipinski.NumHDonors(mol)\n",
        "    except: result['rdkit_NumHDonors'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumHAcceptors'] = Lipinski.NumHAcceptors(mol)\n",
        "    except: result['rdkit_NumHAcceptors'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumHeteroatoms'] = Lipinski.NumHeteroAtoms(mol)\n",
        "    except: result['rdkit_NumHeteroatoms'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumRotatableBonds'] = Lipinski.NumRotatableBonds(mol)\n",
        "    except: result['rdkit_NumRotatableBonds'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NHOHCount'] = Lipinski.NHOHCount(mol)\n",
        "    except: result['rdkit_NHOHCount'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NOCount'] = Lipinski.NOCount(mol)\n",
        "    except: result['rdkit_NOCount'] = np.nan\n",
        "\n",
        "    try: result['rdkit_RingCount'] = Lipinski.RingCount(mol)\n",
        "    except: result['rdkit_RingCount'] = np.nan\n",
        "\n",
        "    # 표면적 및 극성\n",
        "    try: result['rdkit_TPSA'] = Descriptors.TPSA(mol)\n",
        "    except: result['rdkit_TPSA'] = np.nan\n",
        "\n",
        "    try: result['rdkit_LabuteASA'] = Descriptors.LabuteASA(mol)\n",
        "    except: result['rdkit_LabuteASA'] = np.nan\n",
        "\n",
        "    # 고리 특징\n",
        "    try: result['rdkit_NumAromaticRings'] = Descriptors.NumAromaticRings(mol)\n",
        "    except: result['rdkit_NumAromaticRings'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumAliphaticRings'] = Descriptors.NumAliphaticRings(mol)\n",
        "    except: result['rdkit_NumAliphaticRings'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumSaturatedRings'] = Descriptors.NumSaturatedRings(mol)\n",
        "    except: result['rdkit_NumSaturatedRings'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumAromaticHeterocycles'] = Descriptors.NumAromaticHeterocycles(mol)\n",
        "    except: result['rdkit_NumAromaticHeterocycles'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumAromaticCarbocycles'] = Descriptors.NumAromaticCarbocycles(mol)\n",
        "    except: result['rdkit_NumAromaticCarbocycles'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumAliphaticHeterocycles'] = Descriptors.NumAliphaticHeterocycles(mol)\n",
        "    except: result['rdkit_NumAliphaticHeterocycles'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumAliphaticCarbocycles'] = Descriptors.NumAliphaticCarbocycles(mol)\n",
        "    except: result['rdkit_NumAliphaticCarbocycles'] = np.nan\n",
        "\n",
        "    # 복잡도\n",
        "    try: result['rdkit_BertzCT'] = Descriptors.BertzCT(mol)\n",
        "    except: result['rdkit_BertzCT'] = np.nan\n",
        "\n",
        "    try: result['rdkit_Ipc'] = Descriptors.Ipc(mol)\n",
        "    except: result['rdkit_Ipc'] = np.nan\n",
        "\n",
        "    try: result['rdkit_HallKierAlpha'] = Descriptors.HallKierAlpha(mol)\n",
        "    except: result['rdkit_HallKierAlpha'] = np.nan\n",
        "\n",
        "    # 연결성 지수 (Kier-Hall)\n",
        "    try: result['rdkit_Chi0v'] = Descriptors.Chi0v(mol)\n",
        "    except: result['rdkit_Chi0v'] = np.nan\n",
        "\n",
        "    try: result['rdkit_Chi1v'] = Descriptors.Chi1v(mol)\n",
        "    except: result['rdkit_Chi1v'] = np.nan\n",
        "\n",
        "    try: result['rdkit_Chi2v'] = Descriptors.Chi2v(mol)\n",
        "    except: result['rdkit_Chi2v'] = np.nan\n",
        "\n",
        "    try: result['rdkit_Chi3v'] = Descriptors.Chi3v(mol)\n",
        "    except: result['rdkit_Chi3v'] = np.nan\n",
        "\n",
        "    try: result['rdkit_Chi4v'] = Descriptors.Chi4v(mol)\n",
        "    except: result['rdkit_Chi4v'] = np.nan\n",
        "\n",
        "    # Kappa\n",
        "    try: result['rdkit_Kappa1'] = Descriptors.Kappa1(mol)\n",
        "    except: result['rdkit_Kappa1'] = np.nan\n",
        "\n",
        "    try: result['rdkit_Kappa2'] = Descriptors.Kappa2(mol)\n",
        "    except: result['rdkit_Kappa2'] = np.nan\n",
        "\n",
        "    try: result['rdkit_Kappa3'] = Descriptors.Kappa3(mol)\n",
        "    except: result['rdkit_Kappa3'] = np.nan\n",
        "\n",
        "    # VSA Descriptors (표면적 관련)\n",
        "    try: result['rdkit_PEOE_VSA1'] = Descriptors.PEOE_VSA1(mol)\n",
        "    except: result['rdkit_PEOE_VSA1'] = np.nan\n",
        "\n",
        "    try: result['rdkit_PEOE_VSA2'] = Descriptors.PEOE_VSA2(mol)\n",
        "    except: result['rdkit_PEOE_VSA2'] = np.nan\n",
        "\n",
        "    try: result['rdkit_PEOE_VSA3'] = Descriptors.PEOE_VSA3(mol)\n",
        "    except: result['rdkit_PEOE_VSA3'] = np.nan\n",
        "\n",
        "    try: result['rdkit_SMR_VSA1'] = Descriptors.SMR_VSA1(mol)\n",
        "    except: result['rdkit_SMR_VSA1'] = np.nan\n",
        "\n",
        "    try: result['rdkit_SMR_VSA2'] = Descriptors.SMR_VSA2(mol)\n",
        "    except: result['rdkit_SMR_VSA2'] = np.nan\n",
        "\n",
        "    try: result['rdkit_SlogP_VSA1'] = Descriptors.SlogP_VSA1(mol)\n",
        "    except: result['rdkit_SlogP_VSA1'] = np.nan\n",
        "\n",
        "    try: result['rdkit_SlogP_VSA2'] = Descriptors.SlogP_VSA2(mol)\n",
        "    except: result['rdkit_SlogP_VSA2'] = np.nan\n",
        "\n",
        "    # 전하 관련\n",
        "    try: result['rdkit_MaxPartialCharge'] = Descriptors.MaxPartialCharge(mol)\n",
        "    except: result['rdkit_MaxPartialCharge'] = np.nan\n",
        "\n",
        "    try: result['rdkit_MinPartialCharge'] = Descriptors.MinPartialCharge(mol)\n",
        "    except: result['rdkit_MinPartialCharge'] = np.nan\n",
        "\n",
        "    try: result['rdkit_MaxAbsPartialCharge'] = Descriptors.MaxAbsPartialCharge(mol)\n",
        "    except: result['rdkit_MaxAbsPartialCharge'] = np.nan\n",
        "\n",
        "    # 구조적 특징\n",
        "    try: result['rdkit_NumBridgeheadAtoms'] = Descriptors.NumBridgeheadAtoms(mol)\n",
        "    except: result['rdkit_NumBridgeheadAtoms'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumSpiroAtoms'] = Descriptors.NumSpiroAtoms(mol)\n",
        "    except: result['rdkit_NumSpiroAtoms'] = np.nan\n",
        "\n",
        "    try: result['rdkit_HeavyAtomCount'] = Descriptors.HeavyAtomCount(mol)\n",
        "    except: result['rdkit_HeavyAtomCount'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumValenceElectrons'] = Descriptors.NumValenceElectrons(mol)\n",
        "    except: result['rdkit_NumValenceElectrons'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumRadicalElectrons'] = Descriptors.NumRadicalElectrons(mol)\n",
        "    except: result['rdkit_NumRadicalElectrons'] = np.nan\n",
        "\n",
        "    # Graph Descriptors\n",
        "    try: result['rdkit_BalabanJ'] = GraphDescriptors.BalabanJ(mol)\n",
        "    except: result['rdkit_BalabanJ'] = np.nan\n",
        "\n",
        "    try: result['rdkit_BertzCT_Graph'] = GraphDescriptors.BertzCT(mol)\n",
        "    except: result['rdkit_BertzCT_Graph'] = np.nan\n",
        "\n",
        "    # 추가 물리화학적 특성\n",
        "    try: result['rdkit_MolMR'] = Crippen.MolMR(mol)\n",
        "    except: result['rdkit_MolMR'] = np.nan\n",
        "\n",
        "    try: result['rdkit_ExactMolWt'] = Descriptors.ExactMolWt(mol)\n",
        "    except: result['rdkit_ExactMolWt'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumSaturatedHeterocycles'] = Descriptors.NumSaturatedHeterocycles(mol)\n",
        "    except: result['rdkit_NumSaturatedHeterocycles'] = np.nan\n",
        "\n",
        "    try: result['rdkit_NumSaturatedCarbocycles'] = Descriptors.NumSaturatedCarbocycles(mol)\n",
        "    except: result['rdkit_NumSaturatedCarbocycles'] = np.nan\n",
        "\n",
        "    return result\n",
        "\n",
        "# ============================================================\n",
        "# 2. 데이터 로드\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 1: 데이터 로드\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "selected_features_df = pd.read_csv('selected_features_top300.csv')\n",
        "top300_features = selected_features_df['feature'].tolist()\n",
        "\n",
        "df_train = pd.read_csv('train.csv')\n",
        "X_train_base = df_train[top300_features].copy()\n",
        "y_train = df_train['label'].astype(int)\n",
        "\n",
        "df_test = pd.read_csv('predict_input.csv')\n",
        "X_test_base = df_test[top300_features].copy()\n",
        "\n",
        "if 'SMILES' in df_test.columns:\n",
        "    smiles_col = 'SMILES'\n",
        "elif 'smiles' in df_test.columns:\n",
        "    smiles_col = 'smiles'\n",
        "else:\n",
        "    smiles_col = df_test.columns[0]\n",
        "\n",
        "train_smiles = df_train[smiles_col]\n",
        "test_smiles = df_test[smiles_col]\n",
        "\n",
        "print(f\"\\n  ✓ Top 300 피처 로드 완료\")\n",
        "print(f\"  ✓ Train 데이터: {X_train_base.shape}\")\n",
        "print(f\"  ✓ Test 데이터: {X_test_base.shape}\")\n",
        "print(f\"  ✓ SMILES 컬럼: '{smiles_col}'\")\n",
        "print(f\"  ✓ Label 분포: Class 0 = {sum(y_train==0)}, Class 1 = {sum(y_train==1)}\")\n",
        "print(f\"  소요 시간: {time.time()-start_time:.2f}초\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. RDKit 50개 생성\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 2: RDKit Descriptors 생성\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train\n",
        "print(f\"\\n[Train RDKit 생성]\")\n",
        "rdkit_train_list = []\n",
        "for idx, smiles in enumerate(train_smiles):\n",
        "    if idx % 1000 == 0:\n",
        "        print(f\"\\r  진행: {idx}/{len(train_smiles)} ({idx/len(train_smiles)*100:.1f}%) - 예상 남은 시간: {((time.time()-start_time)/(idx+1))*(len(train_smiles)-idx):.0f}초\", end='')\n",
        "    rdkit_train_list.append(calculate_extended_rdkit(smiles))\n",
        "\n",
        "print(f\"\\r  ✓ {len(train_smiles)}개 완료 - 총 소요 시간: {time.time()-start_time:.2f}초\")\n",
        "\n",
        "rdkit_train_df = pd.DataFrame(rdkit_train_list)\n",
        "\n",
        "# 결측치 상세 분석\n",
        "print(f\"\\n[RDKit 생성 결과]\")\n",
        "print(f\"  생성된 컬럼: {len(rdkit_train_df.columns)}개\")\n",
        "missing_per_col = rdkit_train_df.isnull().sum()\n",
        "total_missing = missing_per_col.sum()\n",
        "print(f\"  총 결측치: {total_missing:,}개\")\n",
        "\n",
        "# 성공/실패 분석\n",
        "success_cols = missing_per_col[missing_per_col == 0]\n",
        "failed_cols = missing_per_col[missing_per_col == len(rdkit_train_df)]\n",
        "partial_cols = missing_per_col[(missing_per_col > 0) & (missing_per_col < len(rdkit_train_df))]\n",
        "\n",
        "print(f\"\\n  [성공 (결측 0%)]\")\n",
        "print(f\"    개수: {len(success_cols)}개\")\n",
        "if len(success_cols) > 0:\n",
        "    print(f\"    예시: {list(success_cols.index[:5])}\")\n",
        "\n",
        "print(f\"\\n  [부분 실패 (결측 1-99%)]\")\n",
        "print(f\"    개수: {len(partial_cols)}개\")\n",
        "if len(partial_cols) > 0:\n",
        "    print(f\"    예시: {list(partial_cols.index[:3])}\")\n",
        "    print(f\"    결측 비율: {(partial_cols / len(rdkit_train_df) * 100).round(2).to_dict()}\")\n",
        "\n",
        "print(f\"\\n  [완전 실패 (결측 100%)]\")\n",
        "print(f\"    개수: {len(failed_cols)}개\")\n",
        "if len(failed_cols) > 0:\n",
        "    print(f\"    컬럼: {list(failed_cols.index)}\")\n",
        "\n",
        "# Median imputation\n",
        "if total_missing > 0:\n",
        "    print(f\"\\n  → Median imputation 적용 중...\")\n",
        "    rdkit_train_df = rdkit_train_df.fillna(rdkit_train_df.median())\n",
        "    print(f\"  ✓ 완료\")\n",
        "\n",
        "# 통계\n",
        "print(f\"\\n[RDKit Descriptor 통계 (상위 10개)]\")\n",
        "print(rdkit_train_df.describe().loc[['mean', 'std', 'min', 'max']].iloc[:, :10].round(2))\n",
        "\n",
        "# Test\n",
        "print(f\"\\n[Test RDKit 생성]\")\n",
        "start_test = time.time()\n",
        "rdkit_test_list = []\n",
        "for idx, smiles in enumerate(test_smiles):\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"\\r  진행: {idx}/{len(test_smiles)} ({idx/len(test_smiles)*100:.1f}%)\", end='')\n",
        "    rdkit_test_list.append(calculate_extended_rdkit(smiles))\n",
        "\n",
        "print(f\"\\r  ✓ {len(test_smiles)}개 완료 - 소요 시간: {time.time()-start_test:.2f}초\")\n",
        "\n",
        "rdkit_test_df = pd.DataFrame(rdkit_test_list)\n",
        "if rdkit_test_df.isnull().sum().sum() > 0:\n",
        "    rdkit_test_df = rdkit_test_df.fillna(rdkit_test_df.median())\n",
        "\n",
        "# ============================================================\n",
        "# 4. 피처 결합\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 3: 피처 결합 및 전처리 준비\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "X_train = pd.concat([X_train_base.reset_index(drop=True),\n",
        "                     rdkit_train_df.reset_index(drop=True)], axis=1)\n",
        "X_test = pd.concat([X_test_base.reset_index(drop=True),\n",
        "                    rdkit_test_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "print(f\"\\n  ✓ 피처 결합 완료\")\n",
        "print(f\"    Top 300 피처: {len(top300_features)}개\")\n",
        "print(f\"    RDKit 피처: {len(rdkit_train_df.columns)}개\")\n",
        "print(f\"    총 피처: {X_train.shape[1]}개\")\n",
        "print(f\"    최종 Train shape: {X_train.shape}\")\n",
        "print(f\"    최종 Test shape: {X_test.shape}\")\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "# 전처리 설정\n",
        "fp_cols = [col for col in X_train.columns if str(col).startswith(('ecfp_', 'fcfp_', 'ptfp_'))]\n",
        "desc_cols = [col for col in X_train.columns if col in ['MolWt', 'clogp', 'sa_score', 'qed']]\n",
        "rdkit_cols = [col for col in X_train.columns if str(col).startswith('rdkit_')]\n",
        "\n",
        "print(f\"\\n  [피처 타입별 분류]\")\n",
        "print(f\"    Fingerprint: {len(fp_cols)}개\")\n",
        "print(f\"    Descriptor (원본): {len(desc_cols)}개\")\n",
        "print(f\"    RDKit Descriptor: {len(rdkit_cols)}개\")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('fp', SimpleImputer(strategy='constant', fill_value=0), fp_cols),\n",
        "        ('desc', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), desc_cols + rdkit_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "print(f\"\\n  ✓ 전처리 파이프라인 생성 완료\")\n",
        "print(f\"    - Fingerprint: 0으로 결측치 대체\")\n",
        "print(f\"    - Descriptor+RDKit: Median 대체 후 StandardScaler\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. Layer 1: Base Models (5-Fold)\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 4: Layer 1 - Base Models 학습 (5-Fold CV)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "oof_probabilities = {'lgbm': np.zeros(len(X_train)),\n",
        "                     'xgb': np.zeros(len(X_train)),\n",
        "                     'catboost': np.zeros(len(X_train))}\n",
        "\n",
        "test_predictions = {'lgbm': np.zeros((len(X_test), 5)),\n",
        "                   'xgb': np.zeros((len(X_test), 5)),\n",
        "                   'catboost': np.zeros((len(X_test), 5))}\n",
        "\n",
        "fold_times = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    fold_start = time.time()\n",
        "\n",
        "    print(f\"\\n{'─'*70}\")\n",
        "    print(f\"Fold {fold}/5\")\n",
        "    print(f\"{'─'*70}\")\n",
        "\n",
        "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
        "    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
        "\n",
        "    print(f\"  데이터 분할: Train {len(X_tr)}개, Valid {len(X_va)}개\")\n",
        "    print(f\"    Train Label: Class 0 = {sum(y_tr==0)}, Class 1 = {sum(y_tr==1)}\")\n",
        "    print(f\"    Valid Label: Class 0 = {sum(y_va==0)}, Class 1 = {sum(y_va==1)}\")\n",
        "\n",
        "    # 전처리\n",
        "    preprocess_start = time.time()\n",
        "    Xt_tr = preprocessor.fit_transform(X_tr)\n",
        "    Xt_va = preprocessor.transform(X_va)\n",
        "    print(f\"  ✓ 전처리 완료 ({time.time()-preprocess_start:.2f}초)\")\n",
        "    print(f\"    Train shape: {Xt_tr.shape}, Valid shape: {Xt_va.shape}\")\n",
        "\n",
        "    # LightGBM\n",
        "    print(f\"\\n  [1/3] LightGBM 학습 중...\", end=' ')\n",
        "    lgbm_start = time.time()\n",
        "    lgbm = LGBMClassifier(n_estimators=1000, learning_rate=0.03, max_depth=8,\n",
        "                         num_leaves=63, min_child_samples=30, subsample=0.8,\n",
        "                         colsample_bytree=0.8, reg_alpha=0.3, reg_lambda=0.3,\n",
        "                         class_weight={0: 1.5, 1: 1.0}, random_state=RANDOM_STATE,\n",
        "                         n_jobs=-1, verbose=-1)\n",
        "    lgbm.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)],\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "    oof_probabilities['lgbm'][va_idx] = lgbm.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "    lgbm_f1 = f1_score(y_va, (lgbm.predict_proba(Xt_va)[:, 1] >= 0.5).astype(int))\n",
        "    print(f\"완료 ({time.time()-lgbm_start:.2f}초)\")\n",
        "    print(f\"        Early Stop: {lgbm.best_iteration_}회, Valid F1: {lgbm_f1:.4f}\")\n",
        "\n",
        "    # XGBoost\n",
        "    print(f\"  [2/3] XGBoost 학습 중...\", end=' ')\n",
        "    xgb_start = time.time()\n",
        "    xgb = XGBClassifier(n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "                       min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
        "                       gamma=0.1, reg_alpha=0.3, reg_lambda=0.3, scale_pos_weight=0.67,\n",
        "                       random_state=RANDOM_STATE, n_jobs=-1,\n",
        "                       early_stopping_rounds=100, eval_metric='logloss', verbosity=0)\n",
        "    xgb.fit(Xt_tr, y_tr, eval_set=[(Xt_va, y_va)], verbose=False)\n",
        "    oof_probabilities['xgb'][va_idx] = xgb.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "    xgb_f1 = f1_score(y_va, (xgb.predict_proba(Xt_va)[:, 1] >= 0.5).astype(int))\n",
        "    print(f\"완료 ({time.time()-xgb_start:.2f}초)\")\n",
        "    print(f\"        Early Stop: {xgb.best_iteration}회, Valid F1: {xgb_f1:.4f}\")\n",
        "\n",
        "    # CatBoost\n",
        "    print(f\"  [3/3] CatBoost 학습 중...\", end=' ')\n",
        "    cat_start = time.time()\n",
        "    cat = CatBoostClassifier(iterations=1000, learning_rate=0.03, depth=7,\n",
        "                            l2_leaf_reg=3, class_weights=[1.5, 1.0],\n",
        "                            random_seed=RANDOM_STATE, verbose=0,\n",
        "                            early_stopping_rounds=100)\n",
        "    cat.fit(Xt_tr, y_tr, eval_set=(Xt_va, y_va), verbose=False)\n",
        "    oof_probabilities['catboost'][va_idx] = cat.predict_proba(Xt_va)[:, 1]\n",
        "\n",
        "    cat_f1 = f1_score(y_va, (cat.predict_proba(Xt_va)[:, 1] >= 0.5).astype(int))\n",
        "    print(f\"완료 ({time.time()-cat_start:.2f}초)\")\n",
        "    print(f\"        Early Stop: {cat.best_iteration_}회, Valid F1: {cat_f1:.4f}\")\n",
        "\n",
        "    # Test 예측\n",
        "    print(f\"\\n  Test 예측 중...\", end=' ')\n",
        "    test_start = time.time()\n",
        "    Xt_test = preprocessor.transform(X_test)\n",
        "    test_predictions['lgbm'][:, fold-1] = lgbm.predict_proba(Xt_test)[:, 1]\n",
        "    test_predictions['xgb'][:, fold-1] = xgb.predict_proba(Xt_test)[:, 1]\n",
        "    test_predictions['catboost'][:, fold-1] = cat.predict_proba(Xt_test)[:, 1]\n",
        "    print(f\"완료 ({time.time()-test_start:.2f}초)\")\n",
        "\n",
        "    fold_time = time.time() - fold_start\n",
        "    fold_times.append(fold_time)\n",
        "    print(f\"\\n  ✓ Fold {fold} 완료 - 총 소요 시간: {fold_time:.2f}초\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"✓ Layer 1 완료\")\n",
        "print(f\"  평균 Fold 시간: {np.mean(fold_times):.2f}초\")\n",
        "print(f\"  총 소요 시간: {sum(fold_times):.2f}초\")\n",
        "\n",
        "# OOF 확률 통계\n",
        "print(f\"\\n[OOF 확률 통계]\")\n",
        "for model_name, probs in oof_probabilities.items():\n",
        "    print(f\"  {model_name:8s}: Mean={probs.mean():.4f}, Std={probs.std():.4f}, Min={probs.min():.4f}, Max={probs.max():.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. Layer 2: 고급 Stacking\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 5: Layer 2 - 고급 Stacking (Meta-Learner)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "stack_start = time.time()\n",
        "\n",
        "# Meta-features 생성\n",
        "print(f\"\\n[Meta-features 생성 중...]\")\n",
        "meta_features_train = np.column_stack([\n",
        "    # 기본 확률\n",
        "    oof_probabilities['lgbm'],\n",
        "    oof_probabilities['xgb'],\n",
        "    oof_probabilities['catboost'],\n",
        "\n",
        "    # 상호작용\n",
        "    oof_probabilities['lgbm'] * oof_probabilities['xgb'],\n",
        "    oof_probabilities['xgb'] * oof_probabilities['catboost'],\n",
        "    oof_probabilities['lgbm'] * oof_probabilities['catboost'],\n",
        "\n",
        "    # 불일치도\n",
        "    np.abs(oof_probabilities['lgbm'] - oof_probabilities['xgb']),\n",
        "    np.abs(oof_probabilities['xgb'] - oof_probabilities['catboost']),\n",
        "    np.abs(oof_probabilities['lgbm'] - oof_probabilities['catboost']),\n",
        "\n",
        "    # 통계\n",
        "    np.max([oof_probabilities['lgbm'], oof_probabilities['xgb'], oof_probabilities['catboost']], axis=0),\n",
        "    np.min([oof_probabilities['lgbm'], oof_probabilities['xgb'], oof_probabilities['catboost']], axis=0),\n",
        "    np.mean([oof_probabilities['lgbm'], oof_probabilities['xgb'], oof_probabilities['catboost']], axis=0),\n",
        "    np.std([oof_probabilities['lgbm'], oof_probabilities['xgb'], oof_probabilities['catboost']], axis=0),\n",
        "])\n",
        "\n",
        "print(f\"  ✓ Meta-features shape: {meta_features_train.shape}\")\n",
        "print(f\"    1-3:   기본 확률 (LGBM, XGB, CAT)\")\n",
        "print(f\"    4-6:   상호작용 (LGBM*XGB, XGB*CAT, LGBM*CAT)\")\n",
        "print(f\"    7-9:   불일치도 (|LGBM-XGB|, |XGB-CAT|, |LGBM-CAT|)\")\n",
        "print(f\"    10-13: 통계 (max, min, mean, std)\")\n",
        "\n",
        "# LightGBM Meta-Learner\n",
        "print(f\"\\n[LightGBM Meta-Learner 학습 중...]\")\n",
        "meta_model = LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    num_leaves=7,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "meta_model.fit(meta_features_train, y_train)\n",
        "\n",
        "print(f\"  ✓ Meta-Learner 학습 완료 ({time.time()-stack_start:.2f}초)\")\n",
        "print(f\"    모델: LightGBM\")\n",
        "print(f\"    Iterations: {meta_model.n_estimators}\")\n",
        "print(f\"    Learning Rate: {meta_model.learning_rate}\")\n",
        "print(f\"    Max Depth: {meta_model.max_depth}\")\n",
        "\n",
        "# ============================================================\n",
        "# 7. 성능 평가\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 6: 최종 성능 평가\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "stacking_proba = meta_model.predict_proba(meta_features_train)[:, 1]\n",
        "\n",
        "print(f\"\\n[Stacking 확률 분포]\")\n",
        "print(f\"  Mean: {stacking_proba.mean():.4f}\")\n",
        "print(f\"  Std:  {stacking_proba.std():.4f}\")\n",
        "print(f\"  Min:  {stacking_proba.min():.4f}\")\n",
        "print(f\"  Max:  {stacking_proba.max():.4f}\")\n",
        "\n",
        "def get_adaptive_threshold(confidence):\n",
        "    if confidence < 0.05:\n",
        "        return 0.42\n",
        "    elif confidence < 0.10:\n",
        "        return 0.40\n",
        "    else:\n",
        "        return 0.39\n",
        "\n",
        "confidence = np.abs(stacking_proba - 0.5)\n",
        "adaptive_thresholds = np.array([get_adaptive_threshold(c) for c in confidence])\n",
        "predictions = (stacking_proba >= adaptive_thresholds).astype(int)\n",
        "\n",
        "print(f\"\\n[Adaptive Threshold 분포]\")\n",
        "print(f\"  Threshold 0.42: {sum(adaptive_thresholds==0.42)}개 ({sum(adaptive_thresholds==0.42)/len(adaptive_thresholds)*100:.2f}%)\")\n",
        "print(f\"  Threshold 0.40: {sum(adaptive_thresholds==0.40)}개 ({sum(adaptive_thresholds==0.40)/len(adaptive_thresholds)*100:.2f}%)\")\n",
        "print(f\"  Threshold 0.39: {sum(adaptive_thresholds==0.39)}개 ({sum(adaptive_thresholds==0.39)/len(adaptive_thresholds)*100:.2f}%)\")\n",
        "\n",
        "f1 = f1_score(y_train, predictions)\n",
        "auc = roc_auc_score(y_train, stacking_proba)\n",
        "cm = confusion_matrix(y_train, predictions)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "fpr = fp / (fp + tn)\n",
        "precision = precision_score(y_train, predictions)\n",
        "recall = recall_score(y_train, predictions)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"최종 성능 (OOF)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\n  F1 Score:  {f1:.4f}\")\n",
        "print(f\"  AUC Score: {auc:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  FPR:       {fpr:.4f} ({fpr*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n[혼동 행렬]\")\n",
        "print(f\"  TN (True Negative):  {tn:4d}  (정확히 무독성 예측)\")\n",
        "print(f\"  FP (False Positive): {fp:4d}  (독성인데 무독성으로 예측)\")\n",
        "print(f\"  FN (False Negative): {fn:4d}  (무독성인데 독성으로 예측)\")\n",
        "print(f\"  TP (True Positive):  {tp:4d}  (정확히 독성 예측)\")\n",
        "\n",
        "baseline_f1 = 0.8308\n",
        "print(f\"\\n[이전 최고 기록 대비]\")\n",
        "print(f\"  독립 RDKit 9개: {baseline_f1:.4f}\")\n",
        "print(f\"  현재 (RDKit 50): {f1:.4f}\")\n",
        "print(f\"  변화: {(f1-baseline_f1)*100:+.2f}%p\")\n",
        "\n",
        "if f1 > baseline_f1:\n",
        "    print(f\"  ✓✓✓ 성공! 새로운 최고 기록 달성!\")\n",
        "elif abs(f1 - baseline_f1) < 0.001:\n",
        "    print(f\"  △ 유사한 성능 (±0.1%p 이내)\")\n",
        "else:\n",
        "    print(f\"  ⚠️ 이전보다 낮음\")\n",
        "\n",
        "# ============================================================\n",
        "# 8. Test 예측 및 제출\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 7: Test 예측 및 제출 파일 생성\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n[Test Meta-features 생성 중...]\")\n",
        "meta_features_test = np.column_stack([\n",
        "    test_predictions['lgbm'].mean(axis=1),\n",
        "    test_predictions['xgb'].mean(axis=1),\n",
        "    test_predictions['catboost'].mean(axis=1),\n",
        "    test_predictions['lgbm'].mean(axis=1) * test_predictions['xgb'].mean(axis=1),\n",
        "    test_predictions['xgb'].mean(axis=1) * test_predictions['catboost'].mean(axis=1),\n",
        "    test_predictions['lgbm'].mean(axis=1) * test_predictions['catboost'].mean(axis=1),\n",
        "    np.abs(test_predictions['lgbm'].mean(axis=1) - test_predictions['xgb'].mean(axis=1)),\n",
        "    np.abs(test_predictions['xgb'].mean(axis=1) - test_predictions['catboost'].mean(axis=1)),\n",
        "    np.abs(test_predictions['lgbm'].mean(axis=1) - test_predictions['catboost'].mean(axis=1)),\n",
        "    np.max([test_predictions['lgbm'].mean(axis=1), test_predictions['xgb'].mean(axis=1), test_predictions['catboost'].mean(axis=1)], axis=0),\n",
        "    np.min([test_predictions['lgbm'].mean(axis=1), test_predictions['xgb'].mean(axis=1), test_predictions['catboost'].mean(axis=1)], axis=0),\n",
        "    np.mean([test_predictions['lgbm'].mean(axis=1), test_predictions['xgb'].mean(axis=1), test_predictions['catboost'].mean(axis=1)], axis=0),\n",
        "    np.std([test_predictions['lgbm'].mean(axis=1), test_predictions['xgb'].mean(axis=1), test_predictions['catboost'].mean(axis=1)], axis=0),\n",
        "])\n",
        "\n",
        "print(f\"  ✓ Test Meta-features shape: {meta_features_test.shape}\")\n",
        "\n",
        "print(f\"\\n[Meta-Learner로 최종 예측 중...]\")\n",
        "stacking_proba_test = meta_model.predict_proba(meta_features_test)[:, 1]\n",
        "confidence_test = np.abs(stacking_proba_test - 0.5)\n",
        "adaptive_thresholds_test = np.array([get_adaptive_threshold(c) for c in confidence_test])\n",
        "predictions_test = (stacking_proba_test >= adaptive_thresholds_test).astype(int)\n",
        "\n",
        "print(f\"  ✓ Test 예측 완료\")\n",
        "\n",
        "print(f\"\\n[Test 예측 결과 분석]\")\n",
        "print(f\"  Class 0 (무독성): {sum(predictions_test==0):3d}개 ({sum(predictions_test==0)/len(predictions_test)*100:.2f}%)\")\n",
        "print(f\"  Class 1 (독성):   {sum(predictions_test==1):3d}개 ({sum(predictions_test==1)/len(predictions_test)*100:.2f}%)\")\n",
        "print(f\"  평균 확률: {stacking_proba_test.mean():.4f}\")\n",
        "print(f\"  평균 Confidence: {confidence_test.mean():.4f}\")\n",
        "print(f\"  Low Confidence (<0.1): {sum(confidence_test<0.1)}개\")\n",
        "\n",
        "print(f\"\\n[제출 파일 생성 중...]\")\n",
        "submission = pd.DataFrame({\n",
        "    'SMILES': df_test[smiles_col],\n",
        "    'output': predictions_test\n",
        "})\n",
        "submission.to_csv('submission_ultimate.csv', index=False)\n",
        "print(f\"  ✓ 파일 저장: submission_ultimate.csv\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✓✓✓ 모든 프로세스 완료!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\n[최종 요약]\")\n",
        "print(f\"  모델: Top 300 + RDKit {len(rdkit_train_df.columns)}개 + 고급 Stacking\")\n",
        "print(f\"  최종 F1 Score: {f1:.4f}\")\n",
        "print(f\"  AUC Score: {auc:.4f}\")\n",
        "print(f\"  FPR: {fpr*100:.2f}%\")\n",
        "print(f\"  제출 파일: submission_ultimate.csv\")\n",
        "print(f\"\\n  🎉 제출 준비 완료!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRMp1mgiPYrK",
        "outputId": "0e3cd2ab-5b3c-492e-a0cb-3f225e861c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "최종 최적화 모델: Top 300 + RDKit 50개 + 고급 Stacking\n",
            "======================================================================\n",
            "\n",
            "[전략]\n",
            "  1. RDKit 50개 확장 (독성 예측 특화)\n",
            "  2. LightGBM Meta-Learner (LogReg 대체)\n",
            "  3. Meta-features 상호작용 추가\n",
            "  목표: F1 0.835+\n",
            "\n",
            "======================================================================\n",
            "Step 1: 데이터 로드\n",
            "======================================================================\n",
            "\n",
            "  ✓ Top 300 피처 로드 완료\n",
            "  ✓ Train 데이터: (8349, 300)\n",
            "  ✓ Test 데이터: (927, 300)\n",
            "  ✓ SMILES 컬럼: 'SMILES'\n",
            "  ✓ Label 분포: Class 0 = 3807, Class 1 = 4542\n",
            "  소요 시간: 8.56초\n",
            "\n",
            "======================================================================\n",
            "Step 2: RDKit Descriptors 생성\n",
            "======================================================================\n",
            "\n",
            "[Train RDKit 생성]\n",
            "  ✓ 8349개 완료 - 총 소요 시간: 50.30초\n",
            "\n",
            "[RDKit 생성 결과]\n",
            "  생성된 컬럼: 48개\n",
            "  총 결측치: 8,349개\n",
            "\n",
            "  [성공 (결측 0%)]\n",
            "    개수: 47개\n",
            "    예시: ['rdkit_NumHDonors', 'rdkit_NumHAcceptors', 'rdkit_NumRotatableBonds', 'rdkit_NHOHCount', 'rdkit_NOCount']\n",
            "\n",
            "  [부분 실패 (결측 1-99%)]\n",
            "    개수: 0개\n",
            "\n",
            "  [완전 실패 (결측 100%)]\n",
            "    개수: 1개\n",
            "    컬럼: ['rdkit_NumHeteroatoms']\n",
            "\n",
            "  → Median imputation 적용 중...\n",
            "  ✓ 완료\n",
            "\n",
            "[RDKit Descriptor 통계 (상위 10개)]\n",
            "      rdkit_NumHDonors  rdkit_NumHAcceptors  rdkit_NumHeteroatoms  \\\n",
            "mean              1.31                 5.64                   NaN   \n",
            "std               1.10                 2.17                   NaN   \n",
            "min               0.00                 0.00                   NaN   \n",
            "max              18.00                17.00                   NaN   \n",
            "\n",
            "      rdkit_NumRotatableBonds  rdkit_NHOHCount  rdkit_NOCount  \\\n",
            "mean                     5.74             1.49           6.56   \n",
            "std                      2.40             1.29           2.32   \n",
            "min                      0.00             0.00           0.00   \n",
            "max                     32.00            18.00          30.00   \n",
            "\n",
            "      rdkit_RingCount  rdkit_TPSA  rdkit_LabuteASA  rdkit_NumAromaticRings  \n",
            "mean             4.29       78.31           185.55                    2.75  \n",
            "std              1.20       32.28            35.92                    1.02  \n",
            "min              0.00        0.00            41.99                    0.00  \n",
            "max              9.00      496.68           513.21                    7.00  \n",
            "\n",
            "[Test RDKit 생성]\n",
            "  ✓ 927개 완료 - 소요 시간: 4.97초\n",
            "\n",
            "======================================================================\n",
            "Step 3: 피처 결합 및 전처리 준비\n",
            "======================================================================\n",
            "\n",
            "  ✓ 피처 결합 완료\n",
            "    Top 300 피처: 300개\n",
            "    RDKit 피처: 48개\n",
            "    총 피처: 348개\n",
            "    최종 Train shape: (8349, 348)\n",
            "    최종 Test shape: (927, 348)\n",
            "\n",
            "  [피처 타입별 분류]\n",
            "    Fingerprint: 296개\n",
            "    Descriptor (원본): 4개\n",
            "    RDKit Descriptor: 48개\n",
            "\n",
            "  ✓ 전처리 파이프라인 생성 완료\n",
            "    - Fingerprint: 0으로 결측치 대체\n",
            "    - Descriptor+RDKit: Median 대체 후 StandardScaler\n",
            "\n",
            "======================================================================\n",
            "Step 4: Layer 1 - Base Models 학습 (5-Fold CV)\n",
            "======================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Fold 1/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  데이터 분할: Train 6679개, Valid 1670개\n",
            "    Train Label: Class 0 = 3046, Class 1 = 3633\n",
            "    Valid Label: Class 0 = 761, Class 1 = 909\n",
            "  ✓ 전처리 완료 (0.08초)\n",
            "    Train shape: (6679, 347), Valid shape: (1670, 347)\n",
            "\n",
            "  [1/3] LightGBM 학습 중... 완료 (7.95초)\n",
            "        Early Stop: 727회, Valid F1: 0.8323\n",
            "  [2/3] XGBoost 학습 중... 완료 (11.91초)\n",
            "        Early Stop: 852회, Valid F1: 0.8313\n",
            "  [3/3] CatBoost 학습 중... 완료 (46.93초)\n",
            "        Early Stop: 979회, Valid F1: 0.8225\n",
            "\n",
            "  Test 예측 중... 완료 (0.12초)\n",
            "\n",
            "  ✓ Fold 1 완료 - 총 소요 시간: 67.01초\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Fold 2/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  데이터 분할: Train 6679개, Valid 1670개\n",
            "    Train Label: Class 0 = 3046, Class 1 = 3633\n",
            "    Valid Label: Class 0 = 761, Class 1 = 909\n",
            "  ✓ 전처리 완료 (0.06초)\n",
            "    Train shape: (6679, 347), Valid shape: (1670, 347)\n",
            "\n",
            "  [1/3] LightGBM 학습 중... 완료 (4.58초)\n",
            "        Early Stop: 474회, Valid F1: 0.8020\n",
            "  [2/3] XGBoost 학습 중... 완료 (8.76초)\n",
            "        Early Stop: 535회, Valid F1: 0.8061\n",
            "  [3/3] CatBoost 학습 중... 완료 (45.13초)\n",
            "        Early Stop: 929회, Valid F1: 0.7982\n",
            "\n",
            "  Test 예측 중... 완료 (0.08초)\n",
            "\n",
            "  ✓ Fold 2 완료 - 총 소요 시간: 58.63초\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Fold 3/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  데이터 분할: Train 6679개, Valid 1670개\n",
            "    Train Label: Class 0 = 3045, Class 1 = 3634\n",
            "    Valid Label: Class 0 = 762, Class 1 = 908\n",
            "  ✓ 전처리 완료 (0.06초)\n",
            "    Train shape: (6679, 347), Valid shape: (1670, 347)\n",
            "\n",
            "  [1/3] LightGBM 학습 중... 완료 (4.29초)\n",
            "        Early Stop: 434회, Valid F1: 0.7937\n",
            "  [2/3] XGBoost 학습 중... 완료 (8.20초)\n",
            "        Early Stop: 484회, Valid F1: 0.7908\n",
            "  [3/3] CatBoost 학습 중... 완료 (43.76초)\n",
            "        Early Stop: 962회, Valid F1: 0.7864\n",
            "\n",
            "  Test 예측 중... 완료 (0.08초)\n",
            "\n",
            "  ✓ Fold 3 완료 - 총 소요 시간: 56.40초\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Fold 4/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  데이터 분할: Train 6679개, Valid 1670개\n",
            "    Train Label: Class 0 = 3045, Class 1 = 3634\n",
            "    Valid Label: Class 0 = 762, Class 1 = 908\n",
            "  ✓ 전처리 완료 (0.07초)\n",
            "    Train shape: (6679, 347), Valid shape: (1670, 347)\n",
            "\n",
            "  [1/3] LightGBM 학습 중... 완료 (6.33초)\n",
            "        Early Stop: 510회, Valid F1: 0.8117\n",
            "  [2/3] XGBoost 학습 중... 완료 (8.28초)\n",
            "        Early Stop: 649회, Valid F1: 0.8144\n",
            "  [3/3] CatBoost 학습 중... 완료 (45.58초)\n",
            "        Early Stop: 997회, Valid F1: 0.8184\n",
            "\n",
            "  Test 예측 중... 완료 (0.11초)\n",
            "\n",
            "  ✓ Fold 4 완료 - 총 소요 시간: 60.38초\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Fold 5/5\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  데이터 분할: Train 6680개, Valid 1669개\n",
            "    Train Label: Class 0 = 3046, Class 1 = 3634\n",
            "    Valid Label: Class 0 = 761, Class 1 = 908\n",
            "  ✓ 전처리 완료 (0.06초)\n",
            "    Train shape: (6680, 347), Valid shape: (1669, 347)\n",
            "\n",
            "  [1/3] LightGBM 학습 중... 완료 (6.77초)\n",
            "        Early Stop: 582회, Valid F1: 0.8231\n",
            "  [2/3] XGBoost 학습 중... 완료 (9.31초)\n",
            "        Early Stop: 763회, Valid F1: 0.8327\n",
            "  [3/3] CatBoost 학습 중... 완료 (46.03초)\n",
            "        Early Stop: 997회, Valid F1: 0.8324\n",
            "\n",
            "  Test 예측 중... 완료 (0.10초)\n",
            "\n",
            "  ✓ Fold 5 완료 - 총 소요 시간: 62.29초\n",
            "\n",
            "======================================================================\n",
            "✓ Layer 1 완료\n",
            "  평균 Fold 시간: 60.94초\n",
            "  총 소요 시간: 304.70초\n",
            "\n",
            "[OOF 확률 통계]\n",
            "  lgbm    : Mean=0.5130, Std=0.3480, Min=0.0013, Max=0.9996\n",
            "  xgb     : Mean=0.5145, Std=0.3506, Min=0.0013, Max=0.9993\n",
            "  catboost: Mean=0.5084, Std=0.3301, Min=0.0032, Max=0.9994\n",
            "\n",
            "======================================================================\n",
            "Step 5: Layer 2 - 고급 Stacking (Meta-Learner)\n",
            "======================================================================\n",
            "\n",
            "[Meta-features 생성 중...]\n",
            "  ✓ Meta-features shape: (8349, 13)\n",
            "    1-3:   기본 확률 (LGBM, XGB, CAT)\n",
            "    4-6:   상호작용 (LGBM*XGB, XGB*CAT, LGBM*CAT)\n",
            "    7-9:   불일치도 (|LGBM-XGB|, |XGB-CAT|, |LGBM-CAT|)\n",
            "    10-13: 통계 (max, min, mean, std)\n",
            "\n",
            "[LightGBM Meta-Learner 학습 중...]\n",
            "  ✓ Meta-Learner 학습 완료 (0.08초)\n",
            "    모델: LightGBM\n",
            "    Iterations: 100\n",
            "    Learning Rate: 0.05\n",
            "    Max Depth: 3\n",
            "\n",
            "======================================================================\n",
            "Step 6: 최종 성능 평가\n",
            "======================================================================\n",
            "\n",
            "[Stacking 확률 분포]\n",
            "  Mean: 0.5202\n",
            "  Std:  0.3332\n",
            "  Min:  0.0114\n",
            "  Max:  0.9852\n",
            "\n",
            "[Adaptive Threshold 분포]\n",
            "  Threshold 0.42: 713개 (8.54%)\n",
            "  Threshold 0.40: 387개 (4.64%)\n",
            "  Threshold 0.39: 7249개 (86.82%)\n",
            "\n",
            "======================================================================\n",
            "최종 성능 (OOF)\n",
            "======================================================================\n",
            "\n",
            "  F1 Score:  0.8321\n",
            "  AUC Score: 0.8953\n",
            "  Precision: 0.7864\n",
            "  Recall:    0.8835\n",
            "  FPR:       0.2863 (28.63%)\n",
            "\n",
            "[혼동 행렬]\n",
            "  TN (True Negative):  2717  (정확히 무독성 예측)\n",
            "  FP (False Positive): 1090  (독성인데 무독성으로 예측)\n",
            "  FN (False Negative):  529  (무독성인데 독성으로 예측)\n",
            "  TP (True Positive):  4013  (정확히 독성 예측)\n",
            "\n",
            "[이전 최고 기록 대비]\n",
            "  독립 RDKit 9개: 0.8308\n",
            "  현재 (RDKit 50): 0.8321\n",
            "  변화: +0.13%p\n",
            "  ✓✓✓ 성공! 새로운 최고 기록 달성!\n",
            "\n",
            "======================================================================\n",
            "Step 7: Test 예측 및 제출 파일 생성\n",
            "======================================================================\n",
            "\n",
            "[Test Meta-features 생성 중...]\n",
            "  ✓ Test Meta-features shape: (927, 13)\n",
            "\n",
            "[Meta-Learner로 최종 예측 중...]\n",
            "  ✓ Test 예측 완료\n",
            "\n",
            "[Test 예측 결과 분석]\n",
            "  Class 0 (무독성): 366개 (39.48%)\n",
            "  Class 1 (독성):   561개 (60.52%)\n",
            "  평균 확률: 0.5144\n",
            "  평균 Confidence: 0.2931\n",
            "  Low Confidence (<0.1): 123개\n",
            "\n",
            "[제출 파일 생성 중...]\n",
            "  ✓ 파일 저장: submission_ultimate.csv\n",
            "\n",
            "======================================================================\n",
            "✓✓✓ 모든 프로세스 완료!\n",
            "======================================================================\n",
            "\n",
            "[최종 요약]\n",
            "  모델: Top 300 + RDKit 48개 + 고급 Stacking\n",
            "  최종 F1 Score: 0.8321\n",
            "  AUC Score: 0.8953\n",
            "  FPR: 28.63%\n",
            "  제출 파일: submission_ultimate.csv\n",
            "\n",
            "  🎉 제출 준비 완료!\n"
          ]
        }
      ]
    }
  ]
}